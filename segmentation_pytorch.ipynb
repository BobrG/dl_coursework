{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "%matplotlib inline\n",
    "#datapreparation\n",
    "import os\n",
    "import scipy.io as sio\n",
    "import cv2\n",
    "#pytorch\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose, ToPILImage\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path, pad=True):\n",
    "    \"\"\"\n",
    "    Load image from a given path and pad it on the sides, so that eash side is divisible by 32 (newtwork requirement)\n",
    "    \n",
    "    if pad = True:\n",
    "        returns image as numpy.array, tuple with padding in pixels as(x_min_pad, y_min_pad, x_max_pad, y_max_pad)\n",
    "    else:\n",
    "        returns image as numpy.array\n",
    "    \"\"\"\n",
    "    img = cv2.imread(str(path))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    if not pad:\n",
    "        return img\n",
    "    \n",
    "    height, width, _ = img.shape\n",
    "    \n",
    "    if height % 32 == 0:\n",
    "        y_min_pad = 0\n",
    "        y_max_pad = 0\n",
    "    else:\n",
    "        y_pad = 32 - height % 32\n",
    "        y_min_pad = int(y_pad / 2)\n",
    "        y_max_pad = y_pad - y_min_pad\n",
    "        \n",
    "    if width % 32 == 0:\n",
    "        x_min_pad = 0\n",
    "        x_max_pad = 0\n",
    "    else:\n",
    "        x_pad = 32 - width % 32\n",
    "        x_min_pad = int(x_pad / 2)\n",
    "        x_max_pad = x_pad - x_min_pad\n",
    "    \n",
    "    img = cv2.copyMakeBorder(img, y_min_pad, y_max_pad, x_min_pad, x_max_pad, cv2.BORDER_REFLECT_101)\n",
    "\n",
    "    return img, (x_min_pad, y_min_pad, x_max_pad, y_max_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SURREAL Dataset for pre-training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SURREALDataset(Dataset):\n",
    "    def __init__(self, dirry, transforms=None, lengt=None, nclasses=True):\n",
    "        self.pics = []\n",
    "        subdirs = [i[0] for i in os.walk(dirry)]\n",
    "        \n",
    "        for i in tqdm_notebook(subdirs):\n",
    "            for j in [k for k in os.listdir(i) if k.endswith('jpg')]:\n",
    "                self.pics.append(i + '/' + j)\n",
    "                if (len(self.pics) > lengt and lengt is not None):\n",
    "                    break\n",
    "        self.classes = nclasses\n",
    "        self.transforms = transforms\n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        num_classes = 25\n",
    "        \n",
    "        # get image and add padding to shape (x_height // 32 == 0, x_width // 32 == 0)\n",
    "        x, pad = load_image(self.pics[i], pad=True)\n",
    "        \n",
    "        # get segmentation map\n",
    "        tmp = self.pics[i].split('frame')\n",
    "        mask = sio.loadmat(tmp[0] + '_segm.mat')['segm_' + str(int(tmp[1][0:-4]) + 1)]   \n",
    "        y = np.zeros((num_classes, 256, 320))\n",
    "        \n",
    "        for i in range(len(mask)):\n",
    "            row = mask[i]\n",
    "            for j in range(len(row)):\n",
    "                y[row[j], i, j] = 1.0\n",
    "\n",
    "        # transorm ToTensor + add normalization to zero mean and unit std\n",
    "        if self.transforms is not None:\n",
    "            x = self.transforms(x)\n",
    "       \n",
    "        for i in range(0, 3):\n",
    "            x[i] -= x[i].mean()\n",
    "            \n",
    "            x[i] /= x[i].std()\n",
    "            \n",
    "        # get all maps or only one\n",
    "        if self.classes:\n",
    "            y = torch.FloatTensor(y) #np.rollaxis(y, 0, 3)\n",
    "        else:\n",
    "            y = torch.FloatTensor(y[16].reshape(1, 256, 320)) #np.rollaxis(y, 0, 3)\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pics)\n",
    "    \n",
    "    def get_pics_path(self, indx):\n",
    "        return self.pics[indx]\n",
    "\n",
    "def show_pics(imgs, col, row):\n",
    "    fig = plt.figure(figsize=(4*col,4*row))\n",
    "    for i in range(0, col*row ):\n",
    "        fig.add_subplot(row, col, i + 1)\n",
    "        plt.imshow(imgs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sitting People Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SittingDataset(Dataset):\n",
    "    def __init__(self, dirry, transforms=None, nclasses = True):\n",
    "        self.pics = []\n",
    "        for j in tqdm_notebook([k for k in os.listdir(dirry) if k.endswith('jpg')]):\n",
    "            self.pics.append(dirry + '/' + j)\n",
    "\n",
    "        self.classes = nclasses\n",
    "        self.transforms = transforms\n",
    "    def __getitem__(self, i):\n",
    "        # get image \n",
    "        x = plt.imread(self.pics[i])\n",
    "        # get segmentation map\n",
    "        tmp = self.pics[i].split('img')\n",
    "        mask = sio.loadmat(tmp[0] + 'masks' + tmp[-1][:-4] + '.mat')['M']   \n",
    "        y = np.zeros((25, 320, 320))\n",
    "        \n",
    "        for i in range(len(mask)):\n",
    "            row = mask[i]\n",
    "            for j in range(len(row)):\n",
    "                y[int(row[j]), i, j] = 1.0\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            x = self.transforms(x)\n",
    "       \n",
    "        for i in range(0, 3):\n",
    "            x[i] -= x[i].mean()\n",
    "            \n",
    "            x[i] /= x[i].std()\n",
    "    \n",
    "        if self.classes:\n",
    "            y = torch.FloatTensor(y.reshape(25, 320, 320)) #np.rollaxis(y, 0, 3)\n",
    "        else:\n",
    "            y = torch.FloatTensor(y[1].reshape(1, 320, 320)) #np.rollaxis(y, 0, 3)\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "635b4ba5704547f6b7c6a40dcae0d696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1967), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b6a0c065e5a44aebdbc554397e70ae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=704), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac60c2626eb249dbbb3d71e59c0845ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=174), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "transf = Compose([\n",
    "    ToTensor(),\n",
    "])\n",
    "\n",
    "dset_train = SURREALDataset('/home/novikov/data/skoltech/segmentation/SURREAL/data/sur/SURREAL/data/cmu/train/run0', transf, 50000, False)\n",
    "dset_test = SURREALDataset('/home/novikov/data/skoltech/segmentation/SURREAL/data/sur/SURREAL/data/cmu/test/run0', transf, 10000, False)\n",
    "dset_val = SURREALDataset('/home/novikov/data/skoltech/segmentation/SURREAL/data/sur/SURREAL/data/cmu/val/run0', transf, 10000, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 24\n",
    "train_loader = DataLoader(dset_train, batch_size=batch_sz, shuffle=False)\n",
    "test_loader = DataLoader(dset_test, batch_size=batch_sz, shuffle=True)\n",
    "val_loader = DataLoader(dset_val, batch_size=batch_sz, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f60d4587e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputs_sur, classes_sur = next(iter(train_loader))\n",
    "\n",
    "show_pics(classes_sur[:, 0], 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d329f10402b14a4fb16c7101658fae0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=201), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dset_sitting = SittingDataset('/home/novikov/data/skoltech/segmentation/SURREAL/data/sitting/img', transf, False)\n",
    "sitting_loader = DataLoader(dset_sitting, batch_size=batch_sz, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_sit, classes_sit = next(iter(sitting_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f616d0612e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_pics(classes_sit[:, 0], 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model from https://github.com/ternaus/TernausNet\n",
    "def conv3x3(in_, out):\n",
    "    return nn.Conv2d(in_, out, 3, padding=1)\n",
    "\n",
    "\n",
    "class ConvRelu(nn.Module):\n",
    "    def __init__(self, in_, out):\n",
    "        super().__init__()\n",
    "        self.conv = conv3x3(in_, out)\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            ConvRelu(in_channels, middle_channels),\n",
    "            nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class UNet11(nn.Module):\n",
    "    def __init__(self, num_filters=32, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        if pretrained == 'vgg':\n",
    "            self.encoder = models.vgg11(pretrained=True).features\n",
    "        else:\n",
    "            self.encoder = models.vgg11(pretrained=False).features\n",
    "\n",
    "        \n",
    "        self.relu = self.encoder[1]\n",
    "        self.conv1 = self.encoder[0]\n",
    "        self.conv2 = self.encoder[3]\n",
    "        self.conv3s = self.encoder[6]\n",
    "        self.conv3 = self.encoder[8]\n",
    "        self.conv4s = self.encoder[11]\n",
    "        self.conv4 = self.encoder[13]\n",
    "        self.conv5s = self.encoder[16]\n",
    "        self.conv5 = self.encoder[18]\n",
    "\n",
    "        self.center = DecoderBlock(num_filters * 8 * 2, num_filters * 8 * 2, num_filters * 8)\n",
    "        self.dec5 = DecoderBlock(num_filters * (16 + 8), num_filters * 8 * 2, num_filters * 8)\n",
    "        self.dec4 = DecoderBlock(num_filters * (16 + 8), num_filters * 8 * 2, num_filters * 4)\n",
    "        self.dec3 = DecoderBlock(num_filters * (8 + 4), num_filters * 4 * 2, num_filters * 2)\n",
    "        self.dec2 = DecoderBlock(num_filters * (4 + 2), num_filters * 2 * 2, num_filters)\n",
    "        self.dec1 = ConvRelu(num_filters * (2 + 1), num_filters)\n",
    "\n",
    "        self.final = nn.Conv2d(num_filters, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.relu(self.conv1(x))\n",
    "        conv2 = self.relu(self.conv2(self.pool(conv1)))\n",
    "        conv3s = self.relu(self.conv3s(self.pool(conv2)))\n",
    "        conv3 = self.relu(self.conv3(conv3s))\n",
    "        conv4s = self.relu(self.conv4s(self.pool(conv3)))\n",
    "        conv4 = self.relu(self.conv4(conv4s))\n",
    "        conv5s = self.relu(self.conv5s(self.pool(conv4)))\n",
    "        conv5 = self.relu(self.conv5(conv5s))\n",
    "\n",
    "        center = self.center(self.pool(conv5))\n",
    "\n",
    "        dec5 = self.dec5(torch.cat([center, conv5], 1))\n",
    "        dec4 = self.dec4(torch.cat([dec5, conv4], 1))\n",
    "        dec3 = self.dec3(torch.cat([dec4, conv3], 1))\n",
    "        dec2 = self.dec2(torch.cat([dec3, conv2], 1))\n",
    "        dec1 = self.dec1(torch.cat([dec2, conv1], 1))\n",
    "        return F.sigmoid(self.final(dec1))\n",
    "\n",
    "\n",
    "def unet11(pretrained=False, path='', **kwargs):\n",
    "    model = UNet11(pretrained=pretrained, **kwargs)\n",
    "\n",
    "    if pretrained == 'carvana':\n",
    "        state = torch.load('TernausNet.pt')\n",
    "        model.load_state_dict(state['model'])\n",
    "    \n",
    "    if pretrained == 'trained_recently':\n",
    "        state = torch.load(path)\n",
    "        model.load_state_dict(state)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_dice_loss(inputs, targets):\n",
    "    num = targets.size(0)\n",
    "    m1  = inputs.view(num,-1)\n",
    "    m2  = targets.view(num,-1)\n",
    "    intersection = (m1 * m2)\n",
    "    score = 2. * (intersection.sum(1)+1) / (m1.sum(1) + m2.sum(1)+1)\n",
    "    score = 1 - score.sum()/num\n",
    "    return score\n",
    "def jaccard_loss(inputs, targets):\n",
    "    #J = Dice / (2 - Dice)\n",
    "    return soft_dice_loss(inputs, targets) / (2 - soft_dice_loss(inputs, targets))\n",
    "def L_loss(inputs, targets):\n",
    "    return F.binary_cross_entropy(inputs, targets) - torch.log(jaccard_loss(inputs, targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet11('carvana')\n",
    "#model = unet11('trained_recently', path='pytorch_segm_model_normalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001, momentum=0.5)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.5, nesterov=True)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    patience=1,\n",
    "    verbose=True,\n",
    "    threshold=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, gpu=False):\n",
    "    model.train()\n",
    "    if gpu == True:\n",
    "        model.cuda()\n",
    "    loss_tr = []\n",
    "    \n",
    "    for batch_idx, (data, target) in (enumerate(train_loader)):\n",
    "    \n",
    "        if gpu == True:\n",
    "            data, target = data.cuda(async=True), target.cuda(async=True) # On GPU\n",
    "        \n",
    "        data, target = torch.unsqueeze(Variable(data), dim=0)[0], Variable(target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "# Simple summ of losses:\n",
    "        loss_1 = F.binary_cross_entropy(model(data), target)\n",
    "        loss_2 = soft_dice_loss(model(data), target)\n",
    "        loss = loss_1 + loss_2\n",
    "        loss_tr.append([loss_1.data[0], loss_2.data[0]])\n",
    "\n",
    "# Joint loss https://arxiv.org/pdf/1706.06169.pdf\n",
    "#         loss = L_loss(model(data), target)\n",
    "#         loss_tr.append(loss.data[0])\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: BinaryCrossEntropyLoss {:.5f}, SoftDiceLoss {:.5f}, SumLoss {:.5f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss_1.data[0], loss_2.data[0], loss.data[0]))\n",
    "    \n",
    "    return loss_tr\n",
    "\n",
    "def validate(gpu=False):\n",
    "    model.eval()\n",
    "    if gpu == True:\n",
    "        model.cuda()\n",
    "    val_loss = []\n",
    "    \n",
    "    for batch_idx, (data, target) in (enumerate(val_loader)):\n",
    "       \n",
    "        if gpu == True:\n",
    "            data, target = data.cuda(async=True), target.cuda(async=True) # On GPU\n",
    "       \n",
    "        data, target = torch.unsqueeze(Variable(data), dim=0)[0], Variable(target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "# Simple summ of losses:\n",
    "        #loss_1 = F.binary_cross_entropy(model(data), target)\n",
    "        #loss_2 = soft_dice_loss(model(data), target)      \n",
    "        val_loss.append(F.binary_cross_entropy(model(data), target).data[0] + soft_dice_loss(model(data), target).data[0])\n",
    "\n",
    "# Joint loss\n",
    "#         loss = L_loss(model(data), target)\n",
    "#         val_loss.append(loss.data[0])\n",
    "        \n",
    "    val_loss.append(np.mean(val_loss))\n",
    "    scheduler.step(val_loss[-1])\n",
    "    print('Loss on validation:', val_loss[-1])\n",
    "    return val_loss[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/51892 (0%)]\tLoss: BinaryCrossEntropyLoss 7.40798, SoftDiceLoss 0.98859, SumLoss 8.39657\n",
      "Train Epoch: 0 [240/51892 (0%)]\tLoss: BinaryCrossEntropyLoss 0.28462, SoftDiceLoss 0.98014, SumLoss 1.26476\n",
      "Train Epoch: 0 [480/51892 (1%)]\tLoss: BinaryCrossEntropyLoss 0.21171, SoftDiceLoss 0.98803, SumLoss 1.19974\n",
      "Train Epoch: 0 [720/51892 (1%)]\tLoss: BinaryCrossEntropyLoss 0.06901, SoftDiceLoss 0.99233, SumLoss 1.06134\n",
      "Train Epoch: 0 [960/51892 (2%)]\tLoss: BinaryCrossEntropyLoss 0.05420, SoftDiceLoss 0.98145, SumLoss 1.03564\n",
      "Train Epoch: 0 [1200/51892 (2%)]\tLoss: BinaryCrossEntropyLoss 0.04801, SoftDiceLoss 0.97945, SumLoss 1.02745\n",
      "Train Epoch: 0 [1440/51892 (3%)]\tLoss: BinaryCrossEntropyLoss 0.04850, SoftDiceLoss 0.97665, SumLoss 1.02515\n",
      "Train Epoch: 0 [1680/51892 (3%)]\tLoss: BinaryCrossEntropyLoss 0.03854, SoftDiceLoss 0.97010, SumLoss 1.00864\n",
      "Train Epoch: 0 [1920/51892 (4%)]\tLoss: BinaryCrossEntropyLoss 0.04644, SoftDiceLoss 0.97194, SumLoss 1.01838\n",
      "Train Epoch: 0 [2160/51892 (4%)]\tLoss: BinaryCrossEntropyLoss 0.03615, SoftDiceLoss 0.97559, SumLoss 1.01174\n",
      "Train Epoch: 0 [2400/51892 (5%)]\tLoss: BinaryCrossEntropyLoss 0.03232, SoftDiceLoss 0.94216, SumLoss 0.97448\n",
      "Train Epoch: 0 [2640/51892 (5%)]\tLoss: BinaryCrossEntropyLoss 0.16224, SoftDiceLoss 0.97441, SumLoss 1.13665\n",
      "Train Epoch: 0 [2880/51892 (6%)]\tLoss: BinaryCrossEntropyLoss 0.05464, SoftDiceLoss 0.98747, SumLoss 1.04212\n",
      "Train Epoch: 0 [3120/51892 (6%)]\tLoss: BinaryCrossEntropyLoss 0.06787, SoftDiceLoss 0.99280, SumLoss 1.06067\n",
      "Train Epoch: 0 [3360/51892 (6%)]\tLoss: BinaryCrossEntropyLoss 0.04541, SoftDiceLoss 0.97857, SumLoss 1.02398\n",
      "Train Epoch: 0 [3600/51892 (7%)]\tLoss: BinaryCrossEntropyLoss 0.03570, SoftDiceLoss 0.95990, SumLoss 0.99561\n",
      "Train Epoch: 0 [3840/51892 (7%)]\tLoss: BinaryCrossEntropyLoss 0.03986, SoftDiceLoss 0.95857, SumLoss 0.99842\n",
      "Train Epoch: 0 [4080/51892 (8%)]\tLoss: BinaryCrossEntropyLoss 0.03484, SoftDiceLoss 0.96427, SumLoss 0.99912\n",
      "Train Epoch: 0 [4320/51892 (8%)]\tLoss: BinaryCrossEntropyLoss 0.04615, SoftDiceLoss 0.97010, SumLoss 1.01626\n",
      "Train Epoch: 0 [4560/51892 (9%)]\tLoss: BinaryCrossEntropyLoss 0.03758, SoftDiceLoss 0.89787, SumLoss 0.93546\n",
      "Train Epoch: 0 [4800/51892 (9%)]\tLoss: BinaryCrossEntropyLoss 0.03259, SoftDiceLoss 0.85825, SumLoss 0.89084\n",
      "Train Epoch: 0 [5040/51892 (10%)]\tLoss: BinaryCrossEntropyLoss 0.05470, SoftDiceLoss 0.95159, SumLoss 1.00630\n",
      "Train Epoch: 0 [5280/51892 (10%)]\tLoss: BinaryCrossEntropyLoss 0.04958, SoftDiceLoss 0.92124, SumLoss 0.97082\n",
      "Train Epoch: 0 [5520/51892 (11%)]\tLoss: BinaryCrossEntropyLoss 0.03933, SoftDiceLoss 0.92534, SumLoss 0.96467\n",
      "Train Epoch: 0 [5760/51892 (11%)]\tLoss: BinaryCrossEntropyLoss 0.04118, SoftDiceLoss 0.91601, SumLoss 0.95719\n",
      "Train Epoch: 0 [6000/51892 (12%)]\tLoss: BinaryCrossEntropyLoss 0.03314, SoftDiceLoss 0.92667, SumLoss 0.95981\n",
      "Train Epoch: 0 [6240/51892 (12%)]\tLoss: BinaryCrossEntropyLoss 0.03086, SoftDiceLoss 0.93079, SumLoss 0.96165\n",
      "Train Epoch: 0 [6480/51892 (12%)]\tLoss: BinaryCrossEntropyLoss 0.03154, SoftDiceLoss 0.88930, SumLoss 0.92084\n",
      "Train Epoch: 0 [6720/51892 (13%)]\tLoss: BinaryCrossEntropyLoss 0.03896, SoftDiceLoss 0.88364, SumLoss 0.92261\n",
      "Train Epoch: 0 [6960/51892 (13%)]\tLoss: BinaryCrossEntropyLoss 0.04015, SoftDiceLoss 0.96839, SumLoss 1.00854\n",
      "Train Epoch: 0 [7200/51892 (14%)]\tLoss: BinaryCrossEntropyLoss 0.03247, SoftDiceLoss 0.93590, SumLoss 0.96837\n",
      "Train Epoch: 0 [7440/51892 (14%)]\tLoss: BinaryCrossEntropyLoss 0.01520, SoftDiceLoss 0.85349, SumLoss 0.86869\n",
      "Train Epoch: 0 [7680/51892 (15%)]\tLoss: BinaryCrossEntropyLoss 0.01791, SoftDiceLoss 0.90068, SumLoss 0.91859\n",
      "Train Epoch: 0 [7920/51892 (15%)]\tLoss: BinaryCrossEntropyLoss 0.01750, SoftDiceLoss 0.86295, SumLoss 0.88045\n",
      "Train Epoch: 0 [8160/51892 (16%)]\tLoss: BinaryCrossEntropyLoss 0.03206, SoftDiceLoss 0.81538, SumLoss 0.84744\n",
      "Train Epoch: 0 [8400/51892 (16%)]\tLoss: BinaryCrossEntropyLoss 0.02123, SoftDiceLoss 0.71977, SumLoss 0.74100\n",
      "Train Epoch: 0 [8640/51892 (17%)]\tLoss: BinaryCrossEntropyLoss 0.02544, SoftDiceLoss 0.81392, SumLoss 0.83936\n",
      "Train Epoch: 0 [8880/51892 (17%)]\tLoss: BinaryCrossEntropyLoss 0.03083, SoftDiceLoss 0.87795, SumLoss 0.90878\n",
      "Train Epoch: 0 [9120/51892 (18%)]\tLoss: BinaryCrossEntropyLoss 0.02318, SoftDiceLoss 0.80762, SumLoss 0.83081\n",
      "Train Epoch: 0 [9360/51892 (18%)]\tLoss: BinaryCrossEntropyLoss 0.02180, SoftDiceLoss 0.79783, SumLoss 0.81963\n",
      "Train Epoch: 0 [9600/51892 (18%)]\tLoss: BinaryCrossEntropyLoss 0.02092, SoftDiceLoss 0.77957, SumLoss 0.80049\n",
      "Train Epoch: 0 [9840/51892 (19%)]\tLoss: BinaryCrossEntropyLoss 0.01719, SoftDiceLoss 0.71859, SumLoss 0.73578\n",
      "Train Epoch: 0 [10080/51892 (19%)]\tLoss: BinaryCrossEntropyLoss 0.01908, SoftDiceLoss 0.76061, SumLoss 0.77969\n",
      "Train Epoch: 0 [10320/51892 (20%)]\tLoss: BinaryCrossEntropyLoss 0.02802, SoftDiceLoss 0.80321, SumLoss 0.83123\n",
      "Train Epoch: 0 [10560/51892 (20%)]\tLoss: BinaryCrossEntropyLoss 0.02354, SoftDiceLoss 0.80298, SumLoss 0.82652\n",
      "Train Epoch: 0 [10800/51892 (21%)]\tLoss: BinaryCrossEntropyLoss 0.02773, SoftDiceLoss 0.77934, SumLoss 0.80707\n",
      "Train Epoch: 0 [11040/51892 (21%)]\tLoss: BinaryCrossEntropyLoss 0.02133, SoftDiceLoss 0.70087, SumLoss 0.72220\n",
      "Train Epoch: 0 [11280/51892 (22%)]\tLoss: BinaryCrossEntropyLoss 0.02299, SoftDiceLoss 0.81908, SumLoss 0.84207\n",
      "Train Epoch: 0 [11520/51892 (22%)]\tLoss: BinaryCrossEntropyLoss 0.01635, SoftDiceLoss 0.47284, SumLoss 0.48919\n",
      "Train Epoch: 0 [11760/51892 (23%)]\tLoss: BinaryCrossEntropyLoss 0.02154, SoftDiceLoss 0.52100, SumLoss 0.54254\n",
      "Train Epoch: 0 [12000/51892 (23%)]\tLoss: BinaryCrossEntropyLoss 0.04210, SoftDiceLoss 0.44183, SumLoss 0.48393\n",
      "Train Epoch: 0 [12240/51892 (24%)]\tLoss: BinaryCrossEntropyLoss 0.02674, SoftDiceLoss 0.52181, SumLoss 0.54856\n",
      "Train Epoch: 0 [12480/51892 (24%)]\tLoss: BinaryCrossEntropyLoss 0.03384, SoftDiceLoss 0.76251, SumLoss 0.79636\n",
      "Train Epoch: 0 [12720/51892 (25%)]\tLoss: BinaryCrossEntropyLoss 0.02657, SoftDiceLoss 0.72646, SumLoss 0.75303\n",
      "Train Epoch: 0 [12960/51892 (25%)]\tLoss: BinaryCrossEntropyLoss 0.02201, SoftDiceLoss 0.64353, SumLoss 0.66555\n",
      "Train Epoch: 0 [13200/51892 (25%)]\tLoss: BinaryCrossEntropyLoss 0.02359, SoftDiceLoss 0.64042, SumLoss 0.66400\n",
      "Train Epoch: 0 [13440/51892 (26%)]\tLoss: BinaryCrossEntropyLoss 0.02148, SoftDiceLoss 0.61521, SumLoss 0.63669\n",
      "Train Epoch: 0 [13680/51892 (26%)]\tLoss: BinaryCrossEntropyLoss 0.02149, SoftDiceLoss 0.57564, SumLoss 0.59713\n",
      "Train Epoch: 0 [13920/51892 (27%)]\tLoss: BinaryCrossEntropyLoss 0.01569, SoftDiceLoss 0.60157, SumLoss 0.61726\n",
      "Train Epoch: 0 [14160/51892 (27%)]\tLoss: BinaryCrossEntropyLoss 0.01390, SoftDiceLoss 0.57545, SumLoss 0.58935\n",
      "Train Epoch: 0 [14400/51892 (28%)]\tLoss: BinaryCrossEntropyLoss 0.01381, SoftDiceLoss 0.54656, SumLoss 0.56037\n",
      "Train Epoch: 0 [14640/51892 (28%)]\tLoss: BinaryCrossEntropyLoss 0.01437, SoftDiceLoss 0.45759, SumLoss 0.47195\n",
      "Train Epoch: 0 [14880/51892 (29%)]\tLoss: BinaryCrossEntropyLoss 0.01768, SoftDiceLoss 0.57000, SumLoss 0.58768\n",
      "Train Epoch: 0 [15120/51892 (29%)]\tLoss: BinaryCrossEntropyLoss 0.01316, SoftDiceLoss 0.48821, SumLoss 0.50136\n",
      "Train Epoch: 0 [15360/51892 (30%)]\tLoss: BinaryCrossEntropyLoss 0.01792, SoftDiceLoss 0.30007, SumLoss 0.31799\n",
      "Train Epoch: 0 [15600/51892 (30%)]\tLoss: BinaryCrossEntropyLoss 0.01647, SoftDiceLoss 0.50539, SumLoss 0.52186\n",
      "Train Epoch: 0 [15840/51892 (31%)]\tLoss: BinaryCrossEntropyLoss 0.02654, SoftDiceLoss 0.67178, SumLoss 0.69832\n",
      "Train Epoch: 0 [16080/51892 (31%)]\tLoss: BinaryCrossEntropyLoss 0.01038, SoftDiceLoss 0.48638, SumLoss 0.49676\n",
      "Train Epoch: 0 [16320/51892 (31%)]\tLoss: BinaryCrossEntropyLoss 0.00939, SoftDiceLoss 0.41839, SumLoss 0.42779\n",
      "Train Epoch: 0 [16560/51892 (32%)]\tLoss: BinaryCrossEntropyLoss 0.01992, SoftDiceLoss 0.55635, SumLoss 0.57627\n",
      "Train Epoch: 0 [16800/51892 (32%)]\tLoss: BinaryCrossEntropyLoss 0.02346, SoftDiceLoss 0.51338, SumLoss 0.53683\n",
      "Train Epoch: 0 [17040/51892 (33%)]\tLoss: BinaryCrossEntropyLoss 0.01659, SoftDiceLoss 0.46133, SumLoss 0.47793\n",
      "Train Epoch: 0 [17280/51892 (33%)]\tLoss: BinaryCrossEntropyLoss 0.01631, SoftDiceLoss 0.42042, SumLoss 0.43673\n",
      "Train Epoch: 0 [17520/51892 (34%)]\tLoss: BinaryCrossEntropyLoss 0.01887, SoftDiceLoss 0.40819, SumLoss 0.42706\n",
      "Train Epoch: 0 [17760/51892 (34%)]\tLoss: BinaryCrossEntropyLoss 0.01354, SoftDiceLoss 0.38540, SumLoss 0.39894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [18000/51892 (35%)]\tLoss: BinaryCrossEntropyLoss 0.02332, SoftDiceLoss 0.58685, SumLoss 0.61017\n",
      "Train Epoch: 0 [18240/51892 (35%)]\tLoss: BinaryCrossEntropyLoss 0.01387, SoftDiceLoss 0.46283, SumLoss 0.47669\n",
      "Train Epoch: 0 [18480/51892 (36%)]\tLoss: BinaryCrossEntropyLoss 0.01410, SoftDiceLoss 0.41866, SumLoss 0.43275\n",
      "Train Epoch: 0 [18720/51892 (36%)]\tLoss: BinaryCrossEntropyLoss 0.01430, SoftDiceLoss 0.38916, SumLoss 0.40347\n",
      "Train Epoch: 0 [18960/51892 (37%)]\tLoss: BinaryCrossEntropyLoss 0.01893, SoftDiceLoss 0.42239, SumLoss 0.44132\n",
      "Train Epoch: 0 [19200/51892 (37%)]\tLoss: BinaryCrossEntropyLoss 0.02899, SoftDiceLoss 0.56868, SumLoss 0.59767\n",
      "Train Epoch: 0 [19440/51892 (37%)]\tLoss: BinaryCrossEntropyLoss 0.01999, SoftDiceLoss 0.46955, SumLoss 0.48954\n",
      "Train Epoch: 0 [19680/51892 (38%)]\tLoss: BinaryCrossEntropyLoss 0.01616, SoftDiceLoss 0.38993, SumLoss 0.40609\n",
      "Train Epoch: 0 [19920/51892 (38%)]\tLoss: BinaryCrossEntropyLoss 0.01524, SoftDiceLoss 0.37654, SumLoss 0.39178\n",
      "Train Epoch: 0 [20160/51892 (39%)]\tLoss: BinaryCrossEntropyLoss 0.02180, SoftDiceLoss 0.49010, SumLoss 0.51190\n",
      "Train Epoch: 0 [20400/51892 (39%)]\tLoss: BinaryCrossEntropyLoss 0.01472, SoftDiceLoss 0.36008, SumLoss 0.37480\n",
      "Train Epoch: 0 [20640/51892 (40%)]\tLoss: BinaryCrossEntropyLoss 0.01457, SoftDiceLoss 0.32346, SumLoss 0.33804\n",
      "Train Epoch: 0 [20880/51892 (40%)]\tLoss: BinaryCrossEntropyLoss 0.01181, SoftDiceLoss 0.27269, SumLoss 0.28450\n",
      "Train Epoch: 0 [21120/51892 (41%)]\tLoss: BinaryCrossEntropyLoss 0.01265, SoftDiceLoss 0.28232, SumLoss 0.29497\n",
      "Train Epoch: 0 [21360/51892 (41%)]\tLoss: BinaryCrossEntropyLoss 0.01257, SoftDiceLoss 0.28065, SumLoss 0.29322\n",
      "Train Epoch: 0 [21600/51892 (42%)]\tLoss: BinaryCrossEntropyLoss 0.01212, SoftDiceLoss 0.24518, SumLoss 0.25730\n",
      "Train Epoch: 0 [21840/51892 (42%)]\tLoss: BinaryCrossEntropyLoss 0.01083, SoftDiceLoss 0.17103, SumLoss 0.18187\n",
      "Train Epoch: 0 [22080/51892 (43%)]\tLoss: BinaryCrossEntropyLoss 0.01278, SoftDiceLoss 0.26018, SumLoss 0.27297\n",
      "Train Epoch: 0 [22320/51892 (43%)]\tLoss: BinaryCrossEntropyLoss 0.01541, SoftDiceLoss 0.46962, SumLoss 0.48504\n",
      "Train Epoch: 0 [22560/51892 (43%)]\tLoss: BinaryCrossEntropyLoss 0.01191, SoftDiceLoss 0.36467, SumLoss 0.37658\n",
      "Train Epoch: 0 [22800/51892 (44%)]\tLoss: BinaryCrossEntropyLoss 0.01346, SoftDiceLoss 0.47147, SumLoss 0.48493\n",
      "Train Epoch: 0 [23040/51892 (44%)]\tLoss: BinaryCrossEntropyLoss 0.01323, SoftDiceLoss 0.37318, SumLoss 0.38641\n",
      "Train Epoch: 0 [23280/51892 (45%)]\tLoss: BinaryCrossEntropyLoss 0.01473, SoftDiceLoss 0.45657, SumLoss 0.47130\n",
      "Train Epoch: 0 [23520/51892 (45%)]\tLoss: BinaryCrossEntropyLoss 0.01084, SoftDiceLoss 0.34293, SumLoss 0.35377\n",
      "Train Epoch: 0 [23760/51892 (46%)]\tLoss: BinaryCrossEntropyLoss 0.01046, SoftDiceLoss 0.32481, SumLoss 0.33527\n",
      "Train Epoch: 0 [24000/51892 (46%)]\tLoss: BinaryCrossEntropyLoss 0.01360, SoftDiceLoss 0.36624, SumLoss 0.37984\n",
      "Train Epoch: 0 [24240/51892 (47%)]\tLoss: BinaryCrossEntropyLoss 0.01061, SoftDiceLoss 0.33236, SumLoss 0.34297\n",
      "Train Epoch: 0 [24480/51892 (47%)]\tLoss: BinaryCrossEntropyLoss 0.01516, SoftDiceLoss 0.38198, SumLoss 0.39713\n",
      "Train Epoch: 0 [24720/51892 (48%)]\tLoss: BinaryCrossEntropyLoss 0.01476, SoftDiceLoss 0.37573, SumLoss 0.39049\n",
      "Train Epoch: 0 [24960/51892 (48%)]\tLoss: BinaryCrossEntropyLoss 0.01178, SoftDiceLoss 0.36783, SumLoss 0.37961\n",
      "Train Epoch: 0 [25200/51892 (49%)]\tLoss: BinaryCrossEntropyLoss 0.01218, SoftDiceLoss 0.26071, SumLoss 0.27289\n",
      "Train Epoch: 0 [25440/51892 (49%)]\tLoss: BinaryCrossEntropyLoss 0.00908, SoftDiceLoss 0.31772, SumLoss 0.32681\n",
      "Train Epoch: 0 [25680/51892 (49%)]\tLoss: BinaryCrossEntropyLoss 0.01211, SoftDiceLoss 0.45804, SumLoss 0.47015\n",
      "Train Epoch: 0 [25920/51892 (50%)]\tLoss: BinaryCrossEntropyLoss 0.01250, SoftDiceLoss 0.36196, SumLoss 0.37446\n",
      "Train Epoch: 0 [26160/51892 (50%)]\tLoss: BinaryCrossEntropyLoss 0.01050, SoftDiceLoss 0.21268, SumLoss 0.22318\n",
      "Train Epoch: 0 [26400/51892 (51%)]\tLoss: BinaryCrossEntropyLoss 0.01259, SoftDiceLoss 0.26246, SumLoss 0.27505\n",
      "Train Epoch: 0 [26640/51892 (51%)]\tLoss: BinaryCrossEntropyLoss 0.00995, SoftDiceLoss 0.34184, SumLoss 0.35179\n",
      "Train Epoch: 0 [26880/51892 (52%)]\tLoss: BinaryCrossEntropyLoss 0.01283, SoftDiceLoss 0.44992, SumLoss 0.46276\n",
      "Train Epoch: 0 [27120/51892 (52%)]\tLoss: BinaryCrossEntropyLoss 0.01109, SoftDiceLoss 0.19475, SumLoss 0.20584\n",
      "Train Epoch: 0 [27360/51892 (53%)]\tLoss: BinaryCrossEntropyLoss 0.01218, SoftDiceLoss 0.35364, SumLoss 0.36583\n",
      "Train Epoch: 0 [27600/51892 (53%)]\tLoss: BinaryCrossEntropyLoss 0.01256, SoftDiceLoss 0.33792, SumLoss 0.35048\n",
      "Train Epoch: 0 [27840/51892 (54%)]\tLoss: BinaryCrossEntropyLoss 0.01055, SoftDiceLoss 0.29540, SumLoss 0.30595\n",
      "Train Epoch: 0 [28080/51892 (54%)]\tLoss: BinaryCrossEntropyLoss 0.01249, SoftDiceLoss 0.31561, SumLoss 0.32810\n",
      "Train Epoch: 0 [28320/51892 (55%)]\tLoss: BinaryCrossEntropyLoss 0.03035, SoftDiceLoss 0.61982, SumLoss 0.65017\n",
      "Train Epoch: 0 [28560/51892 (55%)]\tLoss: BinaryCrossEntropyLoss 0.01873, SoftDiceLoss 0.37274, SumLoss 0.39148\n",
      "Train Epoch: 0 [28800/51892 (55%)]\tLoss: BinaryCrossEntropyLoss 0.02051, SoftDiceLoss 0.54251, SumLoss 0.56302\n",
      "Train Epoch: 0 [29040/51892 (56%)]\tLoss: BinaryCrossEntropyLoss 0.01580, SoftDiceLoss 0.38937, SumLoss 0.40517\n",
      "Train Epoch: 0 [29280/51892 (56%)]\tLoss: BinaryCrossEntropyLoss 0.01835, SoftDiceLoss 0.41027, SumLoss 0.42862\n",
      "Train Epoch: 0 [29520/51892 (57%)]\tLoss: BinaryCrossEntropyLoss 0.02034, SoftDiceLoss 0.42345, SumLoss 0.44379\n",
      "Train Epoch: 0 [29760/51892 (57%)]\tLoss: BinaryCrossEntropyLoss 0.01733, SoftDiceLoss 0.37559, SumLoss 0.39293\n",
      "Train Epoch: 0 [30000/51892 (58%)]\tLoss: BinaryCrossEntropyLoss 0.01596, SoftDiceLoss 0.35997, SumLoss 0.37593\n",
      "Train Epoch: 0 [30240/51892 (58%)]\tLoss: BinaryCrossEntropyLoss 0.01672, SoftDiceLoss 0.38639, SumLoss 0.40311\n",
      "Train Epoch: 0 [30480/51892 (59%)]\tLoss: BinaryCrossEntropyLoss 0.01365, SoftDiceLoss 0.29353, SumLoss 0.30718\n",
      "Train Epoch: 0 [30720/51892 (59%)]\tLoss: BinaryCrossEntropyLoss 0.01073, SoftDiceLoss 0.19575, SumLoss 0.20648\n",
      "Train Epoch: 0 [30960/51892 (60%)]\tLoss: BinaryCrossEntropyLoss 0.00996, SoftDiceLoss 0.19538, SumLoss 0.20535\n",
      "Train Epoch: 0 [31200/51892 (60%)]\tLoss: BinaryCrossEntropyLoss 0.01478, SoftDiceLoss -0.07479, SumLoss -0.06001\n",
      "Train Epoch: 0 [31440/51892 (61%)]\tLoss: BinaryCrossEntropyLoss 0.01953, SoftDiceLoss 0.24333, SumLoss 0.26286\n",
      "Train Epoch: 0 [31680/51892 (61%)]\tLoss: BinaryCrossEntropyLoss 0.03137, SoftDiceLoss 0.53801, SumLoss 0.56938\n",
      "Train Epoch: 0 [31920/51892 (61%)]\tLoss: BinaryCrossEntropyLoss 0.06256, SoftDiceLoss 0.98651, SumLoss 1.04907\n",
      "Train Epoch: 0 [32160/51892 (62%)]\tLoss: BinaryCrossEntropyLoss 0.05571, SoftDiceLoss 0.88379, SumLoss 0.93950\n",
      "Train Epoch: 0 [32400/51892 (62%)]\tLoss: BinaryCrossEntropyLoss 0.05488, SoftDiceLoss 0.86661, SumLoss 0.92149\n",
      "Train Epoch: 0 [32640/51892 (63%)]\tLoss: BinaryCrossEntropyLoss 0.02305, SoftDiceLoss 0.48973, SumLoss 0.51278\n",
      "Train Epoch: 0 [32880/51892 (63%)]\tLoss: BinaryCrossEntropyLoss 0.02638, SoftDiceLoss 0.44056, SumLoss 0.46694\n",
      "Train Epoch: 0 [33120/51892 (64%)]\tLoss: BinaryCrossEntropyLoss 0.01486, SoftDiceLoss 0.44671, SumLoss 0.46157\n",
      "Train Epoch: 0 [33360/51892 (64%)]\tLoss: BinaryCrossEntropyLoss 0.01374, SoftDiceLoss 0.39148, SumLoss 0.40522\n",
      "Train Epoch: 0 [33600/51892 (65%)]\tLoss: BinaryCrossEntropyLoss 0.02744, SoftDiceLoss 0.53394, SumLoss 0.56138\n",
      "Train Epoch: 0 [33840/51892 (65%)]\tLoss: BinaryCrossEntropyLoss 0.01153, SoftDiceLoss 0.34700, SumLoss 0.35853\n",
      "Train Epoch: 0 [34080/51892 (66%)]\tLoss: BinaryCrossEntropyLoss 0.01044, SoftDiceLoss 0.37267, SumLoss 0.38311\n",
      "Train Epoch: 0 [34320/51892 (66%)]\tLoss: BinaryCrossEntropyLoss 0.01860, SoftDiceLoss 0.18374, SumLoss 0.20234\n",
      "Train Epoch: 0 [34560/51892 (67%)]\tLoss: BinaryCrossEntropyLoss 0.02409, SoftDiceLoss 0.37664, SumLoss 0.40073\n",
      "Train Epoch: 0 [34800/51892 (67%)]\tLoss: BinaryCrossEntropyLoss 0.01445, SoftDiceLoss 0.39770, SumLoss 0.41215\n",
      "Train Epoch: 0 [35040/51892 (67%)]\tLoss: BinaryCrossEntropyLoss 0.01237, SoftDiceLoss 0.35052, SumLoss 0.36289\n",
      "Train Epoch: 0 [35280/51892 (68%)]\tLoss: BinaryCrossEntropyLoss 0.02093, SoftDiceLoss 0.47887, SumLoss 0.49980\n",
      "Train Epoch: 0 [35520/51892 (68%)]\tLoss: BinaryCrossEntropyLoss 0.01748, SoftDiceLoss 0.45853, SumLoss 0.47602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [35760/51892 (69%)]\tLoss: BinaryCrossEntropyLoss 0.01567, SoftDiceLoss 0.38762, SumLoss 0.40329\n",
      "Train Epoch: 0 [36000/51892 (69%)]\tLoss: BinaryCrossEntropyLoss 0.01986, SoftDiceLoss 0.25048, SumLoss 0.27034\n",
      "Train Epoch: 0 [36240/51892 (70%)]\tLoss: BinaryCrossEntropyLoss 0.02184, SoftDiceLoss 0.48064, SumLoss 0.50248\n",
      "Train Epoch: 0 [36480/51892 (70%)]\tLoss: BinaryCrossEntropyLoss 0.02291, SoftDiceLoss 0.48645, SumLoss 0.50935\n",
      "Train Epoch: 0 [36720/51892 (71%)]\tLoss: BinaryCrossEntropyLoss 0.02021, SoftDiceLoss 0.51446, SumLoss 0.53467\n",
      "Train Epoch: 0 [36960/51892 (71%)]\tLoss: BinaryCrossEntropyLoss 0.02039, SoftDiceLoss 0.44243, SumLoss 0.46282\n",
      "Train Epoch: 0 [37200/51892 (72%)]\tLoss: BinaryCrossEntropyLoss 0.02842, SoftDiceLoss 0.70437, SumLoss 0.73279\n",
      "Train Epoch: 0 [37440/51892 (72%)]\tLoss: BinaryCrossEntropyLoss 0.02506, SoftDiceLoss 0.75230, SumLoss 0.77736\n",
      "Train Epoch: 0 [37680/51892 (73%)]\tLoss: BinaryCrossEntropyLoss 0.01874, SoftDiceLoss 0.63318, SumLoss 0.65192\n",
      "Train Epoch: 0 [37920/51892 (73%)]\tLoss: BinaryCrossEntropyLoss 0.01653, SoftDiceLoss 0.59128, SumLoss 0.60781\n",
      "Train Epoch: 0 [38160/51892 (74%)]\tLoss: BinaryCrossEntropyLoss 0.02483, SoftDiceLoss 0.63971, SumLoss 0.66454\n",
      "Train Epoch: 0 [38400/51892 (74%)]\tLoss: BinaryCrossEntropyLoss 0.01688, SoftDiceLoss 0.39134, SumLoss 0.40822\n",
      "Train Epoch: 0 [38640/51892 (74%)]\tLoss: BinaryCrossEntropyLoss 0.01351, SoftDiceLoss 0.31397, SumLoss 0.32748\n",
      "Train Epoch: 0 [38880/51892 (75%)]\tLoss: BinaryCrossEntropyLoss 0.01486, SoftDiceLoss 0.33760, SumLoss 0.35246\n",
      "Train Epoch: 0 [39120/51892 (75%)]\tLoss: BinaryCrossEntropyLoss 0.01431, SoftDiceLoss 0.46673, SumLoss 0.48104\n",
      "Train Epoch: 0 [39360/51892 (76%)]\tLoss: BinaryCrossEntropyLoss 0.01185, SoftDiceLoss 0.29554, SumLoss 0.30740\n",
      "Train Epoch: 0 [39600/51892 (76%)]\tLoss: BinaryCrossEntropyLoss 0.00856, SoftDiceLoss 0.24954, SumLoss 0.25811\n",
      "Train Epoch: 0 [39840/51892 (77%)]\tLoss: BinaryCrossEntropyLoss 0.00986, SoftDiceLoss 0.31358, SumLoss 0.32344\n",
      "Train Epoch: 0 [40080/51892 (77%)]\tLoss: BinaryCrossEntropyLoss 0.00752, SoftDiceLoss 0.28725, SumLoss 0.29477\n",
      "Train Epoch: 0 [40320/51892 (78%)]\tLoss: BinaryCrossEntropyLoss 0.02057, SoftDiceLoss 0.14861, SumLoss 0.16918\n",
      "Train Epoch: 0 [40560/51892 (78%)]\tLoss: BinaryCrossEntropyLoss 0.01582, SoftDiceLoss 0.32581, SumLoss 0.34164\n",
      "Train Epoch: 0 [40800/51892 (79%)]\tLoss: BinaryCrossEntropyLoss 0.01304, SoftDiceLoss 0.32130, SumLoss 0.33433\n",
      "Train Epoch: 0 [41040/51892 (79%)]\tLoss: BinaryCrossEntropyLoss 0.01271, SoftDiceLoss 0.27278, SumLoss 0.28549\n",
      "Train Epoch: 0 [41280/51892 (80%)]\tLoss: BinaryCrossEntropyLoss 0.01389, SoftDiceLoss 0.26005, SumLoss 0.27394\n",
      "Train Epoch: 0 [41520/51892 (80%)]\tLoss: BinaryCrossEntropyLoss 0.02251, SoftDiceLoss 0.36347, SumLoss 0.38598\n",
      "Train Epoch: 0 [41760/51892 (80%)]\tLoss: BinaryCrossEntropyLoss 0.02185, SoftDiceLoss 0.25181, SumLoss 0.27365\n",
      "Train Epoch: 0 [42000/51892 (81%)]\tLoss: BinaryCrossEntropyLoss 0.02109, SoftDiceLoss 0.32119, SumLoss 0.34228\n",
      "Train Epoch: 0 [42240/51892 (81%)]\tLoss: BinaryCrossEntropyLoss 0.02178, SoftDiceLoss 0.33646, SumLoss 0.35824\n",
      "Train Epoch: 0 [42480/51892 (82%)]\tLoss: BinaryCrossEntropyLoss 0.01453, SoftDiceLoss 0.36865, SumLoss 0.38318\n",
      "Train Epoch: 0 [42720/51892 (82%)]\tLoss: BinaryCrossEntropyLoss 0.01297, SoftDiceLoss 0.32219, SumLoss 0.33517\n",
      "Train Epoch: 0 [42960/51892 (83%)]\tLoss: BinaryCrossEntropyLoss 0.01217, SoftDiceLoss 0.30390, SumLoss 0.31607\n",
      "Train Epoch: 0 [43200/51892 (83%)]\tLoss: BinaryCrossEntropyLoss 0.01035, SoftDiceLoss 0.28855, SumLoss 0.29890\n",
      "Train Epoch: 0 [43440/51892 (84%)]\tLoss: BinaryCrossEntropyLoss 0.01980, SoftDiceLoss 0.60899, SumLoss 0.62879\n",
      "Train Epoch: 0 [43680/51892 (84%)]\tLoss: BinaryCrossEntropyLoss 0.01239, SoftDiceLoss 0.27504, SumLoss 0.28743\n",
      "Train Epoch: 0 [43920/51892 (85%)]\tLoss: BinaryCrossEntropyLoss 0.01619, SoftDiceLoss 0.28806, SumLoss 0.30425\n",
      "Train Epoch: 0 [44160/51892 (85%)]\tLoss: BinaryCrossEntropyLoss 0.01403, SoftDiceLoss 0.28873, SumLoss 0.30276\n",
      "Train Epoch: 0 [44400/51892 (86%)]\tLoss: BinaryCrossEntropyLoss 0.01224, SoftDiceLoss 0.18585, SumLoss 0.19809\n",
      "Train Epoch: 0 [44640/51892 (86%)]\tLoss: BinaryCrossEntropyLoss 0.01506, SoftDiceLoss 0.26015, SumLoss 0.27521\n",
      "Train Epoch: 0 [44880/51892 (86%)]\tLoss: BinaryCrossEntropyLoss 0.01288, SoftDiceLoss 0.21003, SumLoss 0.22290\n",
      "Train Epoch: 0 [45120/51892 (87%)]\tLoss: BinaryCrossEntropyLoss 0.01142, SoftDiceLoss 0.20975, SumLoss 0.22117\n",
      "Train Epoch: 0 [45360/51892 (87%)]\tLoss: BinaryCrossEntropyLoss 0.01489, SoftDiceLoss 0.30208, SumLoss 0.31697\n",
      "Train Epoch: 0 [45600/51892 (88%)]\tLoss: BinaryCrossEntropyLoss 0.01193, SoftDiceLoss 0.23135, SumLoss 0.24328\n",
      "Train Epoch: 0 [45840/51892 (88%)]\tLoss: BinaryCrossEntropyLoss 0.01343, SoftDiceLoss 0.23236, SumLoss 0.24580\n",
      "Train Epoch: 0 [46080/51892 (89%)]\tLoss: BinaryCrossEntropyLoss 0.00962, SoftDiceLoss 0.19604, SumLoss 0.20566\n",
      "Train Epoch: 0 [46320/51892 (89%)]\tLoss: BinaryCrossEntropyLoss 0.00991, SoftDiceLoss 0.23407, SumLoss 0.24397\n",
      "Train Epoch: 0 [46560/51892 (90%)]\tLoss: BinaryCrossEntropyLoss 0.01101, SoftDiceLoss 0.25471, SumLoss 0.26572\n",
      "Train Epoch: 0 [46800/51892 (90%)]\tLoss: BinaryCrossEntropyLoss 0.02248, SoftDiceLoss 0.30716, SumLoss 0.32964\n",
      "Train Epoch: 0 [47040/51892 (91%)]\tLoss: BinaryCrossEntropyLoss 0.01265, SoftDiceLoss 0.24260, SumLoss 0.25525\n",
      "Train Epoch: 0 [47280/51892 (91%)]\tLoss: BinaryCrossEntropyLoss 0.01797, SoftDiceLoss 0.31969, SumLoss 0.33766\n",
      "Train Epoch: 0 [47520/51892 (92%)]\tLoss: BinaryCrossEntropyLoss 0.01113, SoftDiceLoss 0.24841, SumLoss 0.25954\n",
      "Train Epoch: 0 [47760/51892 (92%)]\tLoss: BinaryCrossEntropyLoss 0.01425, SoftDiceLoss 0.24607, SumLoss 0.26032\n",
      "Train Epoch: 0 [48000/51892 (92%)]\tLoss: BinaryCrossEntropyLoss 0.01118, SoftDiceLoss 0.26066, SumLoss 0.27184\n",
      "Train Epoch: 0 [48240/51892 (93%)]\tLoss: BinaryCrossEntropyLoss 0.00918, SoftDiceLoss 0.17941, SumLoss 0.18859\n",
      "Train Epoch: 0 [48480/51892 (93%)]\tLoss: BinaryCrossEntropyLoss 0.01133, SoftDiceLoss 0.31095, SumLoss 0.32229\n",
      "Train Epoch: 0 [48720/51892 (94%)]\tLoss: BinaryCrossEntropyLoss 0.01683, SoftDiceLoss 0.32180, SumLoss 0.33863\n",
      "Train Epoch: 0 [48960/51892 (94%)]\tLoss: BinaryCrossEntropyLoss 0.01407, SoftDiceLoss 0.28240, SumLoss 0.29646\n",
      "Train Epoch: 0 [49200/51892 (95%)]\tLoss: BinaryCrossEntropyLoss 0.01318, SoftDiceLoss 0.31208, SumLoss 0.32525\n",
      "Train Epoch: 0 [49440/51892 (95%)]\tLoss: BinaryCrossEntropyLoss 0.00985, SoftDiceLoss 0.22361, SumLoss 0.23346\n",
      "Train Epoch: 0 [49680/51892 (96%)]\tLoss: BinaryCrossEntropyLoss 0.01254, SoftDiceLoss 0.25306, SumLoss 0.26560\n",
      "Train Epoch: 0 [49920/51892 (96%)]\tLoss: BinaryCrossEntropyLoss 0.01518, SoftDiceLoss 0.26585, SumLoss 0.28104\n",
      "Train Epoch: 0 [50160/51892 (97%)]\tLoss: BinaryCrossEntropyLoss 0.01603, SoftDiceLoss 0.32882, SumLoss 0.34485\n",
      "Train Epoch: 0 [50400/51892 (97%)]\tLoss: BinaryCrossEntropyLoss 0.02101, SoftDiceLoss 0.37572, SumLoss 0.39673\n",
      "Train Epoch: 0 [50640/51892 (98%)]\tLoss: BinaryCrossEntropyLoss 0.02010, SoftDiceLoss 0.25809, SumLoss 0.27819\n",
      "Train Epoch: 0 [50880/51892 (98%)]\tLoss: BinaryCrossEntropyLoss 0.01843, SoftDiceLoss 0.35343, SumLoss 0.37185\n",
      "Train Epoch: 0 [51120/51892 (98%)]\tLoss: BinaryCrossEntropyLoss 0.02006, SoftDiceLoss 0.36581, SumLoss 0.38587\n",
      "Train Epoch: 0 [51360/51892 (99%)]\tLoss: BinaryCrossEntropyLoss 0.01929, SoftDiceLoss 0.36408, SumLoss 0.38336\n",
      "Train Epoch: 0 [51600/51892 (99%)]\tLoss: BinaryCrossEntropyLoss 0.02221, SoftDiceLoss 0.39353, SumLoss 0.41574\n",
      "Train Epoch: 0 [51840/51892 (100%)]\tLoss: BinaryCrossEntropyLoss 0.01799, SoftDiceLoss 0.35310, SumLoss 0.37109\n",
      "Loss on validation: 0.355193781436\n",
      "Train Epoch: 1 [0/51892 (0%)]\tLoss: BinaryCrossEntropyLoss 0.01441, SoftDiceLoss 0.24636, SumLoss 0.26077\n",
      "Train Epoch: 1 [240/51892 (0%)]\tLoss: BinaryCrossEntropyLoss 0.01019, SoftDiceLoss 0.16195, SumLoss 0.17214\n",
      "Train Epoch: 1 [480/51892 (1%)]\tLoss: BinaryCrossEntropyLoss 0.01645, SoftDiceLoss 0.26562, SumLoss 0.28208\n",
      "Train Epoch: 1 [720/51892 (1%)]\tLoss: BinaryCrossEntropyLoss 0.01676, SoftDiceLoss 0.53314, SumLoss 0.54990\n",
      "Train Epoch: 1 [960/51892 (2%)]\tLoss: BinaryCrossEntropyLoss 0.01008, SoftDiceLoss 0.24964, SumLoss 0.25972\n",
      "Train Epoch: 1 [1200/51892 (2%)]\tLoss: BinaryCrossEntropyLoss 0.00996, SoftDiceLoss 0.25192, SumLoss 0.26189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [1440/51892 (3%)]\tLoss: BinaryCrossEntropyLoss 0.00964, SoftDiceLoss 0.22965, SumLoss 0.23929\n",
      "Train Epoch: 1 [1680/51892 (3%)]\tLoss: BinaryCrossEntropyLoss 0.00881, SoftDiceLoss 0.22407, SumLoss 0.23288\n",
      "Train Epoch: 1 [1920/51892 (4%)]\tLoss: BinaryCrossEntropyLoss 0.00945, SoftDiceLoss 0.20083, SumLoss 0.21028\n",
      "Train Epoch: 1 [2160/51892 (4%)]\tLoss: BinaryCrossEntropyLoss 0.00829, SoftDiceLoss 0.22338, SumLoss 0.23167\n",
      "Train Epoch: 1 [2400/51892 (5%)]\tLoss: BinaryCrossEntropyLoss 0.00945, SoftDiceLoss 0.22617, SumLoss 0.23562\n",
      "Train Epoch: 1 [2640/51892 (5%)]\tLoss: BinaryCrossEntropyLoss 0.01580, SoftDiceLoss 0.32443, SumLoss 0.34023\n",
      "Train Epoch: 1 [2880/51892 (6%)]\tLoss: BinaryCrossEntropyLoss 0.01536, SoftDiceLoss 0.32954, SumLoss 0.34490\n",
      "Train Epoch: 1 [3120/51892 (6%)]\tLoss: BinaryCrossEntropyLoss 0.02943, SoftDiceLoss 0.62725, SumLoss 0.65668\n",
      "Train Epoch: 1 [3360/51892 (6%)]\tLoss: BinaryCrossEntropyLoss 0.01048, SoftDiceLoss 0.25419, SumLoss 0.26467\n",
      "Train Epoch: 1 [3600/51892 (7%)]\tLoss: BinaryCrossEntropyLoss 0.00910, SoftDiceLoss 0.26599, SumLoss 0.27508\n",
      "Train Epoch: 1 [3840/51892 (7%)]\tLoss: BinaryCrossEntropyLoss 0.02011, SoftDiceLoss 0.36935, SumLoss 0.38945\n",
      "Train Epoch: 1 [4080/51892 (8%)]\tLoss: BinaryCrossEntropyLoss 0.01571, SoftDiceLoss 0.29666, SumLoss 0.31237\n",
      "Train Epoch: 1 [4320/51892 (8%)]\tLoss: BinaryCrossEntropyLoss 0.01852, SoftDiceLoss 0.19187, SumLoss 0.21038\n",
      "Train Epoch: 1 [4560/51892 (9%)]\tLoss: BinaryCrossEntropyLoss 0.02167, SoftDiceLoss 0.25615, SumLoss 0.27782\n",
      "Train Epoch: 1 [4800/51892 (9%)]\tLoss: BinaryCrossEntropyLoss 0.01471, SoftDiceLoss 0.13436, SumLoss 0.14907\n",
      "Train Epoch: 1 [5040/51892 (10%)]\tLoss: BinaryCrossEntropyLoss 0.02763, SoftDiceLoss 0.47984, SumLoss 0.50748\n",
      "Train Epoch: 1 [5280/51892 (10%)]\tLoss: BinaryCrossEntropyLoss 0.02202, SoftDiceLoss 0.29301, SumLoss 0.31502\n",
      "Train Epoch: 1 [5520/51892 (11%)]\tLoss: BinaryCrossEntropyLoss 0.01973, SoftDiceLoss 0.28591, SumLoss 0.30564\n",
      "Train Epoch: 1 [5760/51892 (11%)]\tLoss: BinaryCrossEntropyLoss 0.02093, SoftDiceLoss 0.35446, SumLoss 0.37540\n",
      "Train Epoch: 1 [6000/51892 (12%)]\tLoss: BinaryCrossEntropyLoss 0.01669, SoftDiceLoss 0.28548, SumLoss 0.30217\n",
      "Train Epoch: 1 [6240/51892 (12%)]\tLoss: BinaryCrossEntropyLoss 0.01423, SoftDiceLoss 0.18040, SumLoss 0.19463\n",
      "Train Epoch: 1 [6480/51892 (12%)]\tLoss: BinaryCrossEntropyLoss 0.01595, SoftDiceLoss 0.25199, SumLoss 0.26793\n",
      "Train Epoch: 1 [6720/51892 (13%)]\tLoss: BinaryCrossEntropyLoss 0.02436, SoftDiceLoss 0.27187, SumLoss 0.29623\n",
      "Train Epoch: 1 [6960/51892 (13%)]\tLoss: BinaryCrossEntropyLoss 0.01053, SoftDiceLoss 0.23525, SumLoss 0.24578\n",
      "Train Epoch: 1 [7200/51892 (14%)]\tLoss: BinaryCrossEntropyLoss 0.01122, SoftDiceLoss 0.22796, SumLoss 0.23918\n",
      "Train Epoch: 1 [7440/51892 (14%)]\tLoss: BinaryCrossEntropyLoss 0.01170, SoftDiceLoss 0.07270, SumLoss 0.08440\n",
      "Train Epoch: 1 [7680/51892 (15%)]\tLoss: BinaryCrossEntropyLoss 0.01291, SoftDiceLoss 0.29420, SumLoss 0.30711\n",
      "Train Epoch: 1 [7920/51892 (15%)]\tLoss: BinaryCrossEntropyLoss 0.01177, SoftDiceLoss 0.25583, SumLoss 0.26760\n",
      "Train Epoch: 1 [8160/51892 (16%)]\tLoss: BinaryCrossEntropyLoss 0.02317, SoftDiceLoss 0.31292, SumLoss 0.33609\n",
      "Train Epoch: 1 [8400/51892 (16%)]\tLoss: BinaryCrossEntropyLoss 0.01868, SoftDiceLoss 0.23177, SumLoss 0.25045\n",
      "Train Epoch: 1 [8640/51892 (17%)]\tLoss: BinaryCrossEntropyLoss 0.01620, SoftDiceLoss 0.25341, SumLoss 0.26961\n",
      "Train Epoch: 1 [8880/51892 (17%)]\tLoss: BinaryCrossEntropyLoss 0.00909, SoftDiceLoss 0.22261, SumLoss 0.23170\n",
      "Train Epoch: 1 [9120/51892 (18%)]\tLoss: BinaryCrossEntropyLoss 0.01402, SoftDiceLoss 0.24039, SumLoss 0.25441\n",
      "Train Epoch: 1 [9360/51892 (18%)]\tLoss: BinaryCrossEntropyLoss 0.00873, SoftDiceLoss 0.19572, SumLoss 0.20445\n",
      "Train Epoch: 1 [9600/51892 (18%)]\tLoss: BinaryCrossEntropyLoss 0.01153, SoftDiceLoss 0.21862, SumLoss 0.23015\n",
      "Train Epoch: 1 [9840/51892 (19%)]\tLoss: BinaryCrossEntropyLoss 0.00831, SoftDiceLoss 0.17758, SumLoss 0.18590\n",
      "Train Epoch: 1 [10080/51892 (19%)]\tLoss: BinaryCrossEntropyLoss 0.00874, SoftDiceLoss 0.19993, SumLoss 0.20867\n",
      "Train Epoch: 1 [10320/51892 (20%)]\tLoss: BinaryCrossEntropyLoss 0.01408, SoftDiceLoss 0.30153, SumLoss 0.31561\n",
      "Train Epoch: 1 [10560/51892 (20%)]\tLoss: BinaryCrossEntropyLoss 0.01152, SoftDiceLoss 0.27164, SumLoss 0.28316\n",
      "Train Epoch: 1 [10800/51892 (21%)]\tLoss: BinaryCrossEntropyLoss 0.01380, SoftDiceLoss 0.30734, SumLoss 0.32114\n",
      "Train Epoch: 1 [11040/51892 (21%)]\tLoss: BinaryCrossEntropyLoss 0.01072, SoftDiceLoss 0.19388, SumLoss 0.20460\n",
      "Train Epoch: 1 [11280/51892 (22%)]\tLoss: BinaryCrossEntropyLoss 0.01090, SoftDiceLoss 0.24252, SumLoss 0.25342\n",
      "Train Epoch: 1 [11520/51892 (22%)]\tLoss: BinaryCrossEntropyLoss 0.00885, SoftDiceLoss 0.01696, SumLoss 0.02581\n",
      "Train Epoch: 1 [11760/51892 (23%)]\tLoss: BinaryCrossEntropyLoss 0.01064, SoftDiceLoss 0.09583, SumLoss 0.10647\n",
      "Train Epoch: 1 [12000/51892 (23%)]\tLoss: BinaryCrossEntropyLoss 0.03039, SoftDiceLoss 0.22733, SumLoss 0.25772\n",
      "Train Epoch: 1 [12240/51892 (24%)]\tLoss: BinaryCrossEntropyLoss 0.01173, SoftDiceLoss 0.03725, SumLoss 0.04898\n",
      "Train Epoch: 1 [12480/51892 (24%)]\tLoss: BinaryCrossEntropyLoss 0.01494, SoftDiceLoss 0.32058, SumLoss 0.33552\n",
      "Train Epoch: 1 [12720/51892 (25%)]\tLoss: BinaryCrossEntropyLoss 0.01356, SoftDiceLoss 0.17434, SumLoss 0.18791\n",
      "Train Epoch: 1 [12960/51892 (25%)]\tLoss: BinaryCrossEntropyLoss 0.01262, SoftDiceLoss 0.20681, SumLoss 0.21942\n",
      "Train Epoch: 1 [13200/51892 (25%)]\tLoss: BinaryCrossEntropyLoss 0.01359, SoftDiceLoss 0.23334, SumLoss 0.24693\n",
      "Train Epoch: 1 [13440/51892 (26%)]\tLoss: BinaryCrossEntropyLoss 0.01282, SoftDiceLoss 0.18415, SumLoss 0.19697\n",
      "Train Epoch: 1 [13680/51892 (26%)]\tLoss: BinaryCrossEntropyLoss 0.01206, SoftDiceLoss 0.09980, SumLoss 0.11187\n",
      "Train Epoch: 1 [13920/51892 (27%)]\tLoss: BinaryCrossEntropyLoss 0.01265, SoftDiceLoss 0.26439, SumLoss 0.27704\n",
      "Train Epoch: 1 [14160/51892 (27%)]\tLoss: BinaryCrossEntropyLoss 0.01107, SoftDiceLoss 0.24932, SumLoss 0.26039\n",
      "Train Epoch: 1 [14400/51892 (28%)]\tLoss: BinaryCrossEntropyLoss 0.01166, SoftDiceLoss 0.26051, SumLoss 0.27217\n",
      "Train Epoch: 1 [14640/51892 (28%)]\tLoss: BinaryCrossEntropyLoss 0.01002, SoftDiceLoss 0.17790, SumLoss 0.18792\n",
      "Train Epoch: 1 [14880/51892 (29%)]\tLoss: BinaryCrossEntropyLoss 0.01162, SoftDiceLoss 0.22412, SumLoss 0.23574\n",
      "Train Epoch: 1 [15120/51892 (29%)]\tLoss: BinaryCrossEntropyLoss 0.01069, SoftDiceLoss 0.22358, SumLoss 0.23426\n",
      "Train Epoch: 1 [15360/51892 (30%)]\tLoss: BinaryCrossEntropyLoss 0.01473, SoftDiceLoss 0.07580, SumLoss 0.09053\n",
      "Train Epoch: 1 [15600/51892 (30%)]\tLoss: BinaryCrossEntropyLoss 0.00735, SoftDiceLoss 0.14448, SumLoss 0.15184\n",
      "Train Epoch: 1 [15840/51892 (31%)]\tLoss: BinaryCrossEntropyLoss 0.01060, SoftDiceLoss 0.26118, SumLoss 0.27177\n",
      "Train Epoch: 1 [16080/51892 (31%)]\tLoss: BinaryCrossEntropyLoss 0.00689, SoftDiceLoss 0.22379, SumLoss 0.23068\n",
      "Train Epoch: 1 [16320/51892 (31%)]\tLoss: BinaryCrossEntropyLoss 0.00777, SoftDiceLoss 0.22751, SumLoss 0.23529\n",
      "Train Epoch: 1 [16560/51892 (32%)]\tLoss: BinaryCrossEntropyLoss 0.01362, SoftDiceLoss 0.25811, SumLoss 0.27173\n",
      "Train Epoch: 1 [16800/51892 (32%)]\tLoss: BinaryCrossEntropyLoss 0.01619, SoftDiceLoss 0.24663, SumLoss 0.26282\n",
      "Train Epoch: 1 [17040/51892 (33%)]\tLoss: BinaryCrossEntropyLoss 0.01085, SoftDiceLoss 0.20069, SumLoss 0.21154\n",
      "Train Epoch: 1 [17280/51892 (33%)]\tLoss: BinaryCrossEntropyLoss 0.01366, SoftDiceLoss 0.20839, SumLoss 0.22205\n",
      "Train Epoch: 1 [17520/51892 (34%)]\tLoss: BinaryCrossEntropyLoss 0.01522, SoftDiceLoss 0.21769, SumLoss 0.23292\n",
      "Train Epoch: 1 [17760/51892 (34%)]\tLoss: BinaryCrossEntropyLoss 0.01107, SoftDiceLoss 0.18559, SumLoss 0.19665\n",
      "Train Epoch: 1 [18000/51892 (35%)]\tLoss: BinaryCrossEntropyLoss 0.01382, SoftDiceLoss 0.23976, SumLoss 0.25358\n",
      "Train Epoch: 1 [18240/51892 (35%)]\tLoss: BinaryCrossEntropyLoss 0.01037, SoftDiceLoss 0.20921, SumLoss 0.21958\n",
      "Train Epoch: 1 [18480/51892 (36%)]\tLoss: BinaryCrossEntropyLoss 0.01285, SoftDiceLoss 0.22535, SumLoss 0.23820\n",
      "Train Epoch: 1 [18720/51892 (36%)]\tLoss: BinaryCrossEntropyLoss 0.01256, SoftDiceLoss 0.21299, SumLoss 0.22555\n",
      "Train Epoch: 1 [18960/51892 (37%)]\tLoss: BinaryCrossEntropyLoss 0.01368, SoftDiceLoss 0.23889, SumLoss 0.25256\n",
      "Train Epoch: 1 [19200/51892 (37%)]\tLoss: BinaryCrossEntropyLoss 0.01653, SoftDiceLoss 0.26142, SumLoss 0.27794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [19440/51892 (37%)]\tLoss: BinaryCrossEntropyLoss 0.01243, SoftDiceLoss 0.24683, SumLoss 0.25926\n",
      "Train Epoch: 1 [19680/51892 (38%)]\tLoss: BinaryCrossEntropyLoss 0.01429, SoftDiceLoss 0.23671, SumLoss 0.25100\n",
      "Train Epoch: 1 [19920/51892 (38%)]\tLoss: BinaryCrossEntropyLoss 0.01246, SoftDiceLoss 0.22358, SumLoss 0.23604\n",
      "Train Epoch: 1 [20160/51892 (39%)]\tLoss: BinaryCrossEntropyLoss 0.01368, SoftDiceLoss 0.21736, SumLoss 0.23104\n",
      "Train Epoch: 1 [20400/51892 (39%)]\tLoss: BinaryCrossEntropyLoss 0.01300, SoftDiceLoss 0.22254, SumLoss 0.23554\n",
      "Train Epoch: 1 [20640/51892 (40%)]\tLoss: BinaryCrossEntropyLoss 0.01313, SoftDiceLoss 0.19274, SumLoss 0.20587\n",
      "Train Epoch: 1 [20880/51892 (40%)]\tLoss: BinaryCrossEntropyLoss 0.01230, SoftDiceLoss 0.18241, SumLoss 0.19471\n",
      "Train Epoch: 1 [21120/51892 (41%)]\tLoss: BinaryCrossEntropyLoss 0.01187, SoftDiceLoss 0.17443, SumLoss 0.18630\n",
      "Train Epoch: 1 [21360/51892 (41%)]\tLoss: BinaryCrossEntropyLoss 0.01351, SoftDiceLoss 0.19169, SumLoss 0.20519\n",
      "Train Epoch: 1 [21600/51892 (42%)]\tLoss: BinaryCrossEntropyLoss 0.01356, SoftDiceLoss 0.18184, SumLoss 0.19540\n",
      "Train Epoch: 1 [21840/51892 (42%)]\tLoss: BinaryCrossEntropyLoss 0.01147, SoftDiceLoss 0.09098, SumLoss 0.10245\n",
      "Train Epoch: 1 [22080/51892 (43%)]\tLoss: BinaryCrossEntropyLoss 0.01129, SoftDiceLoss 0.19160, SumLoss 0.20289\n",
      "Train Epoch: 1 [22320/51892 (43%)]\tLoss: BinaryCrossEntropyLoss 0.01919, SoftDiceLoss 0.40338, SumLoss 0.42257\n",
      "Train Epoch: 1 [22560/51892 (43%)]\tLoss: BinaryCrossEntropyLoss 0.01419, SoftDiceLoss 0.29665, SumLoss 0.31084\n",
      "Train Epoch: 1 [22800/51892 (44%)]\tLoss: BinaryCrossEntropyLoss 0.01038, SoftDiceLoss 0.27038, SumLoss 0.28076\n",
      "Train Epoch: 1 [23040/51892 (44%)]\tLoss: BinaryCrossEntropyLoss 0.01096, SoftDiceLoss 0.22120, SumLoss 0.23216\n",
      "Train Epoch: 1 [23280/51892 (45%)]\tLoss: BinaryCrossEntropyLoss 0.01142, SoftDiceLoss 0.20682, SumLoss 0.21823\n",
      "Train Epoch: 1 [23520/51892 (45%)]\tLoss: BinaryCrossEntropyLoss 0.01088, SoftDiceLoss 0.24437, SumLoss 0.25525\n",
      "Train Epoch: 1 [23760/51892 (46%)]\tLoss: BinaryCrossEntropyLoss 0.01079, SoftDiceLoss 0.22940, SumLoss 0.24019\n",
      "Train Epoch: 1 [24000/51892 (46%)]\tLoss: BinaryCrossEntropyLoss 0.01161, SoftDiceLoss 0.23863, SumLoss 0.25024\n",
      "Train Epoch: 1 [24240/51892 (47%)]\tLoss: BinaryCrossEntropyLoss 0.00877, SoftDiceLoss 0.20953, SumLoss 0.21830\n",
      "Train Epoch: 1 [24480/51892 (47%)]\tLoss: BinaryCrossEntropyLoss 0.01289, SoftDiceLoss 0.24960, SumLoss 0.26250\n",
      "Train Epoch: 1 [24720/51892 (48%)]\tLoss: BinaryCrossEntropyLoss 0.01203, SoftDiceLoss 0.21808, SumLoss 0.23011\n",
      "Train Epoch: 1 [24960/51892 (48%)]\tLoss: BinaryCrossEntropyLoss 0.01081, SoftDiceLoss 0.25763, SumLoss 0.26843\n",
      "Train Epoch: 1 [25200/51892 (49%)]\tLoss: BinaryCrossEntropyLoss 0.01133, SoftDiceLoss 0.17123, SumLoss 0.18256\n",
      "Train Epoch: 1 [25440/51892 (49%)]\tLoss: BinaryCrossEntropyLoss 0.00888, SoftDiceLoss 0.21303, SumLoss 0.22190\n",
      "Train Epoch: 1 [25680/51892 (49%)]\tLoss: BinaryCrossEntropyLoss 0.01176, SoftDiceLoss 0.36393, SumLoss 0.37569\n",
      "Train Epoch: 1 [25920/51892 (50%)]\tLoss: BinaryCrossEntropyLoss 0.01032, SoftDiceLoss 0.25394, SumLoss 0.26426\n",
      "Train Epoch: 1 [26160/51892 (50%)]\tLoss: BinaryCrossEntropyLoss 0.00897, SoftDiceLoss 0.13798, SumLoss 0.14695\n",
      "Train Epoch: 1 [26400/51892 (51%)]\tLoss: BinaryCrossEntropyLoss 0.01091, SoftDiceLoss 0.07917, SumLoss 0.09008\n",
      "Train Epoch: 1 [26640/51892 (51%)]\tLoss: BinaryCrossEntropyLoss 0.00840, SoftDiceLoss 0.17832, SumLoss 0.18672\n",
      "Train Epoch: 1 [26880/51892 (52%)]\tLoss: BinaryCrossEntropyLoss 0.00991, SoftDiceLoss 0.26126, SumLoss 0.27116\n",
      "Train Epoch: 1 [27120/51892 (52%)]\tLoss: BinaryCrossEntropyLoss 0.00916, SoftDiceLoss 0.09928, SumLoss 0.10843\n",
      "Train Epoch: 1 [27360/51892 (53%)]\tLoss: BinaryCrossEntropyLoss 0.01032, SoftDiceLoss 0.25443, SumLoss 0.26475\n",
      "Train Epoch: 1 [27600/51892 (53%)]\tLoss: BinaryCrossEntropyLoss 0.01014, SoftDiceLoss 0.23385, SumLoss 0.24399\n",
      "Train Epoch: 1 [27840/51892 (54%)]\tLoss: BinaryCrossEntropyLoss 0.00938, SoftDiceLoss 0.14228, SumLoss 0.15166\n",
      "Train Epoch: 1 [28080/51892 (54%)]\tLoss: BinaryCrossEntropyLoss 0.01066, SoftDiceLoss 0.14741, SumLoss 0.15807\n",
      "Train Epoch: 1 [28320/51892 (55%)]\tLoss: BinaryCrossEntropyLoss 0.01769, SoftDiceLoss 0.30064, SumLoss 0.31834\n",
      "Train Epoch: 1 [28560/51892 (55%)]\tLoss: BinaryCrossEntropyLoss 0.01655, SoftDiceLoss 0.25535, SumLoss 0.27191\n",
      "Train Epoch: 1 [28800/51892 (55%)]\tLoss: BinaryCrossEntropyLoss 0.01176, SoftDiceLoss 0.26132, SumLoss 0.27308\n",
      "Train Epoch: 1 [29040/51892 (56%)]\tLoss: BinaryCrossEntropyLoss 0.01045, SoftDiceLoss 0.25331, SumLoss 0.26376\n",
      "Train Epoch: 1 [29280/51892 (56%)]\tLoss: BinaryCrossEntropyLoss 0.01026, SoftDiceLoss 0.20759, SumLoss 0.21785\n",
      "Train Epoch: 1 [29520/51892 (57%)]\tLoss: BinaryCrossEntropyLoss 0.01544, SoftDiceLoss 0.28512, SumLoss 0.30057\n",
      "Train Epoch: 1 [29760/51892 (57%)]\tLoss: BinaryCrossEntropyLoss 0.01211, SoftDiceLoss 0.22654, SumLoss 0.23865\n",
      "Train Epoch: 1 [30000/51892 (58%)]\tLoss: BinaryCrossEntropyLoss 0.01406, SoftDiceLoss 0.27161, SumLoss 0.28566\n",
      "Train Epoch: 1 [30240/51892 (58%)]\tLoss: BinaryCrossEntropyLoss 0.01621, SoftDiceLoss 0.31114, SumLoss 0.32735\n",
      "Train Epoch: 1 [30480/51892 (59%)]\tLoss: BinaryCrossEntropyLoss 0.00953, SoftDiceLoss 0.16107, SumLoss 0.17059\n",
      "Train Epoch: 1 [30720/51892 (59%)]\tLoss: BinaryCrossEntropyLoss 0.01085, SoftDiceLoss 0.15311, SumLoss 0.16396\n",
      "Train Epoch: 1 [30960/51892 (60%)]\tLoss: BinaryCrossEntropyLoss 0.00939, SoftDiceLoss 0.13140, SumLoss 0.14080\n",
      "Train Epoch: 1 [31200/51892 (60%)]\tLoss: BinaryCrossEntropyLoss 0.01850, SoftDiceLoss -0.06658, SumLoss -0.04808\n",
      "Train Epoch: 1 [31440/51892 (61%)]\tLoss: BinaryCrossEntropyLoss 0.02003, SoftDiceLoss 0.18793, SumLoss 0.20796\n",
      "Train Epoch: 1 [31680/51892 (61%)]\tLoss: BinaryCrossEntropyLoss 0.02339, SoftDiceLoss 0.40728, SumLoss 0.43067\n",
      "Train Epoch: 1 [31920/51892 (61%)]\tLoss: BinaryCrossEntropyLoss 0.06089, SoftDiceLoss 0.94545, SumLoss 1.00634\n",
      "Train Epoch: 1 [32160/51892 (62%)]\tLoss: BinaryCrossEntropyLoss 0.05167, SoftDiceLoss 0.80313, SumLoss 0.85480\n",
      "Train Epoch: 1 [32400/51892 (62%)]\tLoss: BinaryCrossEntropyLoss 0.05495, SoftDiceLoss 0.82420, SumLoss 0.87915\n",
      "Train Epoch: 1 [32640/51892 (63%)]\tLoss: BinaryCrossEntropyLoss 0.01963, SoftDiceLoss 0.38349, SumLoss 0.40312\n",
      "Train Epoch: 1 [32880/51892 (63%)]\tLoss: BinaryCrossEntropyLoss 0.02358, SoftDiceLoss 0.38132, SumLoss 0.40490\n",
      "Train Epoch: 1 [33120/51892 (64%)]\tLoss: BinaryCrossEntropyLoss 0.01429, SoftDiceLoss 0.37195, SumLoss 0.38624\n",
      "Train Epoch: 1 [33360/51892 (64%)]\tLoss: BinaryCrossEntropyLoss 0.01236, SoftDiceLoss 0.32376, SumLoss 0.33612\n",
      "Train Epoch: 1 [33600/51892 (65%)]\tLoss: BinaryCrossEntropyLoss 0.02398, SoftDiceLoss 0.46591, SumLoss 0.48989\n",
      "Train Epoch: 1 [33840/51892 (65%)]\tLoss: BinaryCrossEntropyLoss 0.00980, SoftDiceLoss 0.25698, SumLoss 0.26677\n",
      "Train Epoch: 1 [34080/51892 (66%)]\tLoss: BinaryCrossEntropyLoss 0.00817, SoftDiceLoss 0.25253, SumLoss 0.26070\n",
      "Train Epoch: 1 [34320/51892 (66%)]\tLoss: BinaryCrossEntropyLoss 0.01943, SoftDiceLoss 0.14588, SumLoss 0.16532\n",
      "Train Epoch: 1 [34560/51892 (67%)]\tLoss: BinaryCrossEntropyLoss 0.02042, SoftDiceLoss 0.25043, SumLoss 0.27085\n",
      "Train Epoch: 1 [34800/51892 (67%)]\tLoss: BinaryCrossEntropyLoss 0.01228, SoftDiceLoss 0.32631, SumLoss 0.33858\n",
      "Train Epoch: 1 [35040/51892 (67%)]\tLoss: BinaryCrossEntropyLoss 0.00941, SoftDiceLoss 0.25430, SumLoss 0.26370\n",
      "Train Epoch: 1 [35280/51892 (68%)]\tLoss: BinaryCrossEntropyLoss 0.01927, SoftDiceLoss 0.39877, SumLoss 0.41803\n",
      "Train Epoch: 1 [35520/51892 (68%)]\tLoss: BinaryCrossEntropyLoss 0.01653, SoftDiceLoss 0.37569, SumLoss 0.39222\n",
      "Train Epoch: 1 [35760/51892 (69%)]\tLoss: BinaryCrossEntropyLoss 0.01395, SoftDiceLoss 0.31501, SumLoss 0.32896\n",
      "Train Epoch: 1 [36000/51892 (69%)]\tLoss: BinaryCrossEntropyLoss 0.02102, SoftDiceLoss 0.21880, SumLoss 0.23982\n",
      "Train Epoch: 1 [36240/51892 (70%)]\tLoss: BinaryCrossEntropyLoss 0.01605, SoftDiceLoss 0.35051, SumLoss 0.36656\n",
      "Train Epoch: 1 [36480/51892 (70%)]\tLoss: BinaryCrossEntropyLoss 0.01654, SoftDiceLoss 0.36042, SumLoss 0.37696\n",
      "Train Epoch: 1 [36720/51892 (71%)]\tLoss: BinaryCrossEntropyLoss 0.01381, SoftDiceLoss 0.36392, SumLoss 0.37773\n",
      "Train Epoch: 1 [36960/51892 (71%)]\tLoss: BinaryCrossEntropyLoss 0.01426, SoftDiceLoss 0.30609, SumLoss 0.32035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [37200/51892 (72%)]\tLoss: BinaryCrossEntropyLoss 0.02458, SoftDiceLoss 0.63260, SumLoss 0.65718\n",
      "Train Epoch: 1 [37440/51892 (72%)]\tLoss: BinaryCrossEntropyLoss 0.02182, SoftDiceLoss 0.68863, SumLoss 0.71045\n",
      "Train Epoch: 1 [37680/51892 (73%)]\tLoss: BinaryCrossEntropyLoss 0.01613, SoftDiceLoss 0.54241, SumLoss 0.55855\n",
      "Train Epoch: 1 [37920/51892 (73%)]\tLoss: BinaryCrossEntropyLoss 0.01450, SoftDiceLoss 0.52289, SumLoss 0.53740\n",
      "Train Epoch: 1 [38160/51892 (74%)]\tLoss: BinaryCrossEntropyLoss 0.01970, SoftDiceLoss 0.51723, SumLoss 0.53693\n",
      "Train Epoch: 1 [38400/51892 (74%)]\tLoss: BinaryCrossEntropyLoss 0.02014, SoftDiceLoss 0.32711, SumLoss 0.34725\n",
      "Train Epoch: 1 [38640/51892 (74%)]\tLoss: BinaryCrossEntropyLoss 0.01185, SoftDiceLoss 0.25108, SumLoss 0.26293\n",
      "Train Epoch: 1 [38880/51892 (75%)]\tLoss: BinaryCrossEntropyLoss 0.01359, SoftDiceLoss 0.26644, SumLoss 0.28002\n",
      "Train Epoch: 1 [39120/51892 (75%)]\tLoss: BinaryCrossEntropyLoss 0.01108, SoftDiceLoss 0.31028, SumLoss 0.32136\n",
      "Train Epoch: 1 [39360/51892 (76%)]\tLoss: BinaryCrossEntropyLoss 0.00889, SoftDiceLoss 0.21343, SumLoss 0.22231\n",
      "Train Epoch: 1 [39600/51892 (76%)]\tLoss: BinaryCrossEntropyLoss 0.00773, SoftDiceLoss 0.20427, SumLoss 0.21200\n",
      "Train Epoch: 1 [39840/51892 (77%)]\tLoss: BinaryCrossEntropyLoss 0.00865, SoftDiceLoss 0.23810, SumLoss 0.24675\n",
      "Train Epoch: 1 [40080/51892 (77%)]\tLoss: BinaryCrossEntropyLoss 0.00643, SoftDiceLoss 0.22624, SumLoss 0.23267\n",
      "Train Epoch: 1 [40320/51892 (78%)]\tLoss: BinaryCrossEntropyLoss 0.01588, SoftDiceLoss 0.07137, SumLoss 0.08725\n",
      "Train Epoch: 1 [40560/51892 (78%)]\tLoss: BinaryCrossEntropyLoss 0.01213, SoftDiceLoss 0.24190, SumLoss 0.25403\n",
      "Train Epoch: 1 [40800/51892 (79%)]\tLoss: BinaryCrossEntropyLoss 0.01087, SoftDiceLoss 0.25015, SumLoss 0.26103\n",
      "Train Epoch: 1 [41040/51892 (79%)]\tLoss: BinaryCrossEntropyLoss 0.01107, SoftDiceLoss 0.21432, SumLoss 0.22538\n",
      "Train Epoch: 1 [41280/51892 (80%)]\tLoss: BinaryCrossEntropyLoss 0.01197, SoftDiceLoss 0.20506, SumLoss 0.21703\n",
      "Train Epoch: 1 [41520/51892 (80%)]\tLoss: BinaryCrossEntropyLoss 0.01424, SoftDiceLoss 0.28325, SumLoss 0.29749\n",
      "Train Epoch: 1 [41760/51892 (80%)]\tLoss: BinaryCrossEntropyLoss 0.01262, SoftDiceLoss 0.14000, SumLoss 0.15263\n",
      "Train Epoch: 1 [42000/51892 (81%)]\tLoss: BinaryCrossEntropyLoss 0.01441, SoftDiceLoss 0.22797, SumLoss 0.24238\n",
      "Train Epoch: 1 [42240/51892 (81%)]\tLoss: BinaryCrossEntropyLoss 0.01698, SoftDiceLoss 0.25008, SumLoss 0.26706\n",
      "Train Epoch: 1 [42480/51892 (82%)]\tLoss: BinaryCrossEntropyLoss 0.01079, SoftDiceLoss 0.25431, SumLoss 0.26510\n",
      "Train Epoch: 1 [42720/51892 (82%)]\tLoss: BinaryCrossEntropyLoss 0.01092, SoftDiceLoss 0.23423, SumLoss 0.24515\n",
      "Train Epoch: 1 [42960/51892 (83%)]\tLoss: BinaryCrossEntropyLoss 0.01089, SoftDiceLoss 0.23739, SumLoss 0.24829\n",
      "Train Epoch: 1 [43200/51892 (83%)]\tLoss: BinaryCrossEntropyLoss 0.00793, SoftDiceLoss 0.20509, SumLoss 0.21302\n",
      "Train Epoch: 1 [43440/51892 (84%)]\tLoss: BinaryCrossEntropyLoss 0.02160, SoftDiceLoss 0.58325, SumLoss 0.60484\n",
      "Train Epoch: 1 [43680/51892 (84%)]\tLoss: BinaryCrossEntropyLoss 0.01076, SoftDiceLoss 0.21935, SumLoss 0.23011\n",
      "Train Epoch: 1 [43920/51892 (85%)]\tLoss: BinaryCrossEntropyLoss 0.01519, SoftDiceLoss 0.16390, SumLoss 0.17909\n",
      "Train Epoch: 1 [44160/51892 (85%)]\tLoss: BinaryCrossEntropyLoss 0.01272, SoftDiceLoss 0.22716, SumLoss 0.23989\n",
      "Train Epoch: 1 [44400/51892 (86%)]\tLoss: BinaryCrossEntropyLoss 0.01014, SoftDiceLoss 0.12938, SumLoss 0.13952\n",
      "Train Epoch: 1 [44640/51892 (86%)]\tLoss: BinaryCrossEntropyLoss 0.01144, SoftDiceLoss 0.19558, SumLoss 0.20703\n",
      "Train Epoch: 1 [44880/51892 (86%)]\tLoss: BinaryCrossEntropyLoss 0.01157, SoftDiceLoss 0.15893, SumLoss 0.17050\n",
      "Train Epoch: 1 [45120/51892 (87%)]\tLoss: BinaryCrossEntropyLoss 0.00956, SoftDiceLoss 0.12589, SumLoss 0.13546\n",
      "Train Epoch: 1 [45360/51892 (87%)]\tLoss: BinaryCrossEntropyLoss 0.01219, SoftDiceLoss 0.23691, SumLoss 0.24910\n",
      "Train Epoch: 1 [45600/51892 (88%)]\tLoss: BinaryCrossEntropyLoss 0.01126, SoftDiceLoss 0.19308, SumLoss 0.20434\n",
      "Train Epoch: 1 [45840/51892 (88%)]\tLoss: BinaryCrossEntropyLoss 0.01207, SoftDiceLoss 0.18895, SumLoss 0.20102\n",
      "Train Epoch: 1 [46080/51892 (89%)]\tLoss: BinaryCrossEntropyLoss 0.00884, SoftDiceLoss 0.14505, SumLoss 0.15389\n",
      "Train Epoch: 1 [46320/51892 (89%)]\tLoss: BinaryCrossEntropyLoss 0.00966, SoftDiceLoss 0.20006, SumLoss 0.20972\n",
      "Train Epoch: 1 [46560/51892 (90%)]\tLoss: BinaryCrossEntropyLoss 0.01066, SoftDiceLoss 0.22640, SumLoss 0.23706\n",
      "Train Epoch: 1 [46800/51892 (90%)]\tLoss: BinaryCrossEntropyLoss 0.01887, SoftDiceLoss 0.23041, SumLoss 0.24928\n",
      "Train Epoch: 1 [47040/51892 (91%)]\tLoss: BinaryCrossEntropyLoss 0.01203, SoftDiceLoss 0.20108, SumLoss 0.21311\n",
      "Train Epoch: 1 [47280/51892 (91%)]\tLoss: BinaryCrossEntropyLoss 0.01572, SoftDiceLoss 0.25016, SumLoss 0.26589\n",
      "Train Epoch: 1 [47520/51892 (92%)]\tLoss: BinaryCrossEntropyLoss 0.01076, SoftDiceLoss 0.21766, SumLoss 0.22841\n",
      "Train Epoch: 1 [47760/51892 (92%)]\tLoss: BinaryCrossEntropyLoss 0.01280, SoftDiceLoss 0.20012, SumLoss 0.21292\n",
      "Train Epoch: 1 [48000/51892 (92%)]\tLoss: BinaryCrossEntropyLoss 0.01125, SoftDiceLoss 0.23092, SumLoss 0.24217\n",
      "Train Epoch: 1 [48240/51892 (93%)]\tLoss: BinaryCrossEntropyLoss 0.00843, SoftDiceLoss 0.14650, SumLoss 0.15493\n",
      "Train Epoch: 1 [48480/51892 (93%)]\tLoss: BinaryCrossEntropyLoss 0.00911, SoftDiceLoss 0.22483, SumLoss 0.23395\n",
      "Train Epoch: 1 [48720/51892 (94%)]\tLoss: BinaryCrossEntropyLoss 0.01489, SoftDiceLoss 0.26184, SumLoss 0.27673\n",
      "Train Epoch: 1 [48960/51892 (94%)]\tLoss: BinaryCrossEntropyLoss 0.01263, SoftDiceLoss 0.23217, SumLoss 0.24480\n",
      "Train Epoch: 1 [49200/51892 (95%)]\tLoss: BinaryCrossEntropyLoss 0.01182, SoftDiceLoss 0.24960, SumLoss 0.26143\n",
      "Train Epoch: 1 [49440/51892 (95%)]\tLoss: BinaryCrossEntropyLoss 0.00850, SoftDiceLoss 0.18238, SumLoss 0.19088\n",
      "Train Epoch: 1 [49680/51892 (96%)]\tLoss: BinaryCrossEntropyLoss 0.01193, SoftDiceLoss 0.22132, SumLoss 0.23325\n",
      "Train Epoch: 1 [49920/51892 (96%)]\tLoss: BinaryCrossEntropyLoss 0.01321, SoftDiceLoss 0.21772, SumLoss 0.23094\n",
      "Train Epoch: 1 [50160/51892 (97%)]\tLoss: BinaryCrossEntropyLoss 0.01340, SoftDiceLoss 0.28401, SumLoss 0.29740\n",
      "Train Epoch: 1 [50400/51892 (97%)]\tLoss: BinaryCrossEntropyLoss 0.01904, SoftDiceLoss 0.33889, SumLoss 0.35793\n",
      "Train Epoch: 1 [50640/51892 (98%)]\tLoss: BinaryCrossEntropyLoss 0.02019, SoftDiceLoss 0.22629, SumLoss 0.24647\n",
      "Train Epoch: 1 [50880/51892 (98%)]\tLoss: BinaryCrossEntropyLoss 0.01475, SoftDiceLoss 0.28750, SumLoss 0.30225\n",
      "Train Epoch: 1 [51120/51892 (98%)]\tLoss: BinaryCrossEntropyLoss 0.01938, SoftDiceLoss 0.32770, SumLoss 0.34708\n",
      "Train Epoch: 1 [51360/51892 (99%)]\tLoss: BinaryCrossEntropyLoss 0.01618, SoftDiceLoss 0.27566, SumLoss 0.29184\n",
      "Train Epoch: 1 [51600/51892 (99%)]\tLoss: BinaryCrossEntropyLoss 0.02323, SoftDiceLoss 0.35813, SumLoss 0.38136\n",
      "Train Epoch: 1 [51840/51892 (100%)]\tLoss: BinaryCrossEntropyLoss 0.01628, SoftDiceLoss 0.31523, SumLoss 0.33151\n",
      "Loss on validation: 0.301707373054\n",
      "Train Epoch: 2 [0/51892 (0%)]\tLoss: BinaryCrossEntropyLoss 0.01304, SoftDiceLoss 0.19820, SumLoss 0.21124\n",
      "Train Epoch: 2 [240/51892 (0%)]\tLoss: BinaryCrossEntropyLoss 0.00919, SoftDiceLoss 0.14968, SumLoss 0.15887\n",
      "Train Epoch: 2 [480/51892 (1%)]\tLoss: BinaryCrossEntropyLoss 0.01532, SoftDiceLoss 0.23147, SumLoss 0.24680\n",
      "Train Epoch: 2 [720/51892 (1%)]\tLoss: BinaryCrossEntropyLoss 0.01557, SoftDiceLoss 0.49160, SumLoss 0.50717\n",
      "Train Epoch: 2 [960/51892 (2%)]\tLoss: BinaryCrossEntropyLoss 0.00739, SoftDiceLoss 0.17526, SumLoss 0.18266\n",
      "Train Epoch: 2 [1200/51892 (2%)]\tLoss: BinaryCrossEntropyLoss 0.00768, SoftDiceLoss 0.19557, SumLoss 0.20324\n",
      "Train Epoch: 2 [1440/51892 (3%)]\tLoss: BinaryCrossEntropyLoss 0.00781, SoftDiceLoss 0.17766, SumLoss 0.18547\n",
      "Train Epoch: 2 [1680/51892 (3%)]\tLoss: BinaryCrossEntropyLoss 0.00760, SoftDiceLoss 0.17800, SumLoss 0.18559\n",
      "Train Epoch: 2 [1920/51892 (4%)]\tLoss: BinaryCrossEntropyLoss 0.00860, SoftDiceLoss 0.17385, SumLoss 0.18245\n",
      "Train Epoch: 2 [2160/51892 (4%)]\tLoss: BinaryCrossEntropyLoss 0.00757, SoftDiceLoss 0.19291, SumLoss 0.20048\n",
      "Train Epoch: 2 [2400/51892 (5%)]\tLoss: BinaryCrossEntropyLoss 0.00839, SoftDiceLoss 0.19110, SumLoss 0.19949\n",
      "Train Epoch: 2 [2640/51892 (5%)]\tLoss: BinaryCrossEntropyLoss 0.01326, SoftDiceLoss 0.25528, SumLoss 0.26854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [2880/51892 (6%)]\tLoss: BinaryCrossEntropyLoss 0.01461, SoftDiceLoss 0.31121, SumLoss 0.32582\n",
      "Train Epoch: 2 [3120/51892 (6%)]\tLoss: BinaryCrossEntropyLoss 0.02504, SoftDiceLoss 0.54732, SumLoss 0.57237\n",
      "Train Epoch: 2 [3360/51892 (6%)]\tLoss: BinaryCrossEntropyLoss 0.00899, SoftDiceLoss 0.20853, SumLoss 0.21752\n",
      "Train Epoch: 2 [3600/51892 (7%)]\tLoss: BinaryCrossEntropyLoss 0.00839, SoftDiceLoss 0.23484, SumLoss 0.24323\n",
      "Train Epoch: 2 [3840/51892 (7%)]\tLoss: BinaryCrossEntropyLoss 0.01750, SoftDiceLoss 0.31663, SumLoss 0.33413\n",
      "Train Epoch: 2 [4080/51892 (8%)]\tLoss: BinaryCrossEntropyLoss 0.01452, SoftDiceLoss 0.26560, SumLoss 0.28012\n",
      "Train Epoch: 2 [4320/51892 (8%)]\tLoss: BinaryCrossEntropyLoss 0.01514, SoftDiceLoss 0.14553, SumLoss 0.16067\n",
      "Train Epoch: 2 [4560/51892 (9%)]\tLoss: BinaryCrossEntropyLoss 0.01981, SoftDiceLoss 0.22943, SumLoss 0.24924\n",
      "Train Epoch: 2 [4800/51892 (9%)]\tLoss: BinaryCrossEntropyLoss 0.01298, SoftDiceLoss 0.10096, SumLoss 0.11395\n",
      "Train Epoch: 2 [5040/51892 (10%)]\tLoss: BinaryCrossEntropyLoss 0.02432, SoftDiceLoss 0.38904, SumLoss 0.41336\n",
      "Train Epoch: 2 [5280/51892 (10%)]\tLoss: BinaryCrossEntropyLoss 0.01984, SoftDiceLoss 0.25133, SumLoss 0.27117\n",
      "Train Epoch: 2 [5520/51892 (11%)]\tLoss: BinaryCrossEntropyLoss 0.01697, SoftDiceLoss 0.24155, SumLoss 0.25851\n",
      "Train Epoch: 2 [5760/51892 (11%)]\tLoss: BinaryCrossEntropyLoss 0.01738, SoftDiceLoss 0.31743, SumLoss 0.33481\n",
      "Train Epoch: 2 [6000/51892 (12%)]\tLoss: BinaryCrossEntropyLoss 0.01593, SoftDiceLoss 0.25569, SumLoss 0.27162\n",
      "Train Epoch: 2 [6240/51892 (12%)]\tLoss: BinaryCrossEntropyLoss 0.01245, SoftDiceLoss 0.13902, SumLoss 0.15148\n",
      "Train Epoch: 2 [6480/51892 (12%)]\tLoss: BinaryCrossEntropyLoss 0.01370, SoftDiceLoss 0.21594, SumLoss 0.22964\n",
      "Train Epoch: 2 [6720/51892 (13%)]\tLoss: BinaryCrossEntropyLoss 0.02217, SoftDiceLoss 0.23991, SumLoss 0.26208\n",
      "Train Epoch: 2 [6960/51892 (13%)]\tLoss: BinaryCrossEntropyLoss 0.00888, SoftDiceLoss 0.20941, SumLoss 0.21829\n",
      "Train Epoch: 2 [7200/51892 (14%)]\tLoss: BinaryCrossEntropyLoss 0.00969, SoftDiceLoss 0.19839, SumLoss 0.20809\n",
      "Train Epoch: 2 [7440/51892 (14%)]\tLoss: BinaryCrossEntropyLoss 0.00904, SoftDiceLoss 0.01727, SumLoss 0.02632\n",
      "Train Epoch: 2 [7680/51892 (15%)]\tLoss: BinaryCrossEntropyLoss 0.01161, SoftDiceLoss 0.25923, SumLoss 0.27083\n",
      "Train Epoch: 2 [7920/51892 (15%)]\tLoss: BinaryCrossEntropyLoss 0.01086, SoftDiceLoss 0.22772, SumLoss 0.23858\n",
      "Train Epoch: 2 [8160/51892 (16%)]\tLoss: BinaryCrossEntropyLoss 0.01966, SoftDiceLoss 0.23873, SumLoss 0.25838\n",
      "Train Epoch: 2 [8400/51892 (16%)]\tLoss: BinaryCrossEntropyLoss 0.01884, SoftDiceLoss 0.20058, SumLoss 0.21942\n",
      "Train Epoch: 2 [8640/51892 (17%)]\tLoss: BinaryCrossEntropyLoss 0.01526, SoftDiceLoss 0.22457, SumLoss 0.23983\n",
      "Train Epoch: 2 [8880/51892 (17%)]\tLoss: BinaryCrossEntropyLoss 0.00802, SoftDiceLoss 0.18737, SumLoss 0.19539\n",
      "Train Epoch: 2 [9120/51892 (18%)]\tLoss: BinaryCrossEntropyLoss 0.01246, SoftDiceLoss 0.20693, SumLoss 0.21939\n",
      "Train Epoch: 2 [9360/51892 (18%)]\tLoss: BinaryCrossEntropyLoss 0.00767, SoftDiceLoss 0.15995, SumLoss 0.16762\n",
      "Train Epoch: 2 [9600/51892 (18%)]\tLoss: BinaryCrossEntropyLoss 0.01045, SoftDiceLoss 0.19002, SumLoss 0.20046\n",
      "Train Epoch: 2 [9840/51892 (19%)]\tLoss: BinaryCrossEntropyLoss 0.00758, SoftDiceLoss 0.15270, SumLoss 0.16028\n",
      "Train Epoch: 2 [10080/51892 (19%)]\tLoss: BinaryCrossEntropyLoss 0.00792, SoftDiceLoss 0.17432, SumLoss 0.18224\n",
      "Train Epoch: 2 [10320/51892 (20%)]\tLoss: BinaryCrossEntropyLoss 0.01153, SoftDiceLoss 0.17453, SumLoss 0.18606\n",
      "Train Epoch: 2 [10560/51892 (20%)]\tLoss: BinaryCrossEntropyLoss 0.00940, SoftDiceLoss 0.14637, SumLoss 0.15577\n",
      "Train Epoch: 2 [10800/51892 (21%)]\tLoss: BinaryCrossEntropyLoss 0.01137, SoftDiceLoss 0.17742, SumLoss 0.18880\n",
      "Train Epoch: 2 [11040/51892 (21%)]\tLoss: BinaryCrossEntropyLoss 0.00913, SoftDiceLoss 0.14217, SumLoss 0.15130\n",
      "Train Epoch: 2 [11280/51892 (22%)]\tLoss: BinaryCrossEntropyLoss 0.00935, SoftDiceLoss 0.20253, SumLoss 0.21188\n",
      "Train Epoch: 2 [11520/51892 (22%)]\tLoss: BinaryCrossEntropyLoss 0.00848, SoftDiceLoss -0.00397, SumLoss 0.00452\n",
      "Train Epoch: 2 [11760/51892 (23%)]\tLoss: BinaryCrossEntropyLoss 0.01059, SoftDiceLoss 0.08884, SumLoss 0.09943\n",
      "Train Epoch: 2 [12000/51892 (23%)]\tLoss: BinaryCrossEntropyLoss 0.02775, SoftDiceLoss 0.22002, SumLoss 0.24777\n",
      "Train Epoch: 2 [12240/51892 (24%)]\tLoss: BinaryCrossEntropyLoss 0.01065, SoftDiceLoss 0.02346, SumLoss 0.03411\n",
      "Train Epoch: 2 [12480/51892 (24%)]\tLoss: BinaryCrossEntropyLoss 0.01376, SoftDiceLoss 0.29197, SumLoss 0.30573\n",
      "Train Epoch: 2 [12720/51892 (25%)]\tLoss: BinaryCrossEntropyLoss 0.01283, SoftDiceLoss 0.15075, SumLoss 0.16358\n",
      "Train Epoch: 2 [12960/51892 (25%)]\tLoss: BinaryCrossEntropyLoss 0.01156, SoftDiceLoss 0.17163, SumLoss 0.18320\n",
      "Train Epoch: 2 [13200/51892 (25%)]\tLoss: BinaryCrossEntropyLoss 0.01327, SoftDiceLoss 0.22135, SumLoss 0.23462\n",
      "Train Epoch: 2 [13440/51892 (26%)]\tLoss: BinaryCrossEntropyLoss 0.01221, SoftDiceLoss 0.16127, SumLoss 0.17348\n",
      "Train Epoch: 2 [13680/51892 (26%)]\tLoss: BinaryCrossEntropyLoss 0.01116, SoftDiceLoss 0.08006, SumLoss 0.09121\n",
      "Train Epoch: 2 [13920/51892 (27%)]\tLoss: BinaryCrossEntropyLoss 0.01213, SoftDiceLoss 0.24046, SumLoss 0.25259\n",
      "Train Epoch: 2 [14160/51892 (27%)]\tLoss: BinaryCrossEntropyLoss 0.01007, SoftDiceLoss 0.21559, SumLoss 0.22566\n",
      "Train Epoch: 2 [14400/51892 (28%)]\tLoss: BinaryCrossEntropyLoss 0.01090, SoftDiceLoss 0.23476, SumLoss 0.24566\n",
      "Train Epoch: 2 [14640/51892 (28%)]\tLoss: BinaryCrossEntropyLoss 0.00919, SoftDiceLoss 0.15219, SumLoss 0.16138\n",
      "Train Epoch: 2 [14880/51892 (29%)]\tLoss: BinaryCrossEntropyLoss 0.01077, SoftDiceLoss 0.19596, SumLoss 0.20672\n",
      "Train Epoch: 2 [15120/51892 (29%)]\tLoss: BinaryCrossEntropyLoss 0.00903, SoftDiceLoss 0.17603, SumLoss 0.18506\n",
      "Train Epoch: 2 [15360/51892 (30%)]\tLoss: BinaryCrossEntropyLoss 0.01411, SoftDiceLoss 0.07073, SumLoss 0.08484\n",
      "Train Epoch: 2 [15600/51892 (30%)]\tLoss: BinaryCrossEntropyLoss 0.00732, SoftDiceLoss 0.13407, SumLoss 0.14140\n",
      "Train Epoch: 2 [15840/51892 (31%)]\tLoss: BinaryCrossEntropyLoss 0.00963, SoftDiceLoss 0.22451, SumLoss 0.23414\n",
      "Train Epoch: 2 [16080/51892 (31%)]\tLoss: BinaryCrossEntropyLoss 0.00653, SoftDiceLoss 0.20147, SumLoss 0.20799\n",
      "Train Epoch: 2 [16320/51892 (31%)]\tLoss: BinaryCrossEntropyLoss 0.00682, SoftDiceLoss 0.19374, SumLoss 0.20056\n",
      "Train Epoch: 2 [16560/51892 (32%)]\tLoss: BinaryCrossEntropyLoss 0.01022, SoftDiceLoss 0.17495, SumLoss 0.18517\n",
      "Train Epoch: 2 [16800/51892 (32%)]\tLoss: BinaryCrossEntropyLoss 0.01362, SoftDiceLoss 0.20071, SumLoss 0.21432\n",
      "Train Epoch: 2 [17040/51892 (33%)]\tLoss: BinaryCrossEntropyLoss 0.00936, SoftDiceLoss 0.16872, SumLoss 0.17808\n",
      "Train Epoch: 2 [17280/51892 (33%)]\tLoss: BinaryCrossEntropyLoss 0.01211, SoftDiceLoss 0.17682, SumLoss 0.18892\n",
      "Train Epoch: 2 [17520/51892 (34%)]\tLoss: BinaryCrossEntropyLoss 0.01273, SoftDiceLoss 0.18287, SumLoss 0.19560\n",
      "Train Epoch: 2 [17760/51892 (34%)]\tLoss: BinaryCrossEntropyLoss 0.00985, SoftDiceLoss 0.16013, SumLoss 0.16998\n",
      "Train Epoch: 2 [18000/51892 (35%)]\tLoss: BinaryCrossEntropyLoss 0.01275, SoftDiceLoss 0.21496, SumLoss 0.22771\n",
      "Train Epoch: 2 [18240/51892 (35%)]\tLoss: BinaryCrossEntropyLoss 0.00956, SoftDiceLoss 0.18469, SumLoss 0.19425\n",
      "Train Epoch: 2 [18480/51892 (36%)]\tLoss: BinaryCrossEntropyLoss 0.01190, SoftDiceLoss 0.20327, SumLoss 0.21517\n",
      "Train Epoch: 2 [18720/51892 (36%)]\tLoss: BinaryCrossEntropyLoss 0.01124, SoftDiceLoss 0.18887, SumLoss 0.20010\n",
      "Train Epoch: 2 [18960/51892 (37%)]\tLoss: BinaryCrossEntropyLoss 0.01278, SoftDiceLoss 0.21001, SumLoss 0.22279\n",
      "Train Epoch: 2 [19200/51892 (37%)]\tLoss: BinaryCrossEntropyLoss 0.01430, SoftDiceLoss 0.19813, SumLoss 0.21243\n",
      "Train Epoch: 2 [19440/51892 (37%)]\tLoss: BinaryCrossEntropyLoss 0.01153, SoftDiceLoss 0.21023, SumLoss 0.22177\n",
      "Train Epoch: 2 [19680/51892 (38%)]\tLoss: BinaryCrossEntropyLoss 0.01304, SoftDiceLoss 0.20296, SumLoss 0.21600\n",
      "Train Epoch: 2 [19920/51892 (38%)]\tLoss: BinaryCrossEntropyLoss 0.01191, SoftDiceLoss 0.19718, SumLoss 0.20909\n",
      "Train Epoch: 2 [20160/51892 (39%)]\tLoss: BinaryCrossEntropyLoss 0.01289, SoftDiceLoss 0.19023, SumLoss 0.20312\n",
      "Train Epoch: 2 [20400/51892 (39%)]\tLoss: BinaryCrossEntropyLoss 0.01236, SoftDiceLoss 0.19828, SumLoss 0.21064\n",
      "Train Epoch: 2 [20640/51892 (40%)]\tLoss: BinaryCrossEntropyLoss 0.01188, SoftDiceLoss 0.17724, SumLoss 0.18912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [20880/51892 (40%)]\tLoss: BinaryCrossEntropyLoss 0.01140, SoftDiceLoss 0.17111, SumLoss 0.18251\n",
      "Train Epoch: 2 [21120/51892 (41%)]\tLoss: BinaryCrossEntropyLoss 0.01084, SoftDiceLoss 0.16010, SumLoss 0.17094\n",
      "Train Epoch: 2 [21360/51892 (41%)]\tLoss: BinaryCrossEntropyLoss 0.01238, SoftDiceLoss 0.17485, SumLoss 0.18723\n",
      "Train Epoch: 2 [21600/51892 (42%)]\tLoss: BinaryCrossEntropyLoss 0.01263, SoftDiceLoss 0.16947, SumLoss 0.18210\n",
      "Train Epoch: 2 [21840/51892 (42%)]\tLoss: BinaryCrossEntropyLoss 0.01088, SoftDiceLoss 0.08209, SumLoss 0.09297\n",
      "Train Epoch: 2 [22080/51892 (43%)]\tLoss: BinaryCrossEntropyLoss 0.01021, SoftDiceLoss 0.17051, SumLoss 0.18072\n",
      "Train Epoch: 2 [22320/51892 (43%)]\tLoss: BinaryCrossEntropyLoss 0.01322, SoftDiceLoss 0.31374, SumLoss 0.32696\n",
      "Train Epoch: 2 [22560/51892 (43%)]\tLoss: BinaryCrossEntropyLoss 0.01015, SoftDiceLoss 0.23464, SumLoss 0.24479\n",
      "Train Epoch: 2 [22800/51892 (44%)]\tLoss: BinaryCrossEntropyLoss 0.00908, SoftDiceLoss 0.24237, SumLoss 0.25145\n",
      "Train Epoch: 2 [23040/51892 (44%)]\tLoss: BinaryCrossEntropyLoss 0.01033, SoftDiceLoss 0.20320, SumLoss 0.21353\n",
      "Train Epoch: 2 [23280/51892 (45%)]\tLoss: BinaryCrossEntropyLoss 0.00907, SoftDiceLoss 0.15046, SumLoss 0.15953\n",
      "Train Epoch: 2 [23520/51892 (45%)]\tLoss: BinaryCrossEntropyLoss 0.00920, SoftDiceLoss 0.20662, SumLoss 0.21582\n",
      "Train Epoch: 2 [23760/51892 (46%)]\tLoss: BinaryCrossEntropyLoss 0.01037, SoftDiceLoss 0.22131, SumLoss 0.23168\n",
      "Train Epoch: 2 [24000/51892 (46%)]\tLoss: BinaryCrossEntropyLoss 0.00987, SoftDiceLoss 0.20451, SumLoss 0.21438\n",
      "Train Epoch: 2 [24240/51892 (47%)]\tLoss: BinaryCrossEntropyLoss 0.00784, SoftDiceLoss 0.18654, SumLoss 0.19439\n",
      "Train Epoch: 2 [24480/51892 (47%)]\tLoss: BinaryCrossEntropyLoss 0.01165, SoftDiceLoss 0.22674, SumLoss 0.23839\n",
      "Train Epoch: 2 [24720/51892 (48%)]\tLoss: BinaryCrossEntropyLoss 0.01072, SoftDiceLoss 0.19187, SumLoss 0.20259\n",
      "Train Epoch: 2 [24960/51892 (48%)]\tLoss: BinaryCrossEntropyLoss 0.00958, SoftDiceLoss 0.22931, SumLoss 0.23890\n",
      "Train Epoch: 2 [25200/51892 (49%)]\tLoss: BinaryCrossEntropyLoss 0.01012, SoftDiceLoss 0.14816, SumLoss 0.15828\n",
      "Train Epoch: 2 [25440/51892 (49%)]\tLoss: BinaryCrossEntropyLoss 0.00823, SoftDiceLoss 0.19515, SumLoss 0.20338\n",
      "Train Epoch: 2 [25680/51892 (49%)]\tLoss: BinaryCrossEntropyLoss 0.01143, SoftDiceLoss 0.34301, SumLoss 0.35444\n",
      "Train Epoch: 2 [25920/51892 (50%)]\tLoss: BinaryCrossEntropyLoss 0.00850, SoftDiceLoss 0.20702, SumLoss 0.21552\n",
      "Train Epoch: 2 [26160/51892 (50%)]\tLoss: BinaryCrossEntropyLoss 0.00690, SoftDiceLoss 0.06062, SumLoss 0.06752\n",
      "Train Epoch: 2 [26400/51892 (51%)]\tLoss: BinaryCrossEntropyLoss 0.00917, SoftDiceLoss 0.04274, SumLoss 0.05191\n",
      "Train Epoch: 2 [26640/51892 (51%)]\tLoss: BinaryCrossEntropyLoss 0.00750, SoftDiceLoss 0.15417, SumLoss 0.16168\n",
      "Train Epoch: 2 [26880/51892 (52%)]\tLoss: BinaryCrossEntropyLoss 0.00902, SoftDiceLoss 0.22513, SumLoss 0.23414\n",
      "Train Epoch: 2 [27120/51892 (52%)]\tLoss: BinaryCrossEntropyLoss 0.00837, SoftDiceLoss 0.08539, SumLoss 0.09375\n",
      "Train Epoch: 2 [27360/51892 (53%)]\tLoss: BinaryCrossEntropyLoss 0.00922, SoftDiceLoss 0.22462, SumLoss 0.23384\n",
      "Train Epoch: 2 [27600/51892 (53%)]\tLoss: BinaryCrossEntropyLoss 0.00885, SoftDiceLoss 0.20886, SumLoss 0.21771\n",
      "Train Epoch: 2 [27840/51892 (54%)]\tLoss: BinaryCrossEntropyLoss 0.00868, SoftDiceLoss 0.12596, SumLoss 0.13464\n",
      "Train Epoch: 2 [28080/51892 (54%)]\tLoss: BinaryCrossEntropyLoss 0.00961, SoftDiceLoss 0.12943, SumLoss 0.13903\n",
      "Train Epoch: 2 [28320/51892 (55%)]\tLoss: BinaryCrossEntropyLoss 0.01481, SoftDiceLoss 0.25223, SumLoss 0.26704\n",
      "Train Epoch: 2 [28560/51892 (55%)]\tLoss: BinaryCrossEntropyLoss 0.01405, SoftDiceLoss 0.21713, SumLoss 0.23118\n",
      "Train Epoch: 2 [28800/51892 (55%)]\tLoss: BinaryCrossEntropyLoss 0.00998, SoftDiceLoss 0.21161, SumLoss 0.22159\n",
      "Train Epoch: 2 [29040/51892 (56%)]\tLoss: BinaryCrossEntropyLoss 0.00852, SoftDiceLoss 0.20004, SumLoss 0.20857\n",
      "Train Epoch: 2 [29280/51892 (56%)]\tLoss: BinaryCrossEntropyLoss 0.00881, SoftDiceLoss 0.18482, SumLoss 0.19363\n",
      "Train Epoch: 2 [29520/51892 (57%)]\tLoss: BinaryCrossEntropyLoss 0.01257, SoftDiceLoss 0.22866, SumLoss 0.24123\n",
      "Train Epoch: 2 [29760/51892 (57%)]\tLoss: BinaryCrossEntropyLoss 0.01042, SoftDiceLoss 0.18436, SumLoss 0.19477\n",
      "Train Epoch: 2 [30000/51892 (58%)]\tLoss: BinaryCrossEntropyLoss 0.01298, SoftDiceLoss 0.25193, SumLoss 0.26492\n",
      "Train Epoch: 2 [30240/51892 (58%)]\tLoss: BinaryCrossEntropyLoss 0.01507, SoftDiceLoss 0.28013, SumLoss 0.29520\n",
      "Train Epoch: 2 [30480/51892 (59%)]\tLoss: BinaryCrossEntropyLoss 0.00901, SoftDiceLoss 0.14278, SumLoss 0.15179\n",
      "Train Epoch: 2 [30720/51892 (59%)]\tLoss: BinaryCrossEntropyLoss 0.01018, SoftDiceLoss 0.13334, SumLoss 0.14353\n",
      "Train Epoch: 2 [30960/51892 (60%)]\tLoss: BinaryCrossEntropyLoss 0.00881, SoftDiceLoss 0.11936, SumLoss 0.12818\n",
      "Train Epoch: 2 [31200/51892 (60%)]\tLoss: BinaryCrossEntropyLoss 0.01539, SoftDiceLoss -0.10542, SumLoss -0.09002\n",
      "Train Epoch: 2 [31440/51892 (61%)]\tLoss: BinaryCrossEntropyLoss 0.02037, SoftDiceLoss 0.17813, SumLoss 0.19850\n",
      "Train Epoch: 2 [31680/51892 (61%)]\tLoss: BinaryCrossEntropyLoss 0.02246, SoftDiceLoss 0.34840, SumLoss 0.37086\n",
      "Train Epoch: 2 [31920/51892 (61%)]\tLoss: BinaryCrossEntropyLoss 0.05397, SoftDiceLoss 0.87826, SumLoss 0.93223\n",
      "Train Epoch: 2 [32160/51892 (62%)]\tLoss: BinaryCrossEntropyLoss 0.04565, SoftDiceLoss 0.65836, SumLoss 0.70401\n",
      "Train Epoch: 2 [32400/51892 (62%)]\tLoss: BinaryCrossEntropyLoss 0.05095, SoftDiceLoss 0.74635, SumLoss 0.79730\n",
      "Train Epoch: 2 [32640/51892 (63%)]\tLoss: BinaryCrossEntropyLoss 0.01874, SoftDiceLoss 0.35272, SumLoss 0.37146\n",
      "Train Epoch: 2 [32880/51892 (63%)]\tLoss: BinaryCrossEntropyLoss 0.02325, SoftDiceLoss 0.36750, SumLoss 0.39076\n",
      "Train Epoch: 2 [33120/51892 (64%)]\tLoss: BinaryCrossEntropyLoss 0.01467, SoftDiceLoss 0.35676, SumLoss 0.37143\n",
      "Train Epoch: 2 [33360/51892 (64%)]\tLoss: BinaryCrossEntropyLoss 0.01249, SoftDiceLoss 0.30358, SumLoss 0.31607\n",
      "Train Epoch: 2 [33600/51892 (65%)]\tLoss: BinaryCrossEntropyLoss 0.02243, SoftDiceLoss 0.44559, SumLoss 0.46802\n",
      "Train Epoch: 2 [33840/51892 (65%)]\tLoss: BinaryCrossEntropyLoss 0.00910, SoftDiceLoss 0.23225, SumLoss 0.24136\n",
      "Train Epoch: 2 [34080/51892 (66%)]\tLoss: BinaryCrossEntropyLoss 0.00762, SoftDiceLoss 0.21208, SumLoss 0.21969\n",
      "Train Epoch: 2 [34320/51892 (66%)]\tLoss: BinaryCrossEntropyLoss 0.01664, SoftDiceLoss 0.09093, SumLoss 0.10757\n",
      "Train Epoch: 2 [34560/51892 (67%)]\tLoss: BinaryCrossEntropyLoss 0.01808, SoftDiceLoss 0.19521, SumLoss 0.21328\n",
      "Train Epoch: 2 [34800/51892 (67%)]\tLoss: BinaryCrossEntropyLoss 0.01130, SoftDiceLoss 0.30071, SumLoss 0.31201\n",
      "Train Epoch: 2 [35040/51892 (67%)]\tLoss: BinaryCrossEntropyLoss 0.00927, SoftDiceLoss 0.22688, SumLoss 0.23615\n",
      "Train Epoch: 2 [35280/51892 (68%)]\tLoss: BinaryCrossEntropyLoss 0.01926, SoftDiceLoss 0.40455, SumLoss 0.42381\n",
      "Train Epoch: 2 [35520/51892 (68%)]\tLoss: BinaryCrossEntropyLoss 0.01543, SoftDiceLoss 0.33353, SumLoss 0.34896\n",
      "Train Epoch: 2 [35760/51892 (69%)]\tLoss: BinaryCrossEntropyLoss 0.01309, SoftDiceLoss 0.28994, SumLoss 0.30303\n",
      "Train Epoch: 2 [36000/51892 (69%)]\tLoss: BinaryCrossEntropyLoss 0.01818, SoftDiceLoss 0.18673, SumLoss 0.20492\n",
      "Train Epoch: 2 [36240/51892 (70%)]\tLoss: BinaryCrossEntropyLoss 0.01393, SoftDiceLoss 0.28385, SumLoss 0.29778\n",
      "Train Epoch: 2 [36480/51892 (70%)]\tLoss: BinaryCrossEntropyLoss 0.01518, SoftDiceLoss 0.30140, SumLoss 0.31659\n",
      "Train Epoch: 2 [36720/51892 (71%)]\tLoss: BinaryCrossEntropyLoss 0.01343, SoftDiceLoss 0.33661, SumLoss 0.35003\n",
      "Train Epoch: 2 [36960/51892 (71%)]\tLoss: BinaryCrossEntropyLoss 0.01440, SoftDiceLoss 0.27676, SumLoss 0.29116\n",
      "Train Epoch: 2 [37200/51892 (72%)]\tLoss: BinaryCrossEntropyLoss 0.01856, SoftDiceLoss 0.55429, SumLoss 0.57286\n",
      "Train Epoch: 2 [37440/51892 (72%)]\tLoss: BinaryCrossEntropyLoss 0.01825, SoftDiceLoss 0.64988, SumLoss 0.66813\n",
      "Train Epoch: 2 [37680/51892 (73%)]\tLoss: BinaryCrossEntropyLoss 0.01483, SoftDiceLoss 0.51204, SumLoss 0.52687\n",
      "Train Epoch: 2 [37920/51892 (73%)]\tLoss: BinaryCrossEntropyLoss 0.01252, SoftDiceLoss 0.48774, SumLoss 0.50026\n",
      "Train Epoch: 2 [38160/51892 (74%)]\tLoss: BinaryCrossEntropyLoss 0.01694, SoftDiceLoss 0.48069, SumLoss 0.49764\n",
      "Train Epoch: 2 [38400/51892 (74%)]\tLoss: BinaryCrossEntropyLoss 0.02010, SoftDiceLoss 0.30890, SumLoss 0.32899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [38640/51892 (74%)]\tLoss: BinaryCrossEntropyLoss 0.01151, SoftDiceLoss 0.23807, SumLoss 0.24958\n",
      "Train Epoch: 2 [38880/51892 (75%)]\tLoss: BinaryCrossEntropyLoss 0.01302, SoftDiceLoss 0.25035, SumLoss 0.26336\n",
      "Train Epoch: 2 [39120/51892 (75%)]\tLoss: BinaryCrossEntropyLoss 0.00832, SoftDiceLoss 0.23953, SumLoss 0.24785\n",
      "Train Epoch: 2 [39360/51892 (76%)]\tLoss: BinaryCrossEntropyLoss 0.00842, SoftDiceLoss 0.19759, SumLoss 0.20601\n",
      "Train Epoch: 2 [39600/51892 (76%)]\tLoss: BinaryCrossEntropyLoss 0.00733, SoftDiceLoss 0.19444, SumLoss 0.20176\n",
      "Train Epoch: 2 [39840/51892 (77%)]\tLoss: BinaryCrossEntropyLoss 0.00822, SoftDiceLoss 0.21605, SumLoss 0.22427\n",
      "Train Epoch: 2 [40080/51892 (77%)]\tLoss: BinaryCrossEntropyLoss 0.00577, SoftDiceLoss 0.19535, SumLoss 0.20112\n",
      "Train Epoch: 2 [40320/51892 (78%)]\tLoss: BinaryCrossEntropyLoss 0.01259, SoftDiceLoss 0.03447, SumLoss 0.04706\n",
      "Train Epoch: 2 [40560/51892 (78%)]\tLoss: BinaryCrossEntropyLoss 0.01054, SoftDiceLoss 0.19940, SumLoss 0.20993\n",
      "Train Epoch: 2 [40800/51892 (79%)]\tLoss: BinaryCrossEntropyLoss 0.01001, SoftDiceLoss 0.22489, SumLoss 0.23490\n",
      "Train Epoch: 2 [41040/51892 (79%)]\tLoss: BinaryCrossEntropyLoss 0.01025, SoftDiceLoss 0.19364, SumLoss 0.20388\n",
      "Train Epoch: 2 [41280/51892 (80%)]\tLoss: BinaryCrossEntropyLoss 0.01124, SoftDiceLoss 0.18940, SumLoss 0.20064\n",
      "Train Epoch: 2 [41520/51892 (80%)]\tLoss: BinaryCrossEntropyLoss 0.01158, SoftDiceLoss 0.24317, SumLoss 0.25476\n",
      "Train Epoch: 2 [41760/51892 (80%)]\tLoss: BinaryCrossEntropyLoss 0.01182, SoftDiceLoss 0.12825, SumLoss 0.14007\n",
      "Train Epoch: 2 [42000/51892 (81%)]\tLoss: BinaryCrossEntropyLoss 0.01321, SoftDiceLoss 0.21244, SumLoss 0.22565\n",
      "Train Epoch: 2 [42240/51892 (81%)]\tLoss: BinaryCrossEntropyLoss 0.01474, SoftDiceLoss 0.20538, SumLoss 0.22013\n",
      "Train Epoch: 2 [42480/51892 (82%)]\tLoss: BinaryCrossEntropyLoss 0.00933, SoftDiceLoss 0.21521, SumLoss 0.22454\n",
      "Train Epoch: 2 [42720/51892 (82%)]\tLoss: BinaryCrossEntropyLoss 0.00961, SoftDiceLoss 0.20017, SumLoss 0.20978\n",
      "Train Epoch: 2 [42960/51892 (83%)]\tLoss: BinaryCrossEntropyLoss 0.00979, SoftDiceLoss 0.20898, SumLoss 0.21876\n",
      "Train Epoch: 2 [43200/51892 (83%)]\tLoss: BinaryCrossEntropyLoss 0.00703, SoftDiceLoss 0.17839, SumLoss 0.18542\n",
      "Train Epoch: 2 [43440/51892 (84%)]\tLoss: BinaryCrossEntropyLoss 0.02049, SoftDiceLoss 0.55933, SumLoss 0.57983\n",
      "Train Epoch: 2 [43680/51892 (84%)]\tLoss: BinaryCrossEntropyLoss 0.00962, SoftDiceLoss 0.17932, SumLoss 0.18894\n",
      "Train Epoch: 2 [43920/51892 (85%)]\tLoss: BinaryCrossEntropyLoss 0.01401, SoftDiceLoss 0.14905, SumLoss 0.16306\n",
      "Train Epoch: 2 [44160/51892 (85%)]\tLoss: BinaryCrossEntropyLoss 0.01165, SoftDiceLoss 0.20116, SumLoss 0.21281\n",
      "Train Epoch: 2 [44400/51892 (86%)]\tLoss: BinaryCrossEntropyLoss 0.00913, SoftDiceLoss 0.10739, SumLoss 0.11652\n",
      "Train Epoch: 2 [44640/51892 (86%)]\tLoss: BinaryCrossEntropyLoss 0.00983, SoftDiceLoss 0.14966, SumLoss 0.15949\n",
      "Train Epoch: 2 [44880/51892 (86%)]\tLoss: BinaryCrossEntropyLoss 0.01070, SoftDiceLoss 0.14450, SumLoss 0.15521\n",
      "Train Epoch: 2 [45120/51892 (87%)]\tLoss: BinaryCrossEntropyLoss 0.00882, SoftDiceLoss 0.11166, SumLoss 0.12048\n",
      "Train Epoch: 2 [45360/51892 (87%)]\tLoss: BinaryCrossEntropyLoss 0.01104, SoftDiceLoss 0.21956, SumLoss 0.23060\n",
      "Train Epoch: 2 [45600/51892 (88%)]\tLoss: BinaryCrossEntropyLoss 0.01044, SoftDiceLoss 0.17408, SumLoss 0.18451\n",
      "Train Epoch: 2 [45840/51892 (88%)]\tLoss: BinaryCrossEntropyLoss 0.01113, SoftDiceLoss 0.15801, SumLoss 0.16914\n",
      "Train Epoch: 2 [46080/51892 (89%)]\tLoss: BinaryCrossEntropyLoss 0.00841, SoftDiceLoss 0.13257, SumLoss 0.14098\n",
      "Train Epoch: 2 [46320/51892 (89%)]\tLoss: BinaryCrossEntropyLoss 0.00909, SoftDiceLoss 0.18277, SumLoss 0.19186\n",
      "Train Epoch: 2 [46560/51892 (90%)]\tLoss: BinaryCrossEntropyLoss 0.01032, SoftDiceLoss 0.20714, SumLoss 0.21746\n",
      "Train Epoch: 2 [46800/51892 (90%)]\tLoss: BinaryCrossEntropyLoss 0.01648, SoftDiceLoss 0.19850, SumLoss 0.21498\n",
      "Train Epoch: 2 [47040/51892 (91%)]\tLoss: BinaryCrossEntropyLoss 0.01149, SoftDiceLoss 0.18118, SumLoss 0.19266\n",
      "Train Epoch: 2 [47280/51892 (91%)]\tLoss: BinaryCrossEntropyLoss 0.01465, SoftDiceLoss 0.22677, SumLoss 0.24142\n",
      "Train Epoch: 2 [47520/51892 (92%)]\tLoss: BinaryCrossEntropyLoss 0.01002, SoftDiceLoss 0.19879, SumLoss 0.20881\n",
      "Train Epoch: 2 [47760/51892 (92%)]\tLoss: BinaryCrossEntropyLoss 0.01206, SoftDiceLoss 0.18186, SumLoss 0.19392\n",
      "Train Epoch: 2 [48000/51892 (92%)]\tLoss: BinaryCrossEntropyLoss 0.01082, SoftDiceLoss 0.21272, SumLoss 0.22353\n",
      "Train Epoch: 2 [48240/51892 (93%)]\tLoss: BinaryCrossEntropyLoss 0.00787, SoftDiceLoss 0.13635, SumLoss 0.14422\n",
      "Train Epoch: 2 [48480/51892 (93%)]\tLoss: BinaryCrossEntropyLoss 0.00882, SoftDiceLoss 0.20511, SumLoss 0.21392\n",
      "Train Epoch: 2 [48720/51892 (94%)]\tLoss: BinaryCrossEntropyLoss 0.01419, SoftDiceLoss 0.24340, SumLoss 0.25759\n",
      "Train Epoch: 2 [48960/51892 (94%)]\tLoss: BinaryCrossEntropyLoss 0.01207, SoftDiceLoss 0.21418, SumLoss 0.22625\n",
      "Train Epoch: 2 [49200/51892 (95%)]\tLoss: BinaryCrossEntropyLoss 0.01108, SoftDiceLoss 0.21520, SumLoss 0.22628\n",
      "Train Epoch: 2 [49440/51892 (95%)]\tLoss: BinaryCrossEntropyLoss 0.00794, SoftDiceLoss 0.16623, SumLoss 0.17417\n",
      "Train Epoch: 2 [49680/51892 (96%)]\tLoss: BinaryCrossEntropyLoss 0.01146, SoftDiceLoss 0.20553, SumLoss 0.21699\n",
      "Train Epoch: 2 [49920/51892 (96%)]\tLoss: BinaryCrossEntropyLoss 0.01229, SoftDiceLoss 0.20525, SumLoss 0.21754\n",
      "Train Epoch: 2 [50160/51892 (97%)]\tLoss: BinaryCrossEntropyLoss 0.01205, SoftDiceLoss 0.24795, SumLoss 0.26001\n",
      "Train Epoch: 2 [50400/51892 (97%)]\tLoss: BinaryCrossEntropyLoss 0.01777, SoftDiceLoss 0.31328, SumLoss 0.33104\n",
      "Train Epoch: 2 [50640/51892 (98%)]\tLoss: BinaryCrossEntropyLoss 0.01961, SoftDiceLoss 0.21106, SumLoss 0.23067\n",
      "Train Epoch: 2 [50880/51892 (98%)]\tLoss: BinaryCrossEntropyLoss 0.01411, SoftDiceLoss 0.26309, SumLoss 0.27720\n",
      "Train Epoch: 2 [51120/51892 (98%)]\tLoss: BinaryCrossEntropyLoss 0.01852, SoftDiceLoss 0.30543, SumLoss 0.32395\n",
      "Train Epoch: 2 [51360/51892 (99%)]\tLoss: BinaryCrossEntropyLoss 0.01591, SoftDiceLoss 0.28907, SumLoss 0.30498\n",
      "Train Epoch: 2 [51600/51892 (99%)]\tLoss: BinaryCrossEntropyLoss 0.02282, SoftDiceLoss 0.33554, SumLoss 0.35836\n",
      "Train Epoch: 2 [51840/51892 (100%)]\tLoss: BinaryCrossEntropyLoss 0.01505, SoftDiceLoss 0.27403, SumLoss 0.28909\n",
      "Loss on validation: 0.278849537349\n",
      "Train Epoch: 3 [0/51892 (0%)]\tLoss: BinaryCrossEntropyLoss 0.01246, SoftDiceLoss 0.18609, SumLoss 0.19855\n",
      "Train Epoch: 3 [240/51892 (0%)]\tLoss: BinaryCrossEntropyLoss 0.00863, SoftDiceLoss 0.11885, SumLoss 0.12747\n",
      "Train Epoch: 3 [480/51892 (1%)]\tLoss: BinaryCrossEntropyLoss 0.01391, SoftDiceLoss 0.20463, SumLoss 0.21854\n",
      "Train Epoch: 3 [720/51892 (1%)]\tLoss: BinaryCrossEntropyLoss 0.01368, SoftDiceLoss 0.36116, SumLoss 0.37484\n",
      "Train Epoch: 3 [960/51892 (2%)]\tLoss: BinaryCrossEntropyLoss 0.00689, SoftDiceLoss 0.15784, SumLoss 0.16473\n",
      "Train Epoch: 3 [1200/51892 (2%)]\tLoss: BinaryCrossEntropyLoss 0.00722, SoftDiceLoss 0.17576, SumLoss 0.18298\n",
      "Train Epoch: 3 [1440/51892 (3%)]\tLoss: BinaryCrossEntropyLoss 0.00748, SoftDiceLoss 0.16697, SumLoss 0.17445\n",
      "Train Epoch: 3 [1680/51892 (3%)]\tLoss: BinaryCrossEntropyLoss 0.00719, SoftDiceLoss 0.16441, SumLoss 0.17160\n",
      "Train Epoch: 3 [1920/51892 (4%)]\tLoss: BinaryCrossEntropyLoss 0.00836, SoftDiceLoss 0.16541, SumLoss 0.17377\n",
      "Train Epoch: 3 [2160/51892 (4%)]\tLoss: BinaryCrossEntropyLoss 0.00715, SoftDiceLoss 0.18004, SumLoss 0.18719\n",
      "Train Epoch: 3 [2400/51892 (5%)]\tLoss: BinaryCrossEntropyLoss 0.00784, SoftDiceLoss 0.17623, SumLoss 0.18407\n",
      "Train Epoch: 3 [2640/51892 (5%)]\tLoss: BinaryCrossEntropyLoss 0.01289, SoftDiceLoss 0.24942, SumLoss 0.26231\n",
      "Train Epoch: 3 [2880/51892 (6%)]\tLoss: BinaryCrossEntropyLoss 0.01415, SoftDiceLoss 0.30078, SumLoss 0.31493\n",
      "Train Epoch: 3 [3120/51892 (6%)]\tLoss: BinaryCrossEntropyLoss 0.02314, SoftDiceLoss 0.51934, SumLoss 0.54247\n",
      "Train Epoch: 3 [3360/51892 (6%)]\tLoss: BinaryCrossEntropyLoss 0.00850, SoftDiceLoss 0.19192, SumLoss 0.20041\n",
      "Train Epoch: 3 [3600/51892 (7%)]\tLoss: BinaryCrossEntropyLoss 0.00800, SoftDiceLoss 0.21731, SumLoss 0.22531\n",
      "Train Epoch: 3 [3840/51892 (7%)]\tLoss: BinaryCrossEntropyLoss 0.01636, SoftDiceLoss 0.29521, SumLoss 0.31157\n",
      "Train Epoch: 3 [4080/51892 (8%)]\tLoss: BinaryCrossEntropyLoss 0.01441, SoftDiceLoss 0.25719, SumLoss 0.27160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [4320/51892 (8%)]\tLoss: BinaryCrossEntropyLoss 0.01355, SoftDiceLoss 0.12219, SumLoss 0.13575\n",
      "Train Epoch: 3 [4560/51892 (9%)]\tLoss: BinaryCrossEntropyLoss 0.01859, SoftDiceLoss 0.21728, SumLoss 0.23586\n",
      "Train Epoch: 3 [4800/51892 (9%)]\tLoss: BinaryCrossEntropyLoss 0.01232, SoftDiceLoss 0.08949, SumLoss 0.10181\n",
      "Train Epoch: 3 [5040/51892 (10%)]\tLoss: BinaryCrossEntropyLoss 0.02233, SoftDiceLoss 0.35924, SumLoss 0.38157\n",
      "Train Epoch: 3 [5280/51892 (10%)]\tLoss: BinaryCrossEntropyLoss 0.01880, SoftDiceLoss 0.23665, SumLoss 0.25546\n",
      "Train Epoch: 3 [5520/51892 (11%)]\tLoss: BinaryCrossEntropyLoss 0.01544, SoftDiceLoss 0.22493, SumLoss 0.24037\n",
      "Train Epoch: 3 [5760/51892 (11%)]\tLoss: BinaryCrossEntropyLoss 0.01533, SoftDiceLoss 0.29304, SumLoss 0.30837\n",
      "Train Epoch: 3 [6000/51892 (12%)]\tLoss: BinaryCrossEntropyLoss 0.01594, SoftDiceLoss 0.24482, SumLoss 0.26076\n",
      "Train Epoch: 3 [6240/51892 (12%)]\tLoss: BinaryCrossEntropyLoss 0.01143, SoftDiceLoss 0.11949, SumLoss 0.13091\n",
      "Train Epoch: 3 [6480/51892 (12%)]\tLoss: BinaryCrossEntropyLoss 0.01230, SoftDiceLoss 0.19804, SumLoss 0.21035\n",
      "Train Epoch: 3 [6720/51892 (13%)]\tLoss: BinaryCrossEntropyLoss 0.02021, SoftDiceLoss 0.21924, SumLoss 0.23945\n",
      "Train Epoch: 3 [6960/51892 (13%)]\tLoss: BinaryCrossEntropyLoss 0.00824, SoftDiceLoss 0.20092, SumLoss 0.20916\n",
      "Train Epoch: 3 [7200/51892 (14%)]\tLoss: BinaryCrossEntropyLoss 0.00902, SoftDiceLoss 0.18843, SumLoss 0.19745\n",
      "Train Epoch: 3 [7440/51892 (14%)]\tLoss: BinaryCrossEntropyLoss 0.00849, SoftDiceLoss 0.00570, SumLoss 0.01419\n",
      "Train Epoch: 3 [7680/51892 (15%)]\tLoss: BinaryCrossEntropyLoss 0.01075, SoftDiceLoss 0.24004, SumLoss 0.25079\n",
      "Train Epoch: 3 [7920/51892 (15%)]\tLoss: BinaryCrossEntropyLoss 0.01032, SoftDiceLoss 0.21432, SumLoss 0.22464\n",
      "Train Epoch: 3 [8160/51892 (16%)]\tLoss: BinaryCrossEntropyLoss 0.01813, SoftDiceLoss 0.21825, SumLoss 0.23638\n",
      "Train Epoch: 3 [8400/51892 (16%)]\tLoss: BinaryCrossEntropyLoss 0.01847, SoftDiceLoss 0.18874, SumLoss 0.20721\n",
      "Train Epoch: 3 [8640/51892 (17%)]\tLoss: BinaryCrossEntropyLoss 0.01378, SoftDiceLoss 0.20315, SumLoss 0.21693\n",
      "Train Epoch: 3 [8880/51892 (17%)]\tLoss: BinaryCrossEntropyLoss 0.00737, SoftDiceLoss 0.17195, SumLoss 0.17932\n",
      "Train Epoch: 3 [9120/51892 (18%)]\tLoss: BinaryCrossEntropyLoss 0.01133, SoftDiceLoss 0.19169, SumLoss 0.20301\n",
      "Train Epoch: 3 [9360/51892 (18%)]\tLoss: BinaryCrossEntropyLoss 0.00717, SoftDiceLoss 0.14903, SumLoss 0.15620\n",
      "Train Epoch: 3 [9600/51892 (18%)]\tLoss: BinaryCrossEntropyLoss 0.00938, SoftDiceLoss 0.17239, SumLoss 0.18177\n",
      "Train Epoch: 3 [9840/51892 (19%)]\tLoss: BinaryCrossEntropyLoss 0.00706, SoftDiceLoss 0.14363, SumLoss 0.15069\n",
      "Train Epoch: 3 [10080/51892 (19%)]\tLoss: BinaryCrossEntropyLoss 0.00733, SoftDiceLoss 0.16243, SumLoss 0.16976\n",
      "Train Epoch: 3 [10320/51892 (20%)]\tLoss: BinaryCrossEntropyLoss 0.00985, SoftDiceLoss 0.13956, SumLoss 0.14942\n",
      "Train Epoch: 3 [10560/51892 (20%)]\tLoss: BinaryCrossEntropyLoss 0.00877, SoftDiceLoss 0.13361, SumLoss 0.14239\n",
      "Train Epoch: 3 [10800/51892 (21%)]\tLoss: BinaryCrossEntropyLoss 0.01006, SoftDiceLoss 0.15514, SumLoss 0.16520\n",
      "Train Epoch: 3 [11040/51892 (21%)]\tLoss: BinaryCrossEntropyLoss 0.00849, SoftDiceLoss 0.12695, SumLoss 0.13544\n",
      "Train Epoch: 3 [11280/51892 (22%)]\tLoss: BinaryCrossEntropyLoss 0.00839, SoftDiceLoss 0.18291, SumLoss 0.19130\n",
      "Train Epoch: 3 [11520/51892 (22%)]\tLoss: BinaryCrossEntropyLoss 0.00801, SoftDiceLoss -0.01699, SumLoss -0.00898\n",
      "Train Epoch: 3 [11760/51892 (23%)]\tLoss: BinaryCrossEntropyLoss 0.01027, SoftDiceLoss 0.08190, SumLoss 0.09218\n",
      "Train Epoch: 3 [12000/51892 (23%)]\tLoss: BinaryCrossEntropyLoss 0.02526, SoftDiceLoss 0.21477, SumLoss 0.24003\n",
      "Train Epoch: 3 [12240/51892 (24%)]\tLoss: BinaryCrossEntropyLoss 0.01021, SoftDiceLoss 0.02056, SumLoss 0.03077\n",
      "Train Epoch: 3 [12480/51892 (24%)]\tLoss: BinaryCrossEntropyLoss 0.01290, SoftDiceLoss 0.27744, SumLoss 0.29033\n",
      "Train Epoch: 3 [12720/51892 (25%)]\tLoss: BinaryCrossEntropyLoss 0.01213, SoftDiceLoss 0.13973, SumLoss 0.15186\n",
      "Train Epoch: 3 [12960/51892 (25%)]\tLoss: BinaryCrossEntropyLoss 0.01064, SoftDiceLoss 0.14190, SumLoss 0.15254\n",
      "Train Epoch: 3 [13200/51892 (25%)]\tLoss: BinaryCrossEntropyLoss 0.01253, SoftDiceLoss 0.21381, SumLoss 0.22634\n",
      "Train Epoch: 3 [13440/51892 (26%)]\tLoss: BinaryCrossEntropyLoss 0.01187, SoftDiceLoss 0.15269, SumLoss 0.16456\n",
      "Train Epoch: 3 [13680/51892 (26%)]\tLoss: BinaryCrossEntropyLoss 0.01037, SoftDiceLoss 0.07006, SumLoss 0.08043\n",
      "Train Epoch: 3 [13920/51892 (27%)]\tLoss: BinaryCrossEntropyLoss 0.01135, SoftDiceLoss 0.22555, SumLoss 0.23690\n",
      "Train Epoch: 3 [14160/51892 (27%)]\tLoss: BinaryCrossEntropyLoss 0.00924, SoftDiceLoss 0.19843, SumLoss 0.20767\n",
      "Train Epoch: 3 [14400/51892 (28%)]\tLoss: BinaryCrossEntropyLoss 0.01024, SoftDiceLoss 0.22171, SumLoss 0.23194\n",
      "Train Epoch: 3 [14640/51892 (28%)]\tLoss: BinaryCrossEntropyLoss 0.00870, SoftDiceLoss 0.13957, SumLoss 0.14827\n",
      "Train Epoch: 3 [14880/51892 (29%)]\tLoss: BinaryCrossEntropyLoss 0.00999, SoftDiceLoss 0.18699, SumLoss 0.19699\n",
      "Train Epoch: 3 [15120/51892 (29%)]\tLoss: BinaryCrossEntropyLoss 0.00842, SoftDiceLoss 0.16472, SumLoss 0.17313\n",
      "Train Epoch: 3 [15360/51892 (30%)]\tLoss: BinaryCrossEntropyLoss 0.01370, SoftDiceLoss 0.06689, SumLoss 0.08060\n",
      "Train Epoch: 3 [15600/51892 (30%)]\tLoss: BinaryCrossEntropyLoss 0.00703, SoftDiceLoss 0.12872, SumLoss 0.13575\n",
      "Train Epoch: 3 [15840/51892 (31%)]\tLoss: BinaryCrossEntropyLoss 0.00901, SoftDiceLoss 0.21256, SumLoss 0.22157\n",
      "Train Epoch: 3 [16080/51892 (31%)]\tLoss: BinaryCrossEntropyLoss 0.00626, SoftDiceLoss 0.18812, SumLoss 0.19438\n",
      "Train Epoch: 3 [16320/51892 (31%)]\tLoss: BinaryCrossEntropyLoss 0.00629, SoftDiceLoss 0.17608, SumLoss 0.18237\n",
      "Train Epoch: 3 [16560/51892 (32%)]\tLoss: BinaryCrossEntropyLoss 0.00914, SoftDiceLoss 0.15906, SumLoss 0.16820\n",
      "Train Epoch: 3 [16800/51892 (32%)]\tLoss: BinaryCrossEntropyLoss 0.01206, SoftDiceLoss 0.17946, SumLoss 0.19152\n",
      "Train Epoch: 3 [17040/51892 (33%)]\tLoss: BinaryCrossEntropyLoss 0.00831, SoftDiceLoss 0.15410, SumLoss 0.16241\n",
      "Train Epoch: 3 [17280/51892 (33%)]\tLoss: BinaryCrossEntropyLoss 0.01115, SoftDiceLoss 0.16102, SumLoss 0.17217\n",
      "Train Epoch: 3 [17520/51892 (34%)]\tLoss: BinaryCrossEntropyLoss 0.01118, SoftDiceLoss 0.16487, SumLoss 0.17606\n",
      "Train Epoch: 3 [17760/51892 (34%)]\tLoss: BinaryCrossEntropyLoss 0.00914, SoftDiceLoss 0.14792, SumLoss 0.15706\n",
      "Train Epoch: 3 [18000/51892 (35%)]\tLoss: BinaryCrossEntropyLoss 0.01172, SoftDiceLoss 0.19752, SumLoss 0.20924\n",
      "Train Epoch: 3 [18240/51892 (35%)]\tLoss: BinaryCrossEntropyLoss 0.00891, SoftDiceLoss 0.17204, SumLoss 0.18094\n",
      "Train Epoch: 3 [18480/51892 (36%)]\tLoss: BinaryCrossEntropyLoss 0.01137, SoftDiceLoss 0.19229, SumLoss 0.20366\n",
      "Train Epoch: 3 [18720/51892 (36%)]\tLoss: BinaryCrossEntropyLoss 0.01053, SoftDiceLoss 0.17807, SumLoss 0.18860\n",
      "Train Epoch: 3 [18960/51892 (37%)]\tLoss: BinaryCrossEntropyLoss 0.01255, SoftDiceLoss 0.20314, SumLoss 0.21568\n",
      "Train Epoch: 3 [19200/51892 (37%)]\tLoss: BinaryCrossEntropyLoss 0.01364, SoftDiceLoss 0.18298, SumLoss 0.19663\n",
      "Train Epoch: 3 [19440/51892 (37%)]\tLoss: BinaryCrossEntropyLoss 0.01107, SoftDiceLoss 0.19816, SumLoss 0.20923\n",
      "Train Epoch: 3 [19680/51892 (38%)]\tLoss: BinaryCrossEntropyLoss 0.01233, SoftDiceLoss 0.18945, SumLoss 0.20178\n",
      "Train Epoch: 3 [19920/51892 (38%)]\tLoss: BinaryCrossEntropyLoss 0.01160, SoftDiceLoss 0.18835, SumLoss 0.19995\n",
      "Train Epoch: 3 [20160/51892 (39%)]\tLoss: BinaryCrossEntropyLoss 0.01239, SoftDiceLoss 0.18062, SumLoss 0.19301\n",
      "Train Epoch: 3 [20400/51892 (39%)]\tLoss: BinaryCrossEntropyLoss 0.01216, SoftDiceLoss 0.19068, SumLoss 0.20285\n",
      "Train Epoch: 3 [20640/51892 (40%)]\tLoss: BinaryCrossEntropyLoss 0.01079, SoftDiceLoss 0.16671, SumLoss 0.17750\n",
      "Train Epoch: 3 [20880/51892 (40%)]\tLoss: BinaryCrossEntropyLoss 0.01060, SoftDiceLoss 0.16431, SumLoss 0.17491\n",
      "Train Epoch: 3 [21120/51892 (41%)]\tLoss: BinaryCrossEntropyLoss 0.00990, SoftDiceLoss 0.15447, SumLoss 0.16437\n",
      "Train Epoch: 3 [21360/51892 (41%)]\tLoss: BinaryCrossEntropyLoss 0.01144, SoftDiceLoss 0.16650, SumLoss 0.17794\n",
      "Train Epoch: 3 [21600/51892 (42%)]\tLoss: BinaryCrossEntropyLoss 0.01169, SoftDiceLoss 0.16167, SumLoss 0.17336\n",
      "Train Epoch: 3 [21840/51892 (42%)]\tLoss: BinaryCrossEntropyLoss 0.01042, SoftDiceLoss 0.07688, SumLoss 0.08730\n",
      "Train Epoch: 3 [22080/51892 (43%)]\tLoss: BinaryCrossEntropyLoss 0.00953, SoftDiceLoss 0.15987, SumLoss 0.16941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [22320/51892 (43%)]\tLoss: BinaryCrossEntropyLoss 0.00854, SoftDiceLoss 0.22996, SumLoss 0.23849\n",
      "Train Epoch: 3 [22560/51892 (43%)]\tLoss: BinaryCrossEntropyLoss 0.00933, SoftDiceLoss 0.20648, SumLoss 0.21581\n",
      "Train Epoch: 3 [22800/51892 (44%)]\tLoss: BinaryCrossEntropyLoss 0.00807, SoftDiceLoss 0.21106, SumLoss 0.21913\n",
      "Train Epoch: 3 [23040/51892 (44%)]\tLoss: BinaryCrossEntropyLoss 0.00991, SoftDiceLoss 0.19307, SumLoss 0.20298\n",
      "Train Epoch: 3 [23280/51892 (45%)]\tLoss: BinaryCrossEntropyLoss 0.00821, SoftDiceLoss 0.13285, SumLoss 0.14106\n",
      "Train Epoch: 3 [23520/51892 (45%)]\tLoss: BinaryCrossEntropyLoss 0.00864, SoftDiceLoss 0.19576, SumLoss 0.20440\n",
      "Train Epoch: 3 [23760/51892 (46%)]\tLoss: BinaryCrossEntropyLoss 0.00987, SoftDiceLoss 0.21094, SumLoss 0.22081\n",
      "Train Epoch: 3 [24000/51892 (46%)]\tLoss: BinaryCrossEntropyLoss 0.00907, SoftDiceLoss 0.18816, SumLoss 0.19722\n",
      "Train Epoch: 3 [24240/51892 (47%)]\tLoss: BinaryCrossEntropyLoss 0.00739, SoftDiceLoss 0.17917, SumLoss 0.18656\n",
      "Train Epoch: 3 [24480/51892 (47%)]\tLoss: BinaryCrossEntropyLoss 0.01072, SoftDiceLoss 0.20819, SumLoss 0.21891\n",
      "Train Epoch: 3 [24720/51892 (48%)]\tLoss: BinaryCrossEntropyLoss 0.01005, SoftDiceLoss 0.18134, SumLoss 0.19139\n",
      "Train Epoch: 3 [24960/51892 (48%)]\tLoss: BinaryCrossEntropyLoss 0.00895, SoftDiceLoss 0.22013, SumLoss 0.22908\n",
      "Train Epoch: 3 [25200/51892 (49%)]\tLoss: BinaryCrossEntropyLoss 0.00956, SoftDiceLoss 0.13764, SumLoss 0.14721\n",
      "Train Epoch: 3 [25440/51892 (49%)]\tLoss: BinaryCrossEntropyLoss 0.00789, SoftDiceLoss 0.18657, SumLoss 0.19446\n",
      "Train Epoch: 3 [25680/51892 (49%)]\tLoss: BinaryCrossEntropyLoss 0.01127, SoftDiceLoss 0.33159, SumLoss 0.34286\n",
      "Train Epoch: 3 [25920/51892 (50%)]\tLoss: BinaryCrossEntropyLoss 0.00762, SoftDiceLoss 0.18389, SumLoss 0.19150\n",
      "Train Epoch: 3 [26160/51892 (50%)]\tLoss: BinaryCrossEntropyLoss 0.00632, SoftDiceLoss 0.04917, SumLoss 0.05550\n",
      "Train Epoch: 3 [26400/51892 (51%)]\tLoss: BinaryCrossEntropyLoss 0.00810, SoftDiceLoss 0.01793, SumLoss 0.02603\n",
      "Train Epoch: 3 [26640/51892 (51%)]\tLoss: BinaryCrossEntropyLoss 0.00708, SoftDiceLoss 0.14462, SumLoss 0.15170\n",
      "Train Epoch: 3 [26880/51892 (52%)]\tLoss: BinaryCrossEntropyLoss 0.00851, SoftDiceLoss 0.21025, SumLoss 0.21877\n",
      "Train Epoch: 3 [27120/51892 (52%)]\tLoss: BinaryCrossEntropyLoss 0.00796, SoftDiceLoss 0.07740, SumLoss 0.08535\n",
      "Train Epoch: 3 [27360/51892 (53%)]\tLoss: BinaryCrossEntropyLoss 0.00858, SoftDiceLoss 0.20607, SumLoss 0.21465\n",
      "Train Epoch: 3 [27600/51892 (53%)]\tLoss: BinaryCrossEntropyLoss 0.00807, SoftDiceLoss 0.19533, SumLoss 0.20340\n",
      "Train Epoch: 3 [27840/51892 (54%)]\tLoss: BinaryCrossEntropyLoss 0.00833, SoftDiceLoss 0.11829, SumLoss 0.12662\n",
      "Train Epoch: 3 [28080/51892 (54%)]\tLoss: BinaryCrossEntropyLoss 0.00906, SoftDiceLoss 0.11731, SumLoss 0.12637\n",
      "Train Epoch: 3 [28320/51892 (55%)]\tLoss: BinaryCrossEntropyLoss 0.01316, SoftDiceLoss 0.22803, SumLoss 0.24119\n",
      "Train Epoch: 3 [28560/51892 (55%)]\tLoss: BinaryCrossEntropyLoss 0.01261, SoftDiceLoss 0.19763, SumLoss 0.21024\n",
      "Train Epoch: 3 [28800/51892 (55%)]\tLoss: BinaryCrossEntropyLoss 0.00908, SoftDiceLoss 0.19137, SumLoss 0.20045\n",
      "Train Epoch: 3 [29040/51892 (56%)]\tLoss: BinaryCrossEntropyLoss 0.00772, SoftDiceLoss 0.18108, SumLoss 0.18880\n",
      "Train Epoch: 3 [29280/51892 (56%)]\tLoss: BinaryCrossEntropyLoss 0.00820, SoftDiceLoss 0.17309, SumLoss 0.18129\n",
      "Train Epoch: 3 [29520/51892 (57%)]\tLoss: BinaryCrossEntropyLoss 0.01155, SoftDiceLoss 0.20681, SumLoss 0.21835\n",
      "Train Epoch: 3 [29760/51892 (57%)]\tLoss: BinaryCrossEntropyLoss 0.00979, SoftDiceLoss 0.17360, SumLoss 0.18339\n",
      "Train Epoch: 3 [30000/51892 (58%)]\tLoss: BinaryCrossEntropyLoss 0.01228, SoftDiceLoss 0.23754, SumLoss 0.24982\n",
      "Train Epoch: 3 [30240/51892 (58%)]\tLoss: BinaryCrossEntropyLoss 0.01443, SoftDiceLoss 0.26619, SumLoss 0.28061\n",
      "Train Epoch: 3 [30480/51892 (59%)]\tLoss: BinaryCrossEntropyLoss 0.00868, SoftDiceLoss 0.13580, SumLoss 0.14448\n",
      "Train Epoch: 3 [30720/51892 (59%)]\tLoss: BinaryCrossEntropyLoss 0.00986, SoftDiceLoss 0.12567, SumLoss 0.13553\n",
      "Train Epoch: 3 [30960/51892 (60%)]\tLoss: BinaryCrossEntropyLoss 0.00858, SoftDiceLoss 0.11367, SumLoss 0.12224\n",
      "Train Epoch: 3 [31200/51892 (60%)]\tLoss: BinaryCrossEntropyLoss 0.01496, SoftDiceLoss -0.10811, SumLoss -0.09315\n",
      "Train Epoch: 3 [31440/51892 (61%)]\tLoss: BinaryCrossEntropyLoss 0.02020, SoftDiceLoss 0.17318, SumLoss 0.19338\n",
      "Train Epoch: 3 [31680/51892 (61%)]\tLoss: BinaryCrossEntropyLoss 0.02115, SoftDiceLoss 0.32034, SumLoss 0.34149\n",
      "Train Epoch: 3 [31920/51892 (61%)]\tLoss: BinaryCrossEntropyLoss 0.04669, SoftDiceLoss 0.84206, SumLoss 0.88876\n",
      "Train Epoch: 3 [32160/51892 (62%)]\tLoss: BinaryCrossEntropyLoss 0.04067, SoftDiceLoss 0.63056, SumLoss 0.67123\n",
      "Train Epoch: 3 [32400/51892 (62%)]\tLoss: BinaryCrossEntropyLoss 0.04270, SoftDiceLoss 0.71483, SumLoss 0.75754\n",
      "Train Epoch: 3 [32640/51892 (63%)]\tLoss: BinaryCrossEntropyLoss 0.01814, SoftDiceLoss 0.34237, SumLoss 0.36050\n",
      "Train Epoch: 3 [32880/51892 (63%)]\tLoss: BinaryCrossEntropyLoss 0.02211, SoftDiceLoss 0.33444, SumLoss 0.35655\n",
      "Train Epoch: 3 [33120/51892 (64%)]\tLoss: BinaryCrossEntropyLoss 0.01490, SoftDiceLoss 0.34538, SumLoss 0.36028\n",
      "Train Epoch: 3 [33360/51892 (64%)]\tLoss: BinaryCrossEntropyLoss 0.01218, SoftDiceLoss 0.29169, SumLoss 0.30386\n",
      "Train Epoch: 3 [33600/51892 (65%)]\tLoss: BinaryCrossEntropyLoss 0.02189, SoftDiceLoss 0.44186, SumLoss 0.46375\n",
      "Train Epoch: 3 [33840/51892 (65%)]\tLoss: BinaryCrossEntropyLoss 0.00870, SoftDiceLoss 0.21786, SumLoss 0.22656\n",
      "Train Epoch: 3 [34080/51892 (66%)]\tLoss: BinaryCrossEntropyLoss 0.00713, SoftDiceLoss 0.19408, SumLoss 0.20121\n",
      "Train Epoch: 3 [34320/51892 (66%)]\tLoss: BinaryCrossEntropyLoss 0.01470, SoftDiceLoss 0.06544, SumLoss 0.08015\n",
      "Train Epoch: 3 [34560/51892 (67%)]\tLoss: BinaryCrossEntropyLoss 0.01670, SoftDiceLoss 0.17842, SumLoss 0.19511\n",
      "Train Epoch: 3 [34800/51892 (67%)]\tLoss: BinaryCrossEntropyLoss 0.01098, SoftDiceLoss 0.28070, SumLoss 0.29168\n",
      "Train Epoch: 3 [35040/51892 (67%)]\tLoss: BinaryCrossEntropyLoss 0.00900, SoftDiceLoss 0.21473, SumLoss 0.22373\n",
      "Train Epoch: 3 [35280/51892 (68%)]\tLoss: BinaryCrossEntropyLoss 0.01880, SoftDiceLoss 0.39634, SumLoss 0.41514\n",
      "Train Epoch: 3 [35520/51892 (68%)]\tLoss: BinaryCrossEntropyLoss 0.01454, SoftDiceLoss 0.31010, SumLoss 0.32464\n",
      "Train Epoch: 3 [35760/51892 (69%)]\tLoss: BinaryCrossEntropyLoss 0.01206, SoftDiceLoss 0.26943, SumLoss 0.28148\n",
      "Train Epoch: 3 [36000/51892 (69%)]\tLoss: BinaryCrossEntropyLoss 0.01207, SoftDiceLoss 0.15097, SumLoss 0.16305\n",
      "Train Epoch: 3 [36240/51892 (70%)]\tLoss: BinaryCrossEntropyLoss 0.01340, SoftDiceLoss 0.26331, SumLoss 0.27671\n",
      "Train Epoch: 3 [36480/51892 (70%)]\tLoss: BinaryCrossEntropyLoss 0.01481, SoftDiceLoss 0.28837, SumLoss 0.30317\n",
      "Train Epoch: 3 [36720/51892 (71%)]\tLoss: BinaryCrossEntropyLoss 0.01317, SoftDiceLoss 0.32361, SumLoss 0.33678\n",
      "Train Epoch: 3 [36960/51892 (71%)]\tLoss: BinaryCrossEntropyLoss 0.01419, SoftDiceLoss 0.26149, SumLoss 0.27568\n",
      "Train Epoch: 3 [37200/51892 (72%)]\tLoss: BinaryCrossEntropyLoss 0.01548, SoftDiceLoss 0.51538, SumLoss 0.53085\n",
      "Train Epoch: 3 [37440/51892 (72%)]\tLoss: BinaryCrossEntropyLoss 0.01522, SoftDiceLoss 0.60271, SumLoss 0.61793\n",
      "Train Epoch: 3 [37680/51892 (73%)]\tLoss: BinaryCrossEntropyLoss 0.01386, SoftDiceLoss 0.49254, SumLoss 0.50640\n",
      "Train Epoch: 3 [37920/51892 (73%)]\tLoss: BinaryCrossEntropyLoss 0.01065, SoftDiceLoss 0.43590, SumLoss 0.44655\n",
      "Train Epoch: 3 [38160/51892 (74%)]\tLoss: BinaryCrossEntropyLoss 0.01528, SoftDiceLoss 0.45133, SumLoss 0.46661\n",
      "Train Epoch: 3 [38400/51892 (74%)]\tLoss: BinaryCrossEntropyLoss 0.01711, SoftDiceLoss 0.29301, SumLoss 0.31013\n",
      "Train Epoch: 3 [38640/51892 (74%)]\tLoss: BinaryCrossEntropyLoss 0.01113, SoftDiceLoss 0.22945, SumLoss 0.24058\n",
      "Train Epoch: 3 [38880/51892 (75%)]\tLoss: BinaryCrossEntropyLoss 0.01259, SoftDiceLoss 0.24678, SumLoss 0.25936\n",
      "Train Epoch: 3 [39120/51892 (75%)]\tLoss: BinaryCrossEntropyLoss 0.00753, SoftDiceLoss 0.20912, SumLoss 0.21666\n",
      "Train Epoch: 3 [39360/51892 (76%)]\tLoss: BinaryCrossEntropyLoss 0.00807, SoftDiceLoss 0.18853, SumLoss 0.19660\n",
      "Train Epoch: 3 [39600/51892 (76%)]\tLoss: BinaryCrossEntropyLoss 0.00703, SoftDiceLoss 0.18667, SumLoss 0.19370\n",
      "Train Epoch: 3 [39840/51892 (77%)]\tLoss: BinaryCrossEntropyLoss 0.00793, SoftDiceLoss 0.20490, SumLoss 0.21284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [40080/51892 (77%)]\tLoss: BinaryCrossEntropyLoss 0.00553, SoftDiceLoss 0.18463, SumLoss 0.19016\n",
      "Train Epoch: 3 [40320/51892 (78%)]\tLoss: BinaryCrossEntropyLoss 0.01074, SoftDiceLoss 0.01199, SumLoss 0.02273\n",
      "Train Epoch: 3 [40560/51892 (78%)]\tLoss: BinaryCrossEntropyLoss 0.00997, SoftDiceLoss 0.18529, SumLoss 0.19526\n",
      "Train Epoch: 3 [40800/51892 (79%)]\tLoss: BinaryCrossEntropyLoss 0.00963, SoftDiceLoss 0.21710, SumLoss 0.22673\n",
      "Train Epoch: 3 [41040/51892 (79%)]\tLoss: BinaryCrossEntropyLoss 0.00976, SoftDiceLoss 0.18358, SumLoss 0.19334\n",
      "Train Epoch: 3 [41280/51892 (80%)]\tLoss: BinaryCrossEntropyLoss 0.01078, SoftDiceLoss 0.18075, SumLoss 0.19153\n",
      "Train Epoch: 3 [41520/51892 (80%)]\tLoss: BinaryCrossEntropyLoss 0.01083, SoftDiceLoss 0.23138, SumLoss 0.24221\n",
      "Train Epoch: 3 [41760/51892 (80%)]\tLoss: BinaryCrossEntropyLoss 0.01126, SoftDiceLoss 0.11962, SumLoss 0.13088\n",
      "Train Epoch: 3 [42000/51892 (81%)]\tLoss: BinaryCrossEntropyLoss 0.01229, SoftDiceLoss 0.20218, SumLoss 0.21447\n",
      "Train Epoch: 3 [42240/51892 (81%)]\tLoss: BinaryCrossEntropyLoss 0.01439, SoftDiceLoss 0.19196, SumLoss 0.20634\n",
      "Train Epoch: 3 [42480/51892 (82%)]\tLoss: BinaryCrossEntropyLoss 0.00861, SoftDiceLoss 0.18769, SumLoss 0.19631\n",
      "Train Epoch: 3 [42720/51892 (82%)]\tLoss: BinaryCrossEntropyLoss 0.00902, SoftDiceLoss 0.18386, SumLoss 0.19288\n",
      "Train Epoch: 3 [42960/51892 (83%)]\tLoss: BinaryCrossEntropyLoss 0.00915, SoftDiceLoss 0.19175, SumLoss 0.20089\n",
      "Train Epoch: 3 [43200/51892 (83%)]\tLoss: BinaryCrossEntropyLoss 0.00651, SoftDiceLoss 0.16310, SumLoss 0.16961\n",
      "Train Epoch: 3 [43440/51892 (84%)]\tLoss: BinaryCrossEntropyLoss 0.01943, SoftDiceLoss 0.52831, SumLoss 0.54774\n",
      "Train Epoch: 3 [43680/51892 (84%)]\tLoss: BinaryCrossEntropyLoss 0.00918, SoftDiceLoss 0.16583, SumLoss 0.17500\n",
      "Train Epoch: 3 [43920/51892 (85%)]\tLoss: BinaryCrossEntropyLoss 0.01302, SoftDiceLoss 0.13904, SumLoss 0.15206\n",
      "Train Epoch: 3 [44160/51892 (85%)]\tLoss: BinaryCrossEntropyLoss 0.01097, SoftDiceLoss 0.19086, SumLoss 0.20182\n",
      "Train Epoch: 3 [44400/51892 (86%)]\tLoss: BinaryCrossEntropyLoss 0.00858, SoftDiceLoss 0.09661, SumLoss 0.10519\n",
      "Train Epoch: 3 [44640/51892 (86%)]\tLoss: BinaryCrossEntropyLoss 0.00907, SoftDiceLoss 0.12574, SumLoss 0.13481\n",
      "Train Epoch: 3 [44880/51892 (86%)]\tLoss: BinaryCrossEntropyLoss 0.01003, SoftDiceLoss 0.13429, SumLoss 0.14432\n",
      "Train Epoch: 3 [45120/51892 (87%)]\tLoss: BinaryCrossEntropyLoss 0.00831, SoftDiceLoss 0.10279, SumLoss 0.11110\n",
      "Train Epoch: 3 [45360/51892 (87%)]\tLoss: BinaryCrossEntropyLoss 0.01026, SoftDiceLoss 0.20919, SumLoss 0.21945\n",
      "Train Epoch: 3 [45600/51892 (88%)]\tLoss: BinaryCrossEntropyLoss 0.00994, SoftDiceLoss 0.16741, SumLoss 0.17734\n",
      "Train Epoch: 3 [45840/51892 (88%)]\tLoss: BinaryCrossEntropyLoss 0.01047, SoftDiceLoss 0.14226, SumLoss 0.15273\n",
      "Train Epoch: 3 [46080/51892 (89%)]\tLoss: BinaryCrossEntropyLoss 0.00804, SoftDiceLoss 0.12371, SumLoss 0.13175\n",
      "Train Epoch: 3 [46320/51892 (89%)]\tLoss: BinaryCrossEntropyLoss 0.00865, SoftDiceLoss 0.17436, SumLoss 0.18301\n",
      "Train Epoch: 3 [46560/51892 (90%)]\tLoss: BinaryCrossEntropyLoss 0.00986, SoftDiceLoss 0.19241, SumLoss 0.20227\n",
      "Train Epoch: 3 [46800/51892 (90%)]\tLoss: BinaryCrossEntropyLoss 0.01572, SoftDiceLoss 0.18709, SumLoss 0.20282\n",
      "Train Epoch: 3 [47040/51892 (91%)]\tLoss: BinaryCrossEntropyLoss 0.01089, SoftDiceLoss 0.16876, SumLoss 0.17965\n",
      "Train Epoch: 3 [47280/51892 (91%)]\tLoss: BinaryCrossEntropyLoss 0.01413, SoftDiceLoss 0.21337, SumLoss 0.22750\n",
      "Train Epoch: 3 [47520/51892 (92%)]\tLoss: BinaryCrossEntropyLoss 0.00980, SoftDiceLoss 0.18578, SumLoss 0.19558\n",
      "Train Epoch: 3 [47760/51892 (92%)]\tLoss: BinaryCrossEntropyLoss 0.01159, SoftDiceLoss 0.17307, SumLoss 0.18466\n",
      "Train Epoch: 3 [48000/51892 (92%)]\tLoss: BinaryCrossEntropyLoss 0.01042, SoftDiceLoss 0.19946, SumLoss 0.20988\n",
      "Train Epoch: 3 [48240/51892 (93%)]\tLoss: BinaryCrossEntropyLoss 0.00751, SoftDiceLoss 0.13101, SumLoss 0.13852\n",
      "Train Epoch: 3 [48480/51892 (93%)]\tLoss: BinaryCrossEntropyLoss 0.00853, SoftDiceLoss 0.19519, SumLoss 0.20372\n",
      "Train Epoch: 3 [48720/51892 (94%)]\tLoss: BinaryCrossEntropyLoss 0.01362, SoftDiceLoss 0.23304, SumLoss 0.24666\n",
      "Train Epoch: 3 [48960/51892 (94%)]\tLoss: BinaryCrossEntropyLoss 0.01137, SoftDiceLoss 0.19940, SumLoss 0.21077\n",
      "Train Epoch: 3 [49200/51892 (95%)]\tLoss: BinaryCrossEntropyLoss 0.01061, SoftDiceLoss 0.19969, SumLoss 0.21029\n",
      "Train Epoch: 3 [49440/51892 (95%)]\tLoss: BinaryCrossEntropyLoss 0.00753, SoftDiceLoss 0.15626, SumLoss 0.16379\n",
      "Train Epoch: 3 [49680/51892 (96%)]\tLoss: BinaryCrossEntropyLoss 0.01107, SoftDiceLoss 0.19628, SumLoss 0.20735\n",
      "Train Epoch: 3 [49920/51892 (96%)]\tLoss: BinaryCrossEntropyLoss 0.01161, SoftDiceLoss 0.19605, SumLoss 0.20766\n",
      "Train Epoch: 3 [50160/51892 (97%)]\tLoss: BinaryCrossEntropyLoss 0.01131, SoftDiceLoss 0.22877, SumLoss 0.24008\n",
      "Train Epoch: 3 [50400/51892 (97%)]\tLoss: BinaryCrossEntropyLoss 0.01692, SoftDiceLoss 0.30163, SumLoss 0.31854\n",
      "Train Epoch: 3 [50640/51892 (98%)]\tLoss: BinaryCrossEntropyLoss 0.01821, SoftDiceLoss 0.18975, SumLoss 0.20797\n",
      "Train Epoch: 3 [50880/51892 (98%)]\tLoss: BinaryCrossEntropyLoss 0.01364, SoftDiceLoss 0.25095, SumLoss 0.26459\n",
      "Train Epoch: 3 [51120/51892 (98%)]\tLoss: BinaryCrossEntropyLoss 0.01837, SoftDiceLoss 0.29069, SumLoss 0.30906\n",
      "Train Epoch: 3 [51360/51892 (99%)]\tLoss: BinaryCrossEntropyLoss 0.01814, SoftDiceLoss 0.34455, SumLoss 0.36269\n",
      "Train Epoch: 3 [51600/51892 (99%)]\tLoss: BinaryCrossEntropyLoss 0.02366, SoftDiceLoss 0.34419, SumLoss 0.36785\n",
      "Train Epoch: 3 [51840/51892 (100%)]\tLoss: BinaryCrossEntropyLoss 0.01407, SoftDiceLoss 0.25672, SumLoss 0.27079\n",
      "Loss on validation: 0.264942838688\n",
      "Train Epoch: 4 [0/51892 (0%)]\tLoss: BinaryCrossEntropyLoss 0.01166, SoftDiceLoss 0.17491, SumLoss 0.18657\n",
      "Train Epoch: 4 [240/51892 (0%)]\tLoss: BinaryCrossEntropyLoss 0.00807, SoftDiceLoss 0.10883, SumLoss 0.11691\n",
      "Train Epoch: 4 [480/51892 (1%)]\tLoss: BinaryCrossEntropyLoss 0.01286, SoftDiceLoss 0.19093, SumLoss 0.20380\n",
      "Train Epoch: 4 [720/51892 (1%)]\tLoss: BinaryCrossEntropyLoss 0.01285, SoftDiceLoss 0.31353, SumLoss 0.32639\n",
      "Train Epoch: 4 [960/51892 (2%)]\tLoss: BinaryCrossEntropyLoss 0.00632, SoftDiceLoss 0.14611, SumLoss 0.15243\n",
      "Train Epoch: 4 [1200/51892 (2%)]\tLoss: BinaryCrossEntropyLoss 0.00676, SoftDiceLoss 0.16427, SumLoss 0.17103\n",
      "Train Epoch: 4 [1440/51892 (3%)]\tLoss: BinaryCrossEntropyLoss 0.00707, SoftDiceLoss 0.15857, SumLoss 0.16563\n",
      "Train Epoch: 4 [1680/51892 (3%)]\tLoss: BinaryCrossEntropyLoss 0.00681, SoftDiceLoss 0.15621, SumLoss 0.16302\n",
      "Train Epoch: 4 [1920/51892 (4%)]\tLoss: BinaryCrossEntropyLoss 0.00785, SoftDiceLoss 0.15731, SumLoss 0.16516\n",
      "Train Epoch: 4 [2160/51892 (4%)]\tLoss: BinaryCrossEntropyLoss 0.00672, SoftDiceLoss 0.17093, SumLoss 0.17765\n",
      "Train Epoch: 4 [2400/51892 (5%)]\tLoss: BinaryCrossEntropyLoss 0.00733, SoftDiceLoss 0.16615, SumLoss 0.17348\n",
      "Train Epoch: 4 [2640/51892 (5%)]\tLoss: BinaryCrossEntropyLoss 0.01222, SoftDiceLoss 0.22875, SumLoss 0.24097\n",
      "Train Epoch: 4 [2880/51892 (6%)]\tLoss: BinaryCrossEntropyLoss 0.01350, SoftDiceLoss 0.29031, SumLoss 0.30381\n",
      "Train Epoch: 4 [3120/51892 (6%)]\tLoss: BinaryCrossEntropyLoss 0.02180, SoftDiceLoss 0.48967, SumLoss 0.51147\n",
      "Train Epoch: 4 [3360/51892 (6%)]\tLoss: BinaryCrossEntropyLoss 0.00796, SoftDiceLoss 0.18143, SumLoss 0.18938\n",
      "Train Epoch: 4 [3600/51892 (7%)]\tLoss: BinaryCrossEntropyLoss 0.00762, SoftDiceLoss 0.20762, SumLoss 0.21524\n",
      "Train Epoch: 4 [3840/51892 (7%)]\tLoss: BinaryCrossEntropyLoss 0.01557, SoftDiceLoss 0.27977, SumLoss 0.29534\n",
      "Train Epoch: 4 [4080/51892 (8%)]\tLoss: BinaryCrossEntropyLoss 0.01447, SoftDiceLoss 0.25059, SumLoss 0.26506\n",
      "Train Epoch: 4 [4320/51892 (8%)]\tLoss: BinaryCrossEntropyLoss 0.01284, SoftDiceLoss 0.10704, SumLoss 0.11989\n",
      "Train Epoch: 4 [4560/51892 (9%)]\tLoss: BinaryCrossEntropyLoss 0.01791, SoftDiceLoss 0.21075, SumLoss 0.22866\n",
      "Train Epoch: 4 [4800/51892 (9%)]\tLoss: BinaryCrossEntropyLoss 0.01192, SoftDiceLoss 0.08429, SumLoss 0.09621\n",
      "Train Epoch: 4 [5040/51892 (10%)]\tLoss: BinaryCrossEntropyLoss 0.02077, SoftDiceLoss 0.33783, SumLoss 0.35860\n",
      "Train Epoch: 4 [5280/51892 (10%)]\tLoss: BinaryCrossEntropyLoss 0.01805, SoftDiceLoss 0.22490, SumLoss 0.24295\n",
      "Train Epoch: 4 [5520/51892 (11%)]\tLoss: BinaryCrossEntropyLoss 0.01420, SoftDiceLoss 0.20994, SumLoss 0.22414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [5760/51892 (11%)]\tLoss: BinaryCrossEntropyLoss 0.01397, SoftDiceLoss 0.27412, SumLoss 0.28809\n",
      "Train Epoch: 4 [6000/51892 (12%)]\tLoss: BinaryCrossEntropyLoss 0.01493, SoftDiceLoss 0.22986, SumLoss 0.24479\n",
      "Train Epoch: 4 [6240/51892 (12%)]\tLoss: BinaryCrossEntropyLoss 0.01085, SoftDiceLoss 0.10444, SumLoss 0.11529\n",
      "Train Epoch: 4 [6480/51892 (12%)]\tLoss: BinaryCrossEntropyLoss 0.01159, SoftDiceLoss 0.18511, SumLoss 0.19671\n",
      "Train Epoch: 4 [6720/51892 (13%)]\tLoss: BinaryCrossEntropyLoss 0.01903, SoftDiceLoss 0.21596, SumLoss 0.23500\n",
      "Train Epoch: 4 [6960/51892 (13%)]\tLoss: BinaryCrossEntropyLoss 0.00807, SoftDiceLoss 0.19643, SumLoss 0.20451\n",
      "Train Epoch: 4 [7200/51892 (14%)]\tLoss: BinaryCrossEntropyLoss 0.00880, SoftDiceLoss 0.18269, SumLoss 0.19149\n",
      "Train Epoch: 4 [7440/51892 (14%)]\tLoss: BinaryCrossEntropyLoss 0.00826, SoftDiceLoss -0.00073, SumLoss 0.00753\n",
      "Train Epoch: 4 [7680/51892 (15%)]\tLoss: BinaryCrossEntropyLoss 0.01050, SoftDiceLoss 0.23159, SumLoss 0.24209\n",
      "Train Epoch: 4 [7920/51892 (15%)]\tLoss: BinaryCrossEntropyLoss 0.01009, SoftDiceLoss 0.20717, SumLoss 0.21726\n",
      "Train Epoch: 4 [8160/51892 (16%)]\tLoss: BinaryCrossEntropyLoss 0.01703, SoftDiceLoss 0.20586, SumLoss 0.22289\n",
      "Train Epoch: 4 [8400/51892 (16%)]\tLoss: BinaryCrossEntropyLoss 0.01812, SoftDiceLoss 0.18128, SumLoss 0.19940\n",
      "Train Epoch: 4 [8640/51892 (17%)]\tLoss: BinaryCrossEntropyLoss 0.01290, SoftDiceLoss 0.19078, SumLoss 0.20368\n",
      "Train Epoch: 4 [8880/51892 (17%)]\tLoss: BinaryCrossEntropyLoss 0.00685, SoftDiceLoss 0.15925, SumLoss 0.16609\n",
      "Train Epoch: 4 [9120/51892 (18%)]\tLoss: BinaryCrossEntropyLoss 0.00940, SoftDiceLoss 0.16315, SumLoss 0.17255\n",
      "Train Epoch: 4 [9360/51892 (18%)]\tLoss: BinaryCrossEntropyLoss 0.00698, SoftDiceLoss 0.14267, SumLoss 0.14966\n",
      "Train Epoch: 4 [9600/51892 (18%)]\tLoss: BinaryCrossEntropyLoss 0.00831, SoftDiceLoss 0.15387, SumLoss 0.16218\n",
      "Train Epoch: 4 [9840/51892 (19%)]\tLoss: BinaryCrossEntropyLoss 0.00680, SoftDiceLoss 0.13863, SumLoss 0.14543\n",
      "Train Epoch: 4 [10080/51892 (19%)]\tLoss: BinaryCrossEntropyLoss 0.00699, SoftDiceLoss 0.15468, SumLoss 0.16167\n",
      "Train Epoch: 4 [10320/51892 (20%)]\tLoss: BinaryCrossEntropyLoss 0.00921, SoftDiceLoss 0.12657, SumLoss 0.13579\n",
      "Train Epoch: 4 [10560/51892 (20%)]\tLoss: BinaryCrossEntropyLoss 0.00834, SoftDiceLoss 0.12505, SumLoss 0.13340\n",
      "Train Epoch: 4 [10800/51892 (21%)]\tLoss: BinaryCrossEntropyLoss 0.00924, SoftDiceLoss 0.13933, SumLoss 0.14856\n",
      "Train Epoch: 4 [11040/51892 (21%)]\tLoss: BinaryCrossEntropyLoss 0.00802, SoftDiceLoss 0.11730, SumLoss 0.12531\n",
      "Train Epoch: 4 [11280/51892 (22%)]\tLoss: BinaryCrossEntropyLoss 0.00779, SoftDiceLoss 0.16400, SumLoss 0.17179\n",
      "Train Epoch: 4 [11520/51892 (22%)]\tLoss: BinaryCrossEntropyLoss 0.00762, SoftDiceLoss -0.02558, SumLoss -0.01796\n",
      "Train Epoch: 4 [11760/51892 (23%)]\tLoss: BinaryCrossEntropyLoss 0.00997, SoftDiceLoss 0.07650, SumLoss 0.08647\n",
      "Train Epoch: 4 [12000/51892 (23%)]\tLoss: BinaryCrossEntropyLoss 0.02321, SoftDiceLoss 0.21044, SumLoss 0.23366\n",
      "Train Epoch: 4 [12240/51892 (24%)]\tLoss: BinaryCrossEntropyLoss 0.00985, SoftDiceLoss 0.01924, SumLoss 0.02909\n",
      "Train Epoch: 4 [12480/51892 (24%)]\tLoss: BinaryCrossEntropyLoss 0.01240, SoftDiceLoss 0.26799, SumLoss 0.28039\n",
      "Train Epoch: 4 [12720/51892 (25%)]\tLoss: BinaryCrossEntropyLoss 0.01174, SoftDiceLoss 0.13347, SumLoss 0.14521\n",
      "Train Epoch: 4 [12960/51892 (25%)]\tLoss: BinaryCrossEntropyLoss 0.01028, SoftDiceLoss 0.13278, SumLoss 0.14305\n",
      "Train Epoch: 4 [13200/51892 (25%)]\tLoss: BinaryCrossEntropyLoss 0.01167, SoftDiceLoss 0.20574, SumLoss 0.21742\n",
      "Train Epoch: 4 [13440/51892 (26%)]\tLoss: BinaryCrossEntropyLoss 0.01128, SoftDiceLoss 0.14507, SumLoss 0.15635\n",
      "Train Epoch: 4 [13680/51892 (26%)]\tLoss: BinaryCrossEntropyLoss 0.00987, SoftDiceLoss 0.06601, SumLoss 0.07587\n",
      "Train Epoch: 4 [13920/51892 (27%)]\tLoss: BinaryCrossEntropyLoss 0.01043, SoftDiceLoss 0.21344, SumLoss 0.22387\n",
      "Train Epoch: 4 [14160/51892 (27%)]\tLoss: BinaryCrossEntropyLoss 0.00872, SoftDiceLoss 0.18799, SumLoss 0.19671\n",
      "Train Epoch: 4 [14400/51892 (28%)]\tLoss: BinaryCrossEntropyLoss 0.00964, SoftDiceLoss 0.21208, SumLoss 0.22171\n",
      "Train Epoch: 4 [14640/51892 (28%)]\tLoss: BinaryCrossEntropyLoss 0.00841, SoftDiceLoss 0.13421, SumLoss 0.14261\n",
      "Train Epoch: 4 [14880/51892 (29%)]\tLoss: BinaryCrossEntropyLoss 0.00948, SoftDiceLoss 0.18318, SumLoss 0.19266\n",
      "Train Epoch: 4 [15120/51892 (29%)]\tLoss: BinaryCrossEntropyLoss 0.00810, SoftDiceLoss 0.16149, SumLoss 0.16960\n",
      "Train Epoch: 4 [15360/51892 (30%)]\tLoss: BinaryCrossEntropyLoss 0.01305, SoftDiceLoss 0.06223, SumLoss 0.07529\n",
      "Train Epoch: 4 [15600/51892 (30%)]\tLoss: BinaryCrossEntropyLoss 0.00677, SoftDiceLoss 0.12467, SumLoss 0.13143\n",
      "Train Epoch: 4 [15840/51892 (31%)]\tLoss: BinaryCrossEntropyLoss 0.00844, SoftDiceLoss 0.19825, SumLoss 0.20670\n",
      "Train Epoch: 4 [16080/51892 (31%)]\tLoss: BinaryCrossEntropyLoss 0.00600, SoftDiceLoss 0.17691, SumLoss 0.18290\n",
      "Train Epoch: 4 [16320/51892 (31%)]\tLoss: BinaryCrossEntropyLoss 0.00589, SoftDiceLoss 0.16382, SumLoss 0.16971\n",
      "Train Epoch: 4 [16560/51892 (32%)]\tLoss: BinaryCrossEntropyLoss 0.00855, SoftDiceLoss 0.14802, SumLoss 0.15656\n",
      "Train Epoch: 4 [16800/51892 (32%)]\tLoss: BinaryCrossEntropyLoss 0.01077, SoftDiceLoss 0.15995, SumLoss 0.17072\n",
      "Train Epoch: 4 [17040/51892 (33%)]\tLoss: BinaryCrossEntropyLoss 0.00765, SoftDiceLoss 0.14217, SumLoss 0.14982\n",
      "Train Epoch: 4 [17280/51892 (33%)]\tLoss: BinaryCrossEntropyLoss 0.01041, SoftDiceLoss 0.14818, SumLoss 0.15859\n",
      "Train Epoch: 4 [17520/51892 (34%)]\tLoss: BinaryCrossEntropyLoss 0.01019, SoftDiceLoss 0.15150, SumLoss 0.16170\n",
      "Train Epoch: 4 [17760/51892 (34%)]\tLoss: BinaryCrossEntropyLoss 0.00863, SoftDiceLoss 0.13815, SumLoss 0.14677\n",
      "Train Epoch: 4 [18000/51892 (35%)]\tLoss: BinaryCrossEntropyLoss 0.01107, SoftDiceLoss 0.18770, SumLoss 0.19877\n",
      "Train Epoch: 4 [18240/51892 (35%)]\tLoss: BinaryCrossEntropyLoss 0.00845, SoftDiceLoss 0.16450, SumLoss 0.17295\n",
      "Train Epoch: 4 [18480/51892 (36%)]\tLoss: BinaryCrossEntropyLoss 0.01099, SoftDiceLoss 0.18625, SumLoss 0.19724\n",
      "Train Epoch: 4 [18720/51892 (36%)]\tLoss: BinaryCrossEntropyLoss 0.01006, SoftDiceLoss 0.17024, SumLoss 0.18030\n",
      "Train Epoch: 4 [18960/51892 (37%)]\tLoss: BinaryCrossEntropyLoss 0.01229, SoftDiceLoss 0.19851, SumLoss 0.21080\n",
      "Train Epoch: 4 [19200/51892 (37%)]\tLoss: BinaryCrossEntropyLoss 0.01314, SoftDiceLoss 0.17618, SumLoss 0.18931\n",
      "Train Epoch: 4 [19440/51892 (37%)]\tLoss: BinaryCrossEntropyLoss 0.01080, SoftDiceLoss 0.19241, SumLoss 0.20322\n",
      "Train Epoch: 4 [19680/51892 (38%)]\tLoss: BinaryCrossEntropyLoss 0.01182, SoftDiceLoss 0.18116, SumLoss 0.19298\n",
      "Train Epoch: 4 [19920/51892 (38%)]\tLoss: BinaryCrossEntropyLoss 0.01136, SoftDiceLoss 0.18408, SumLoss 0.19544\n",
      "Train Epoch: 4 [20160/51892 (39%)]\tLoss: BinaryCrossEntropyLoss 0.01201, SoftDiceLoss 0.17498, SumLoss 0.18699\n",
      "Train Epoch: 4 [20400/51892 (39%)]\tLoss: BinaryCrossEntropyLoss 0.01202, SoftDiceLoss 0.18608, SumLoss 0.19809\n",
      "Train Epoch: 4 [20640/51892 (40%)]\tLoss: BinaryCrossEntropyLoss 0.01034, SoftDiceLoss 0.16061, SumLoss 0.17094\n",
      "Train Epoch: 4 [20880/51892 (40%)]\tLoss: BinaryCrossEntropyLoss 0.01022, SoftDiceLoss 0.15993, SumLoss 0.17014\n",
      "Train Epoch: 4 [21120/51892 (41%)]\tLoss: BinaryCrossEntropyLoss 0.00950, SoftDiceLoss 0.15188, SumLoss 0.16138\n",
      "Train Epoch: 4 [21360/51892 (41%)]\tLoss: BinaryCrossEntropyLoss 0.01088, SoftDiceLoss 0.16169, SumLoss 0.17257\n",
      "Train Epoch: 4 [21600/51892 (42%)]\tLoss: BinaryCrossEntropyLoss 0.01122, SoftDiceLoss 0.15725, SumLoss 0.16847\n",
      "Train Epoch: 4 [21840/51892 (42%)]\tLoss: BinaryCrossEntropyLoss 0.01033, SoftDiceLoss 0.07339, SumLoss 0.08372\n",
      "Train Epoch: 4 [22080/51892 (43%)]\tLoss: BinaryCrossEntropyLoss 0.00928, SoftDiceLoss 0.15505, SumLoss 0.16433\n",
      "Train Epoch: 4 [22320/51892 (43%)]\tLoss: BinaryCrossEntropyLoss 0.00832, SoftDiceLoss 0.21663, SumLoss 0.22495\n",
      "Train Epoch: 4 [22560/51892 (43%)]\tLoss: BinaryCrossEntropyLoss 0.00911, SoftDiceLoss 0.19863, SumLoss 0.20774\n",
      "Train Epoch: 4 [22800/51892 (44%)]\tLoss: BinaryCrossEntropyLoss 0.00778, SoftDiceLoss 0.19921, SumLoss 0.20698\n",
      "Train Epoch: 4 [23040/51892 (44%)]\tLoss: BinaryCrossEntropyLoss 0.00955, SoftDiceLoss 0.18472, SumLoss 0.19427\n",
      "Train Epoch: 4 [23280/51892 (45%)]\tLoss: BinaryCrossEntropyLoss 0.00776, SoftDiceLoss 0.12194, SumLoss 0.12970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [23520/51892 (45%)]\tLoss: BinaryCrossEntropyLoss 0.00838, SoftDiceLoss 0.18863, SumLoss 0.19701\n",
      "Train Epoch: 4 [23760/51892 (46%)]\tLoss: BinaryCrossEntropyLoss 0.00954, SoftDiceLoss 0.20324, SumLoss 0.21277\n",
      "Train Epoch: 4 [24000/51892 (46%)]\tLoss: BinaryCrossEntropyLoss 0.00872, SoftDiceLoss 0.18090, SumLoss 0.18962\n",
      "Train Epoch: 4 [24240/51892 (47%)]\tLoss: BinaryCrossEntropyLoss 0.00713, SoftDiceLoss 0.17321, SumLoss 0.18034\n",
      "Train Epoch: 4 [24480/51892 (47%)]\tLoss: BinaryCrossEntropyLoss 0.01021, SoftDiceLoss 0.19240, SumLoss 0.20261\n",
      "Train Epoch: 4 [24720/51892 (48%)]\tLoss: BinaryCrossEntropyLoss 0.00970, SoftDiceLoss 0.17555, SumLoss 0.18525\n",
      "Train Epoch: 4 [24960/51892 (48%)]\tLoss: BinaryCrossEntropyLoss 0.00861, SoftDiceLoss 0.21318, SumLoss 0.22178\n",
      "Train Epoch: 4 [25200/51892 (49%)]\tLoss: BinaryCrossEntropyLoss 0.00918, SoftDiceLoss 0.12986, SumLoss 0.13904\n",
      "Train Epoch: 4 [25440/51892 (49%)]\tLoss: BinaryCrossEntropyLoss 0.00768, SoftDiceLoss 0.18042, SumLoss 0.18810\n",
      "Train Epoch: 4 [25680/51892 (49%)]\tLoss: BinaryCrossEntropyLoss 0.01117, SoftDiceLoss 0.32561, SumLoss 0.33679\n",
      "Train Epoch: 4 [25920/51892 (50%)]\tLoss: BinaryCrossEntropyLoss 0.00718, SoftDiceLoss 0.16998, SumLoss 0.17716\n",
      "Train Epoch: 4 [26160/51892 (50%)]\tLoss: BinaryCrossEntropyLoss 0.00593, SoftDiceLoss 0.03706, SumLoss 0.04299\n",
      "Train Epoch: 4 [26400/51892 (51%)]\tLoss: BinaryCrossEntropyLoss 0.00738, SoftDiceLoss 0.00386, SumLoss 0.01124\n",
      "Train Epoch: 4 [26640/51892 (51%)]\tLoss: BinaryCrossEntropyLoss 0.00685, SoftDiceLoss 0.13982, SumLoss 0.14667\n",
      "Train Epoch: 4 [26880/51892 (52%)]\tLoss: BinaryCrossEntropyLoss 0.00821, SoftDiceLoss 0.19939, SumLoss 0.20760\n",
      "Train Epoch: 4 [27120/51892 (52%)]\tLoss: BinaryCrossEntropyLoss 0.00774, SoftDiceLoss 0.07199, SumLoss 0.07974\n",
      "Train Epoch: 4 [27360/51892 (53%)]\tLoss: BinaryCrossEntropyLoss 0.00819, SoftDiceLoss 0.19234, SumLoss 0.20054\n",
      "Train Epoch: 4 [27600/51892 (53%)]\tLoss: BinaryCrossEntropyLoss 0.00748, SoftDiceLoss 0.18551, SumLoss 0.19299\n",
      "Train Epoch: 4 [27840/51892 (54%)]\tLoss: BinaryCrossEntropyLoss 0.00807, SoftDiceLoss 0.11346, SumLoss 0.12154\n",
      "Train Epoch: 4 [28080/51892 (54%)]\tLoss: BinaryCrossEntropyLoss 0.00866, SoftDiceLoss 0.10676, SumLoss 0.11542\n",
      "Train Epoch: 4 [28320/51892 (55%)]\tLoss: BinaryCrossEntropyLoss 0.01198, SoftDiceLoss 0.21272, SumLoss 0.22470\n",
      "Train Epoch: 4 [28560/51892 (55%)]\tLoss: BinaryCrossEntropyLoss 0.01163, SoftDiceLoss 0.18607, SumLoss 0.19770\n",
      "Train Epoch: 4 [28800/51892 (55%)]\tLoss: BinaryCrossEntropyLoss 0.00843, SoftDiceLoss 0.18018, SumLoss 0.18861\n",
      "Train Epoch: 4 [29040/51892 (56%)]\tLoss: BinaryCrossEntropyLoss 0.00708, SoftDiceLoss 0.16842, SumLoss 0.17550\n",
      "Train Epoch: 4 [29280/51892 (56%)]\tLoss: BinaryCrossEntropyLoss 0.00790, SoftDiceLoss 0.16578, SumLoss 0.17369\n",
      "Train Epoch: 4 [29520/51892 (57%)]\tLoss: BinaryCrossEntropyLoss 0.01115, SoftDiceLoss 0.19722, SumLoss 0.20837\n",
      "Train Epoch: 4 [29760/51892 (57%)]\tLoss: BinaryCrossEntropyLoss 0.00947, SoftDiceLoss 0.16727, SumLoss 0.17674\n",
      "Train Epoch: 4 [30000/51892 (58%)]\tLoss: BinaryCrossEntropyLoss 0.01181, SoftDiceLoss 0.22351, SumLoss 0.23533\n",
      "Train Epoch: 4 [30240/51892 (58%)]\tLoss: BinaryCrossEntropyLoss 0.01415, SoftDiceLoss 0.25854, SumLoss 0.27269\n",
      "Train Epoch: 4 [30480/51892 (59%)]\tLoss: BinaryCrossEntropyLoss 0.00848, SoftDiceLoss 0.13151, SumLoss 0.13999\n",
      "Train Epoch: 4 [30720/51892 (59%)]\tLoss: BinaryCrossEntropyLoss 0.00972, SoftDiceLoss 0.12044, SumLoss 0.13015\n",
      "Train Epoch: 4 [30960/51892 (60%)]\tLoss: BinaryCrossEntropyLoss 0.00832, SoftDiceLoss 0.10791, SumLoss 0.11623\n",
      "Train Epoch: 4 [31200/51892 (60%)]\tLoss: BinaryCrossEntropyLoss 0.01480, SoftDiceLoss -0.10900, SumLoss -0.09420\n",
      "Train Epoch: 4 [31440/51892 (61%)]\tLoss: BinaryCrossEntropyLoss 0.02034, SoftDiceLoss 0.16866, SumLoss 0.18900\n",
      "Train Epoch: 4 [31680/51892 (61%)]\tLoss: BinaryCrossEntropyLoss 0.02030, SoftDiceLoss 0.30749, SumLoss 0.32779\n",
      "Train Epoch: 4 [31920/51892 (61%)]\tLoss: BinaryCrossEntropyLoss 0.04047, SoftDiceLoss 0.80982, SumLoss 0.85029\n",
      "Train Epoch: 4 [32160/51892 (62%)]\tLoss: BinaryCrossEntropyLoss 0.03510, SoftDiceLoss 0.60383, SumLoss 0.63893\n",
      "Train Epoch: 4 [32400/51892 (62%)]\tLoss: BinaryCrossEntropyLoss 0.03145, SoftDiceLoss 0.67897, SumLoss 0.71041\n",
      "Train Epoch: 4 [32640/51892 (63%)]\tLoss: BinaryCrossEntropyLoss 0.01806, SoftDiceLoss 0.33485, SumLoss 0.35290\n",
      "Train Epoch: 4 [32880/51892 (63%)]\tLoss: BinaryCrossEntropyLoss 0.02150, SoftDiceLoss 0.31820, SumLoss 0.33970\n",
      "Train Epoch: 4 [33120/51892 (64%)]\tLoss: BinaryCrossEntropyLoss 0.01502, SoftDiceLoss 0.33745, SumLoss 0.35247\n",
      "Train Epoch: 4 [33360/51892 (64%)]\tLoss: BinaryCrossEntropyLoss 0.01217, SoftDiceLoss 0.28997, SumLoss 0.30215\n",
      "Train Epoch: 4 [33600/51892 (65%)]\tLoss: BinaryCrossEntropyLoss 0.02188, SoftDiceLoss 0.43923, SumLoss 0.46110\n",
      "Train Epoch: 4 [33840/51892 (65%)]\tLoss: BinaryCrossEntropyLoss 0.00852, SoftDiceLoss 0.21034, SumLoss 0.21886\n",
      "Train Epoch: 4 [34080/51892 (66%)]\tLoss: BinaryCrossEntropyLoss 0.00700, SoftDiceLoss 0.18546, SumLoss 0.19245\n",
      "Train Epoch: 4 [34320/51892 (66%)]\tLoss: BinaryCrossEntropyLoss 0.01407, SoftDiceLoss 0.05145, SumLoss 0.06552\n",
      "Train Epoch: 4 [34560/51892 (67%)]\tLoss: BinaryCrossEntropyLoss 0.01596, SoftDiceLoss 0.16725, SumLoss 0.18320\n",
      "Train Epoch: 4 [34800/51892 (67%)]\tLoss: BinaryCrossEntropyLoss 0.01090, SoftDiceLoss 0.27102, SumLoss 0.28191\n",
      "Train Epoch: 4 [35040/51892 (67%)]\tLoss: BinaryCrossEntropyLoss 0.00890, SoftDiceLoss 0.20399, SumLoss 0.21289\n",
      "Train Epoch: 4 [35280/51892 (68%)]\tLoss: BinaryCrossEntropyLoss 0.01851, SoftDiceLoss 0.37235, SumLoss 0.39087\n",
      "Train Epoch: 4 [35520/51892 (68%)]\tLoss: BinaryCrossEntropyLoss 0.01392, SoftDiceLoss 0.29187, SumLoss 0.30579\n",
      "Train Epoch: 4 [35760/51892 (69%)]\tLoss: BinaryCrossEntropyLoss 0.01160, SoftDiceLoss 0.25592, SumLoss 0.26752\n",
      "Train Epoch: 4 [36000/51892 (69%)]\tLoss: BinaryCrossEntropyLoss 0.00894, SoftDiceLoss 0.11328, SumLoss 0.12223\n",
      "Train Epoch: 4 [36240/51892 (70%)]\tLoss: BinaryCrossEntropyLoss 0.01350, SoftDiceLoss 0.25374, SumLoss 0.26724\n",
      "Train Epoch: 4 [36480/51892 (70%)]\tLoss: BinaryCrossEntropyLoss 0.01466, SoftDiceLoss 0.28087, SumLoss 0.29553\n",
      "Train Epoch: 4 [36720/51892 (71%)]\tLoss: BinaryCrossEntropyLoss 0.01337, SoftDiceLoss 0.31780, SumLoss 0.33117\n",
      "Train Epoch: 4 [36960/51892 (71%)]\tLoss: BinaryCrossEntropyLoss 0.01442, SoftDiceLoss 0.25688, SumLoss 0.27130\n",
      "Train Epoch: 4 [37200/51892 (72%)]\tLoss: BinaryCrossEntropyLoss 0.01472, SoftDiceLoss 0.47906, SumLoss 0.49378\n",
      "Train Epoch: 4 [37440/51892 (72%)]\tLoss: BinaryCrossEntropyLoss 0.01412, SoftDiceLoss 0.56001, SumLoss 0.57413\n",
      "Train Epoch: 4 [37680/51892 (73%)]\tLoss: BinaryCrossEntropyLoss 0.01382, SoftDiceLoss 0.48245, SumLoss 0.49627\n",
      "Train Epoch: 4 [37920/51892 (73%)]\tLoss: BinaryCrossEntropyLoss 0.01150, SoftDiceLoss 0.42120, SumLoss 0.43271\n",
      "Train Epoch: 4 [38160/51892 (74%)]\tLoss: BinaryCrossEntropyLoss 0.01523, SoftDiceLoss 0.43872, SumLoss 0.45394\n",
      "Train Epoch: 4 [38400/51892 (74%)]\tLoss: BinaryCrossEntropyLoss 0.01531, SoftDiceLoss 0.26850, SumLoss 0.28380\n",
      "Train Epoch: 4 [38640/51892 (74%)]\tLoss: BinaryCrossEntropyLoss 0.01113, SoftDiceLoss 0.22282, SumLoss 0.23395\n",
      "Train Epoch: 4 [38880/51892 (75%)]\tLoss: BinaryCrossEntropyLoss 0.01251, SoftDiceLoss 0.24367, SumLoss 0.25617\n",
      "Train Epoch: 4 [39120/51892 (75%)]\tLoss: BinaryCrossEntropyLoss 0.00729, SoftDiceLoss 0.19792, SumLoss 0.20522\n",
      "Train Epoch: 4 [39360/51892 (76%)]\tLoss: BinaryCrossEntropyLoss 0.00791, SoftDiceLoss 0.18141, SumLoss 0.18932\n",
      "Train Epoch: 4 [39600/51892 (76%)]\tLoss: BinaryCrossEntropyLoss 0.00680, SoftDiceLoss 0.17908, SumLoss 0.18588\n",
      "Train Epoch: 4 [39840/51892 (77%)]\tLoss: BinaryCrossEntropyLoss 0.00783, SoftDiceLoss 0.19688, SumLoss 0.20472\n",
      "Train Epoch: 4 [40080/51892 (77%)]\tLoss: BinaryCrossEntropyLoss 0.00545, SoftDiceLoss 0.17928, SumLoss 0.18472\n",
      "Train Epoch: 4 [40320/51892 (78%)]\tLoss: BinaryCrossEntropyLoss 0.00944, SoftDiceLoss -0.00656, SumLoss 0.00289\n",
      "Train Epoch: 4 [40560/51892 (78%)]\tLoss: BinaryCrossEntropyLoss 0.00945, SoftDiceLoss 0.17301, SumLoss 0.18245\n",
      "Train Epoch: 4 [40800/51892 (79%)]\tLoss: BinaryCrossEntropyLoss 0.00947, SoftDiceLoss 0.21465, SumLoss 0.22411\n",
      "Train Epoch: 4 [41040/51892 (79%)]\tLoss: BinaryCrossEntropyLoss 0.00955, SoftDiceLoss 0.17812, SumLoss 0.18767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [41280/51892 (80%)]\tLoss: BinaryCrossEntropyLoss 0.01054, SoftDiceLoss 0.17550, SumLoss 0.18604\n",
      "Train Epoch: 4 [41520/51892 (80%)]\tLoss: BinaryCrossEntropyLoss 0.01037, SoftDiceLoss 0.21814, SumLoss 0.22851\n",
      "Train Epoch: 4 [41760/51892 (80%)]\tLoss: BinaryCrossEntropyLoss 0.01089, SoftDiceLoss 0.11684, SumLoss 0.12773\n",
      "Train Epoch: 4 [42000/51892 (81%)]\tLoss: BinaryCrossEntropyLoss 0.01182, SoftDiceLoss 0.19629, SumLoss 0.20811\n",
      "Train Epoch: 4 [42240/51892 (81%)]\tLoss: BinaryCrossEntropyLoss 0.01394, SoftDiceLoss 0.18261, SumLoss 0.19655\n",
      "Train Epoch: 4 [42480/51892 (82%)]\tLoss: BinaryCrossEntropyLoss 0.00817, SoftDiceLoss 0.17401, SumLoss 0.18217\n",
      "Train Epoch: 4 [42720/51892 (82%)]\tLoss: BinaryCrossEntropyLoss 0.00854, SoftDiceLoss 0.17433, SumLoss 0.18286\n",
      "Train Epoch: 4 [42960/51892 (83%)]\tLoss: BinaryCrossEntropyLoss 0.00866, SoftDiceLoss 0.18161, SumLoss 0.19027\n",
      "Train Epoch: 4 [43200/51892 (83%)]\tLoss: BinaryCrossEntropyLoss 0.00613, SoftDiceLoss 0.15409, SumLoss 0.16022\n",
      "Train Epoch: 4 [43440/51892 (84%)]\tLoss: BinaryCrossEntropyLoss 0.01918, SoftDiceLoss 0.49915, SumLoss 0.51833\n",
      "Train Epoch: 4 [43680/51892 (84%)]\tLoss: BinaryCrossEntropyLoss 0.00906, SoftDiceLoss 0.16779, SumLoss 0.17685\n",
      "Train Epoch: 4 [43920/51892 (85%)]\tLoss: BinaryCrossEntropyLoss 0.01212, SoftDiceLoss 0.13069, SumLoss 0.14281\n",
      "Train Epoch: 4 [44160/51892 (85%)]\tLoss: BinaryCrossEntropyLoss 0.01047, SoftDiceLoss 0.18521, SumLoss 0.19569\n",
      "Train Epoch: 4 [44400/51892 (86%)]\tLoss: BinaryCrossEntropyLoss 0.00807, SoftDiceLoss 0.08940, SumLoss 0.09747\n",
      "Train Epoch: 4 [44640/51892 (86%)]\tLoss: BinaryCrossEntropyLoss 0.00862, SoftDiceLoss 0.10881, SumLoss 0.11742\n",
      "Train Epoch: 4 [44880/51892 (86%)]\tLoss: BinaryCrossEntropyLoss 0.00959, SoftDiceLoss 0.12690, SumLoss 0.13650\n",
      "Train Epoch: 4 [45120/51892 (87%)]\tLoss: BinaryCrossEntropyLoss 0.00784, SoftDiceLoss 0.09748, SumLoss 0.10532\n",
      "Train Epoch: 4 [45360/51892 (87%)]\tLoss: BinaryCrossEntropyLoss 0.00945, SoftDiceLoss 0.20192, SumLoss 0.21138\n",
      "Train Epoch: 4 [45600/51892 (88%)]\tLoss: BinaryCrossEntropyLoss 0.00949, SoftDiceLoss 0.16444, SumLoss 0.17393\n",
      "Train Epoch: 4 [45840/51892 (88%)]\tLoss: BinaryCrossEntropyLoss 0.00991, SoftDiceLoss 0.13037, SumLoss 0.14028\n",
      "Train Epoch: 4 [46080/51892 (89%)]\tLoss: BinaryCrossEntropyLoss 0.00766, SoftDiceLoss 0.11773, SumLoss 0.12538\n",
      "Train Epoch: 4 [46320/51892 (89%)]\tLoss: BinaryCrossEntropyLoss 0.00818, SoftDiceLoss 0.16900, SumLoss 0.17718\n",
      "Train Epoch: 4 [46560/51892 (90%)]\tLoss: BinaryCrossEntropyLoss 0.00925, SoftDiceLoss 0.17764, SumLoss 0.18689\n",
      "Train Epoch: 4 [46800/51892 (90%)]\tLoss: BinaryCrossEntropyLoss 0.01510, SoftDiceLoss 0.18111, SumLoss 0.19621\n",
      "Train Epoch: 4 [47040/51892 (91%)]\tLoss: BinaryCrossEntropyLoss 0.01028, SoftDiceLoss 0.16096, SumLoss 0.17124\n",
      "Train Epoch: 4 [47280/51892 (91%)]\tLoss: BinaryCrossEntropyLoss 0.01351, SoftDiceLoss 0.20466, SumLoss 0.21817\n",
      "Train Epoch: 4 [47520/51892 (92%)]\tLoss: BinaryCrossEntropyLoss 0.00954, SoftDiceLoss 0.17865, SumLoss 0.18819\n",
      "Train Epoch: 4 [47760/51892 (92%)]\tLoss: BinaryCrossEntropyLoss 0.01108, SoftDiceLoss 0.16640, SumLoss 0.17748\n",
      "Train Epoch: 4 [48000/51892 (92%)]\tLoss: BinaryCrossEntropyLoss 0.00998, SoftDiceLoss 0.19063, SumLoss 0.20061\n",
      "Train Epoch: 4 [48240/51892 (93%)]\tLoss: BinaryCrossEntropyLoss 0.00719, SoftDiceLoss 0.12770, SumLoss 0.13489\n",
      "Train Epoch: 4 [48480/51892 (93%)]\tLoss: BinaryCrossEntropyLoss 0.00821, SoftDiceLoss 0.19012, SumLoss 0.19833\n",
      "Train Epoch: 4 [48720/51892 (94%)]\tLoss: BinaryCrossEntropyLoss 0.01310, SoftDiceLoss 0.22670, SumLoss 0.23980\n",
      "Train Epoch: 4 [48960/51892 (94%)]\tLoss: BinaryCrossEntropyLoss 0.01072, SoftDiceLoss 0.18642, SumLoss 0.19714\n",
      "Train Epoch: 4 [49200/51892 (95%)]\tLoss: BinaryCrossEntropyLoss 0.01019, SoftDiceLoss 0.19035, SumLoss 0.20054\n",
      "Train Epoch: 4 [49440/51892 (95%)]\tLoss: BinaryCrossEntropyLoss 0.00720, SoftDiceLoss 0.15137, SumLoss 0.15857\n",
      "Train Epoch: 4 [49680/51892 (96%)]\tLoss: BinaryCrossEntropyLoss 0.01069, SoftDiceLoss 0.19236, SumLoss 0.20305\n",
      "Train Epoch: 4 [49920/51892 (96%)]\tLoss: BinaryCrossEntropyLoss 0.01124, SoftDiceLoss 0.19108, SumLoss 0.20232\n",
      "Train Epoch: 4 [50160/51892 (97%)]\tLoss: BinaryCrossEntropyLoss 0.01082, SoftDiceLoss 0.21696, SumLoss 0.22777\n",
      "Train Epoch: 4 [50400/51892 (97%)]\tLoss: BinaryCrossEntropyLoss 0.01620, SoftDiceLoss 0.29428, SumLoss 0.31048\n",
      "Train Epoch: 4 [50640/51892 (98%)]\tLoss: BinaryCrossEntropyLoss 0.01813, SoftDiceLoss 0.19099, SumLoss 0.20912\n",
      "Train Epoch: 4 [50880/51892 (98%)]\tLoss: BinaryCrossEntropyLoss 0.01317, SoftDiceLoss 0.24261, SumLoss 0.25577\n",
      "Train Epoch: 4 [51120/51892 (98%)]\tLoss: BinaryCrossEntropyLoss 0.01779, SoftDiceLoss 0.27630, SumLoss 0.29409\n",
      "Train Epoch: 4 [51360/51892 (99%)]\tLoss: BinaryCrossEntropyLoss 0.01349, SoftDiceLoss 0.21556, SumLoss 0.22906\n",
      "Train Epoch: 4 [51600/51892 (99%)]\tLoss: BinaryCrossEntropyLoss 0.02282, SoftDiceLoss 0.33922, SumLoss 0.36203\n",
      "Train Epoch: 4 [51840/51892 (100%)]\tLoss: BinaryCrossEntropyLoss 0.01353, SoftDiceLoss 0.24971, SumLoss 0.26324\n",
      "Loss on validation: 0.260372473714\n",
      "Train Epoch: 5 [0/51892 (0%)]\tLoss: BinaryCrossEntropyLoss 0.01117, SoftDiceLoss 0.16966, SumLoss 0.18083\n",
      "Train Epoch: 5 [240/51892 (0%)]\tLoss: BinaryCrossEntropyLoss 0.00773, SoftDiceLoss 0.10424, SumLoss 0.11198\n",
      "Train Epoch: 5 [480/51892 (1%)]\tLoss: BinaryCrossEntropyLoss 0.01247, SoftDiceLoss 0.18614, SumLoss 0.19861\n",
      "Train Epoch: 5 [720/51892 (1%)]\tLoss: BinaryCrossEntropyLoss 0.01300, SoftDiceLoss 0.27960, SumLoss 0.29260\n",
      "Train Epoch: 5 [960/51892 (2%)]\tLoss: BinaryCrossEntropyLoss 0.00593, SoftDiceLoss 0.13718, SumLoss 0.14311\n",
      "Train Epoch: 5 [1200/51892 (2%)]\tLoss: BinaryCrossEntropyLoss 0.00635, SoftDiceLoss 0.15541, SumLoss 0.16176\n",
      "Train Epoch: 5 [1440/51892 (3%)]\tLoss: BinaryCrossEntropyLoss 0.00671, SoftDiceLoss 0.15266, SumLoss 0.15937\n",
      "Train Epoch: 5 [1680/51892 (3%)]\tLoss: BinaryCrossEntropyLoss 0.00647, SoftDiceLoss 0.15004, SumLoss 0.15652\n",
      "Train Epoch: 5 [1920/51892 (4%)]\tLoss: BinaryCrossEntropyLoss 0.00743, SoftDiceLoss 0.15106, SumLoss 0.15849\n",
      "Train Epoch: 5 [2160/51892 (4%)]\tLoss: BinaryCrossEntropyLoss 0.00642, SoftDiceLoss 0.16467, SumLoss 0.17108\n",
      "Train Epoch: 5 [2400/51892 (5%)]\tLoss: BinaryCrossEntropyLoss 0.00698, SoftDiceLoss 0.16008, SumLoss 0.16706\n",
      "Train Epoch: 5 [2640/51892 (5%)]\tLoss: BinaryCrossEntropyLoss 0.01201, SoftDiceLoss 0.22112, SumLoss 0.23313\n",
      "Train Epoch: 5 [2880/51892 (6%)]\tLoss: BinaryCrossEntropyLoss 0.01305, SoftDiceLoss 0.27795, SumLoss 0.29099\n",
      "Train Epoch: 5 [3120/51892 (6%)]\tLoss: BinaryCrossEntropyLoss 0.02120, SoftDiceLoss 0.45635, SumLoss 0.47755\n",
      "Train Epoch: 5 [3360/51892 (6%)]\tLoss: BinaryCrossEntropyLoss 0.00752, SoftDiceLoss 0.17053, SumLoss 0.17805\n",
      "Train Epoch: 5 [3600/51892 (7%)]\tLoss: BinaryCrossEntropyLoss 0.00729, SoftDiceLoss 0.19856, SumLoss 0.20585\n",
      "Train Epoch: 5 [3840/51892 (7%)]\tLoss: BinaryCrossEntropyLoss 0.01499, SoftDiceLoss 0.26415, SumLoss 0.27914\n",
      "Train Epoch: 5 [4080/51892 (8%)]\tLoss: BinaryCrossEntropyLoss 0.01445, SoftDiceLoss 0.24339, SumLoss 0.25784\n",
      "Train Epoch: 5 [4320/51892 (8%)]\tLoss: BinaryCrossEntropyLoss 0.01218, SoftDiceLoss 0.09954, SumLoss 0.11172\n",
      "Train Epoch: 5 [4560/51892 (9%)]\tLoss: BinaryCrossEntropyLoss 0.01732, SoftDiceLoss 0.20654, SumLoss 0.22386\n",
      "Train Epoch: 5 [4800/51892 (9%)]\tLoss: BinaryCrossEntropyLoss 0.01162, SoftDiceLoss 0.08128, SumLoss 0.09290\n",
      "Train Epoch: 5 [5040/51892 (10%)]\tLoss: BinaryCrossEntropyLoss 0.01972, SoftDiceLoss 0.32103, SumLoss 0.34075\n",
      "Train Epoch: 5 [5280/51892 (10%)]\tLoss: BinaryCrossEntropyLoss 0.01754, SoftDiceLoss 0.21480, SumLoss 0.23233\n",
      "Train Epoch: 5 [5520/51892 (11%)]\tLoss: BinaryCrossEntropyLoss 0.01324, SoftDiceLoss 0.19547, SumLoss 0.20871\n",
      "Train Epoch: 5 [5760/51892 (11%)]\tLoss: BinaryCrossEntropyLoss 0.01261, SoftDiceLoss 0.26079, SumLoss 0.27339\n",
      "Train Epoch: 5 [6000/51892 (12%)]\tLoss: BinaryCrossEntropyLoss 0.01421, SoftDiceLoss 0.22173, SumLoss 0.23594\n",
      "Train Epoch: 5 [6240/51892 (12%)]\tLoss: BinaryCrossEntropyLoss 0.01025, SoftDiceLoss 0.09237, SumLoss 0.10262\n",
      "Train Epoch: 5 [6480/51892 (12%)]\tLoss: BinaryCrossEntropyLoss 0.01115, SoftDiceLoss 0.17468, SumLoss 0.18583\n",
      "Train Epoch: 5 [6720/51892 (13%)]\tLoss: BinaryCrossEntropyLoss 0.01826, SoftDiceLoss 0.26299, SumLoss 0.28124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [6960/51892 (13%)]\tLoss: BinaryCrossEntropyLoss 0.00796, SoftDiceLoss 0.19207, SumLoss 0.20003\n",
      "Train Epoch: 5 [7200/51892 (14%)]\tLoss: BinaryCrossEntropyLoss 0.00858, SoftDiceLoss 0.17494, SumLoss 0.18353\n",
      "Train Epoch: 5 [7440/51892 (14%)]\tLoss: BinaryCrossEntropyLoss 0.00790, SoftDiceLoss -0.00607, SumLoss 0.00182\n",
      "Train Epoch: 5 [7680/51892 (15%)]\tLoss: BinaryCrossEntropyLoss 0.01018, SoftDiceLoss 0.22510, SumLoss 0.23528\n",
      "Train Epoch: 5 [7920/51892 (15%)]\tLoss: BinaryCrossEntropyLoss 0.00991, SoftDiceLoss 0.20322, SumLoss 0.21312\n",
      "Train Epoch: 5 [8160/51892 (16%)]\tLoss: BinaryCrossEntropyLoss 0.01601, SoftDiceLoss 0.19419, SumLoss 0.21020\n",
      "Train Epoch: 5 [8400/51892 (16%)]\tLoss: BinaryCrossEntropyLoss 0.01752, SoftDiceLoss 0.17624, SumLoss 0.19376\n",
      "Train Epoch: 5 [8640/51892 (17%)]\tLoss: BinaryCrossEntropyLoss 0.01222, SoftDiceLoss 0.18223, SumLoss 0.19445\n",
      "Train Epoch: 5 [8880/51892 (17%)]\tLoss: BinaryCrossEntropyLoss 0.00651, SoftDiceLoss 0.14957, SumLoss 0.15608\n",
      "Train Epoch: 5 [9120/51892 (18%)]\tLoss: BinaryCrossEntropyLoss 0.00878, SoftDiceLoss 0.14786, SumLoss 0.15663\n",
      "Train Epoch: 5 [9360/51892 (18%)]\tLoss: BinaryCrossEntropyLoss 0.00677, SoftDiceLoss 0.13778, SumLoss 0.14454\n",
      "Train Epoch: 5 [9600/51892 (18%)]\tLoss: BinaryCrossEntropyLoss 0.00789, SoftDiceLoss 0.14549, SumLoss 0.15338\n",
      "Train Epoch: 5 [9840/51892 (19%)]\tLoss: BinaryCrossEntropyLoss 0.00656, SoftDiceLoss 0.13439, SumLoss 0.14095\n",
      "Train Epoch: 5 [10080/51892 (19%)]\tLoss: BinaryCrossEntropyLoss 0.00669, SoftDiceLoss 0.14830, SumLoss 0.15499\n",
      "Train Epoch: 5 [10320/51892 (20%)]\tLoss: BinaryCrossEntropyLoss 0.00879, SoftDiceLoss 0.12005, SumLoss 0.12884\n",
      "Train Epoch: 5 [10560/51892 (20%)]\tLoss: BinaryCrossEntropyLoss 0.00809, SoftDiceLoss 0.11955, SumLoss 0.12764\n",
      "Train Epoch: 5 [10800/51892 (21%)]\tLoss: BinaryCrossEntropyLoss 0.00862, SoftDiceLoss 0.12807, SumLoss 0.13669\n",
      "Train Epoch: 5 [11040/51892 (21%)]\tLoss: BinaryCrossEntropyLoss 0.00769, SoftDiceLoss 0.11081, SumLoss 0.11850\n",
      "Train Epoch: 5 [11280/51892 (22%)]\tLoss: BinaryCrossEntropyLoss 0.00735, SoftDiceLoss 0.15270, SumLoss 0.16004\n",
      "Train Epoch: 5 [11520/51892 (22%)]\tLoss: BinaryCrossEntropyLoss 0.00735, SoftDiceLoss -0.03086, SumLoss -0.02351\n",
      "Train Epoch: 5 [11760/51892 (23%)]\tLoss: BinaryCrossEntropyLoss 0.00972, SoftDiceLoss 0.07109, SumLoss 0.08082\n",
      "Train Epoch: 5 [12000/51892 (23%)]\tLoss: BinaryCrossEntropyLoss 0.02187, SoftDiceLoss 0.19223, SumLoss 0.21410\n",
      "Train Epoch: 5 [12240/51892 (24%)]\tLoss: BinaryCrossEntropyLoss 0.00954, SoftDiceLoss 0.01226, SumLoss 0.02180\n",
      "Train Epoch: 5 [12480/51892 (24%)]\tLoss: BinaryCrossEntropyLoss 0.01203, SoftDiceLoss 0.26103, SumLoss 0.27306\n",
      "Train Epoch: 5 [12720/51892 (25%)]\tLoss: BinaryCrossEntropyLoss 0.01135, SoftDiceLoss 0.12997, SumLoss 0.14132\n",
      "Train Epoch: 5 [12960/51892 (25%)]\tLoss: BinaryCrossEntropyLoss 0.01001, SoftDiceLoss 0.12637, SumLoss 0.13637\n",
      "Train Epoch: 5 [13200/51892 (25%)]\tLoss: BinaryCrossEntropyLoss 0.01113, SoftDiceLoss 0.19723, SumLoss 0.20835\n",
      "Train Epoch: 5 [13440/51892 (26%)]\tLoss: BinaryCrossEntropyLoss 0.01099, SoftDiceLoss 0.13985, SumLoss 0.15083\n",
      "Train Epoch: 5 [13680/51892 (26%)]\tLoss: BinaryCrossEntropyLoss 0.00950, SoftDiceLoss 0.06189, SumLoss 0.07139\n",
      "Train Epoch: 5 [13920/51892 (27%)]\tLoss: BinaryCrossEntropyLoss 0.00982, SoftDiceLoss 0.20191, SumLoss 0.21173\n",
      "Train Epoch: 5 [14160/51892 (27%)]\tLoss: BinaryCrossEntropyLoss 0.00841, SoftDiceLoss 0.18031, SumLoss 0.18872\n",
      "Train Epoch: 5 [14400/51892 (28%)]\tLoss: BinaryCrossEntropyLoss 0.00926, SoftDiceLoss 0.20463, SumLoss 0.21389\n",
      "Train Epoch: 5 [14640/51892 (28%)]\tLoss: BinaryCrossEntropyLoss 0.00812, SoftDiceLoss 0.12746, SumLoss 0.13558\n",
      "Train Epoch: 5 [14880/51892 (29%)]\tLoss: BinaryCrossEntropyLoss 0.00913, SoftDiceLoss 0.17934, SumLoss 0.18846\n",
      "Train Epoch: 5 [15120/51892 (29%)]\tLoss: BinaryCrossEntropyLoss 0.00787, SoftDiceLoss 0.15925, SumLoss 0.16712\n",
      "Train Epoch: 5 [15360/51892 (30%)]\tLoss: BinaryCrossEntropyLoss 0.01229, SoftDiceLoss 0.05555, SumLoss 0.06784\n",
      "Train Epoch: 5 [15600/51892 (30%)]\tLoss: BinaryCrossEntropyLoss 0.00647, SoftDiceLoss 0.12045, SumLoss 0.12692\n",
      "Train Epoch: 5 [15840/51892 (31%)]\tLoss: BinaryCrossEntropyLoss 0.00799, SoftDiceLoss 0.18446, SumLoss 0.19245\n",
      "Train Epoch: 5 [16080/51892 (31%)]\tLoss: BinaryCrossEntropyLoss 0.00576, SoftDiceLoss 0.16857, SumLoss 0.17434\n",
      "Train Epoch: 5 [16320/51892 (31%)]\tLoss: BinaryCrossEntropyLoss 0.00564, SoftDiceLoss 0.15552, SumLoss 0.16116\n",
      "Train Epoch: 5 [16560/51892 (32%)]\tLoss: BinaryCrossEntropyLoss 0.00801, SoftDiceLoss 0.13940, SumLoss 0.14742\n",
      "Train Epoch: 5 [16800/51892 (32%)]\tLoss: BinaryCrossEntropyLoss 0.00983, SoftDiceLoss 0.14659, SumLoss 0.15642\n",
      "Train Epoch: 5 [17040/51892 (33%)]\tLoss: BinaryCrossEntropyLoss 0.00720, SoftDiceLoss 0.13419, SumLoss 0.14139\n",
      "Train Epoch: 5 [17280/51892 (33%)]\tLoss: BinaryCrossEntropyLoss 0.00977, SoftDiceLoss 0.13908, SumLoss 0.14884\n",
      "Train Epoch: 5 [17520/51892 (34%)]\tLoss: BinaryCrossEntropyLoss 0.00945, SoftDiceLoss 0.14367, SumLoss 0.15311\n",
      "Train Epoch: 5 [17760/51892 (34%)]\tLoss: BinaryCrossEntropyLoss 0.00817, SoftDiceLoss 0.13074, SumLoss 0.13892\n",
      "Train Epoch: 5 [18000/51892 (35%)]\tLoss: BinaryCrossEntropyLoss 0.01045, SoftDiceLoss 0.17950, SumLoss 0.18995\n",
      "Train Epoch: 5 [18240/51892 (35%)]\tLoss: BinaryCrossEntropyLoss 0.00796, SoftDiceLoss 0.15738, SumLoss 0.16535\n",
      "Train Epoch: 5 [18480/51892 (36%)]\tLoss: BinaryCrossEntropyLoss 0.01062, SoftDiceLoss 0.18100, SumLoss 0.19162\n",
      "Train Epoch: 5 [18720/51892 (36%)]\tLoss: BinaryCrossEntropyLoss 0.00968, SoftDiceLoss 0.16462, SumLoss 0.17430\n",
      "Train Epoch: 5 [18960/51892 (37%)]\tLoss: BinaryCrossEntropyLoss 0.01204, SoftDiceLoss 0.19523, SumLoss 0.20726\n",
      "Train Epoch: 5 [19200/51892 (37%)]\tLoss: BinaryCrossEntropyLoss 0.01264, SoftDiceLoss 0.17165, SumLoss 0.18429\n",
      "Train Epoch: 5 [19440/51892 (37%)]\tLoss: BinaryCrossEntropyLoss 0.01055, SoftDiceLoss 0.18864, SumLoss 0.19918\n",
      "Train Epoch: 5 [19680/51892 (38%)]\tLoss: BinaryCrossEntropyLoss 0.01140, SoftDiceLoss 0.17649, SumLoss 0.18789\n",
      "Train Epoch: 5 [19920/51892 (38%)]\tLoss: BinaryCrossEntropyLoss 0.01108, SoftDiceLoss 0.18115, SumLoss 0.19223\n",
      "Train Epoch: 5 [20160/51892 (39%)]\tLoss: BinaryCrossEntropyLoss 0.01167, SoftDiceLoss 0.17180, SumLoss 0.18347\n",
      "Train Epoch: 5 [20400/51892 (39%)]\tLoss: BinaryCrossEntropyLoss 0.01179, SoftDiceLoss 0.18321, SumLoss 0.19500\n",
      "Train Epoch: 5 [20640/51892 (40%)]\tLoss: BinaryCrossEntropyLoss 0.01000, SoftDiceLoss 0.15748, SumLoss 0.16748\n",
      "Train Epoch: 5 [20880/51892 (40%)]\tLoss: BinaryCrossEntropyLoss 0.00992, SoftDiceLoss 0.15626, SumLoss 0.16618\n",
      "Train Epoch: 5 [21120/51892 (41%)]\tLoss: BinaryCrossEntropyLoss 0.00910, SoftDiceLoss 0.14863, SumLoss 0.15774\n",
      "Train Epoch: 5 [21360/51892 (41%)]\tLoss: BinaryCrossEntropyLoss 0.01046, SoftDiceLoss 0.15843, SumLoss 0.16889\n",
      "Train Epoch: 5 [21600/51892 (42%)]\tLoss: BinaryCrossEntropyLoss 0.01089, SoftDiceLoss 0.15451, SumLoss 0.16540\n",
      "Train Epoch: 5 [21840/51892 (42%)]\tLoss: BinaryCrossEntropyLoss 0.01014, SoftDiceLoss 0.06999, SumLoss 0.08013\n",
      "Train Epoch: 5 [22080/51892 (43%)]\tLoss: BinaryCrossEntropyLoss 0.00905, SoftDiceLoss 0.15154, SumLoss 0.16059\n",
      "Train Epoch: 5 [22320/51892 (43%)]\tLoss: BinaryCrossEntropyLoss 0.00827, SoftDiceLoss 0.20999, SumLoss 0.21826\n",
      "Train Epoch: 5 [22560/51892 (43%)]\tLoss: BinaryCrossEntropyLoss 0.00899, SoftDiceLoss 0.19341, SumLoss 0.20240\n",
      "Train Epoch: 5 [22800/51892 (44%)]\tLoss: BinaryCrossEntropyLoss 0.00754, SoftDiceLoss 0.19231, SumLoss 0.19985\n",
      "Train Epoch: 5 [23040/51892 (44%)]\tLoss: BinaryCrossEntropyLoss 0.00924, SoftDiceLoss 0.17826, SumLoss 0.18751\n",
      "Train Epoch: 5 [23280/51892 (45%)]\tLoss: BinaryCrossEntropyLoss 0.00746, SoftDiceLoss 0.11517, SumLoss 0.12263\n",
      "Train Epoch: 5 [23520/51892 (45%)]\tLoss: BinaryCrossEntropyLoss 0.00813, SoftDiceLoss 0.18428, SumLoss 0.19241\n",
      "Train Epoch: 5 [23760/51892 (46%)]\tLoss: BinaryCrossEntropyLoss 0.00924, SoftDiceLoss 0.19600, SumLoss 0.20524\n",
      "Train Epoch: 5 [24000/51892 (46%)]\tLoss: BinaryCrossEntropyLoss 0.00837, SoftDiceLoss 0.17397, SumLoss 0.18234\n",
      "Train Epoch: 5 [24240/51892 (47%)]\tLoss: BinaryCrossEntropyLoss 0.00692, SoftDiceLoss 0.16865, SumLoss 0.17557\n",
      "Train Epoch: 5 [24480/51892 (47%)]\tLoss: BinaryCrossEntropyLoss 0.00984, SoftDiceLoss 0.18211, SumLoss 0.19195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [24720/51892 (48%)]\tLoss: BinaryCrossEntropyLoss 0.00936, SoftDiceLoss 0.17070, SumLoss 0.18006\n",
      "Train Epoch: 5 [24960/51892 (48%)]\tLoss: BinaryCrossEntropyLoss 0.00836, SoftDiceLoss 0.20885, SumLoss 0.21721\n",
      "Train Epoch: 5 [25200/51892 (49%)]\tLoss: BinaryCrossEntropyLoss 0.00884, SoftDiceLoss 0.12409, SumLoss 0.13293\n",
      "Train Epoch: 5 [25440/51892 (49%)]\tLoss: BinaryCrossEntropyLoss 0.00739, SoftDiceLoss 0.17414, SumLoss 0.18154\n",
      "Train Epoch: 5 [25680/51892 (49%)]\tLoss: BinaryCrossEntropyLoss 0.01127, SoftDiceLoss 0.32212, SumLoss 0.33339\n",
      "Train Epoch: 5 [25920/51892 (50%)]\tLoss: BinaryCrossEntropyLoss 0.00679, SoftDiceLoss 0.16169, SumLoss 0.16848\n",
      "Train Epoch: 5 [26160/51892 (50%)]\tLoss: BinaryCrossEntropyLoss 0.00562, SoftDiceLoss 0.02872, SumLoss 0.03434\n",
      "Train Epoch: 5 [26400/51892 (51%)]\tLoss: BinaryCrossEntropyLoss 0.00691, SoftDiceLoss -0.00586, SumLoss 0.00105\n",
      "Train Epoch: 5 [26640/51892 (51%)]\tLoss: BinaryCrossEntropyLoss 0.00664, SoftDiceLoss 0.13688, SumLoss 0.14353\n",
      "Train Epoch: 5 [26880/51892 (52%)]\tLoss: BinaryCrossEntropyLoss 0.00794, SoftDiceLoss 0.19397, SumLoss 0.20191\n",
      "Train Epoch: 5 [27120/51892 (52%)]\tLoss: BinaryCrossEntropyLoss 0.00751, SoftDiceLoss 0.06807, SumLoss 0.07558\n",
      "Train Epoch: 5 [27360/51892 (53%)]\tLoss: BinaryCrossEntropyLoss 0.00787, SoftDiceLoss 0.18331, SumLoss 0.19118\n",
      "Train Epoch: 5 [27600/51892 (53%)]\tLoss: BinaryCrossEntropyLoss 0.00706, SoftDiceLoss 0.17625, SumLoss 0.18331\n",
      "Train Epoch: 5 [27840/51892 (54%)]\tLoss: BinaryCrossEntropyLoss 0.00784, SoftDiceLoss 0.10940, SumLoss 0.11724\n",
      "Train Epoch: 5 [28080/51892 (54%)]\tLoss: BinaryCrossEntropyLoss 0.00845, SoftDiceLoss 0.10229, SumLoss 0.11074\n",
      "Train Epoch: 5 [28320/51892 (55%)]\tLoss: BinaryCrossEntropyLoss 0.01109, SoftDiceLoss 0.20257, SumLoss 0.21366\n",
      "Train Epoch: 5 [28560/51892 (55%)]\tLoss: BinaryCrossEntropyLoss 0.01087, SoftDiceLoss 0.17889, SumLoss 0.18977\n",
      "Train Epoch: 5 [28800/51892 (55%)]\tLoss: BinaryCrossEntropyLoss 0.00789, SoftDiceLoss 0.17337, SumLoss 0.18126\n",
      "Train Epoch: 5 [29040/51892 (56%)]\tLoss: BinaryCrossEntropyLoss 0.00659, SoftDiceLoss 0.16004, SumLoss 0.16663\n",
      "Train Epoch: 5 [29280/51892 (56%)]\tLoss: BinaryCrossEntropyLoss 0.00769, SoftDiceLoss 0.15968, SumLoss 0.16737\n",
      "Train Epoch: 5 [29520/51892 (57%)]\tLoss: BinaryCrossEntropyLoss 0.01076, SoftDiceLoss 0.19067, SumLoss 0.20144\n",
      "Train Epoch: 5 [29760/51892 (57%)]\tLoss: BinaryCrossEntropyLoss 0.00911, SoftDiceLoss 0.16256, SumLoss 0.17168\n",
      "Train Epoch: 5 [30000/51892 (58%)]\tLoss: BinaryCrossEntropyLoss 0.01137, SoftDiceLoss 0.21343, SumLoss 0.22480\n",
      "Train Epoch: 5 [30240/51892 (58%)]\tLoss: BinaryCrossEntropyLoss 0.01393, SoftDiceLoss 0.25369, SumLoss 0.26762\n",
      "Train Epoch: 5 [30480/51892 (59%)]\tLoss: BinaryCrossEntropyLoss 0.00819, SoftDiceLoss 0.12770, SumLoss 0.13589\n",
      "Train Epoch: 5 [30720/51892 (59%)]\tLoss: BinaryCrossEntropyLoss 0.00953, SoftDiceLoss 0.11524, SumLoss 0.12476\n",
      "Train Epoch: 5 [30960/51892 (60%)]\tLoss: BinaryCrossEntropyLoss 0.00822, SoftDiceLoss 0.10380, SumLoss 0.11203\n",
      "Train Epoch: 5 [31200/51892 (60%)]\tLoss: BinaryCrossEntropyLoss 0.01450, SoftDiceLoss -0.11152, SumLoss -0.09701\n",
      "Train Epoch: 5 [31440/51892 (61%)]\tLoss: BinaryCrossEntropyLoss 0.02022, SoftDiceLoss 0.16531, SumLoss 0.18553\n",
      "Train Epoch: 5 [31680/51892 (61%)]\tLoss: BinaryCrossEntropyLoss 0.01943, SoftDiceLoss 0.29768, SumLoss 0.31710\n",
      "Train Epoch: 5 [31920/51892 (61%)]\tLoss: BinaryCrossEntropyLoss 0.03299, SoftDiceLoss 0.77191, SumLoss 0.80491\n",
      "Train Epoch: 5 [32160/51892 (62%)]\tLoss: BinaryCrossEntropyLoss 0.02988, SoftDiceLoss 0.59255, SumLoss 0.62243\n",
      "Train Epoch: 5 [32400/51892 (62%)]\tLoss: BinaryCrossEntropyLoss 0.02233, SoftDiceLoss 0.63986, SumLoss 0.66219\n",
      "Train Epoch: 5 [32640/51892 (63%)]\tLoss: BinaryCrossEntropyLoss 0.01701, SoftDiceLoss 0.32323, SumLoss 0.34024\n",
      "Train Epoch: 5 [32880/51892 (63%)]\tLoss: BinaryCrossEntropyLoss 0.01996, SoftDiceLoss 0.30186, SumLoss 0.32182\n",
      "Train Epoch: 5 [33120/51892 (64%)]\tLoss: BinaryCrossEntropyLoss 0.01464, SoftDiceLoss 0.33334, SumLoss 0.34798\n",
      "Train Epoch: 5 [33360/51892 (64%)]\tLoss: BinaryCrossEntropyLoss 0.01171, SoftDiceLoss 0.27896, SumLoss 0.29067\n",
      "Train Epoch: 5 [33600/51892 (65%)]\tLoss: BinaryCrossEntropyLoss 0.02132, SoftDiceLoss 0.43485, SumLoss 0.45617\n",
      "Train Epoch: 5 [33840/51892 (65%)]\tLoss: BinaryCrossEntropyLoss 0.00842, SoftDiceLoss 0.20952, SumLoss 0.21794\n",
      "Train Epoch: 5 [34080/51892 (66%)]\tLoss: BinaryCrossEntropyLoss 0.00669, SoftDiceLoss 0.17665, SumLoss 0.18335\n",
      "Train Epoch: 5 [34320/51892 (66%)]\tLoss: BinaryCrossEntropyLoss 0.01367, SoftDiceLoss 0.04683, SumLoss 0.06049\n",
      "Train Epoch: 5 [34560/51892 (67%)]\tLoss: BinaryCrossEntropyLoss 0.01515, SoftDiceLoss 0.16207, SumLoss 0.17722\n",
      "Train Epoch: 5 [34800/51892 (67%)]\tLoss: BinaryCrossEntropyLoss 0.01068, SoftDiceLoss 0.26489, SumLoss 0.27557\n",
      "Train Epoch: 5 [35040/51892 (67%)]\tLoss: BinaryCrossEntropyLoss 0.00868, SoftDiceLoss 0.19873, SumLoss 0.20742\n",
      "Train Epoch: 5 [35280/51892 (68%)]\tLoss: BinaryCrossEntropyLoss 0.01802, SoftDiceLoss 0.37288, SumLoss 0.39089\n",
      "Train Epoch: 5 [35520/51892 (68%)]\tLoss: BinaryCrossEntropyLoss 0.01330, SoftDiceLoss 0.28038, SumLoss 0.29368\n",
      "Train Epoch: 5 [35760/51892 (69%)]\tLoss: BinaryCrossEntropyLoss 0.01122, SoftDiceLoss 0.24749, SumLoss 0.25872\n",
      "Train Epoch: 5 [36000/51892 (69%)]\tLoss: BinaryCrossEntropyLoss 0.00777, SoftDiceLoss 0.09958, SumLoss 0.10735\n",
      "Train Epoch: 5 [36240/51892 (70%)]\tLoss: BinaryCrossEntropyLoss 0.01373, SoftDiceLoss 0.25327, SumLoss 0.26701\n",
      "Train Epoch: 5 [36480/51892 (70%)]\tLoss: BinaryCrossEntropyLoss 0.01428, SoftDiceLoss 0.27782, SumLoss 0.29210\n",
      "Train Epoch: 5 [36720/51892 (71%)]\tLoss: BinaryCrossEntropyLoss 0.01321, SoftDiceLoss 0.31340, SumLoss 0.32660\n",
      "Train Epoch: 5 [36960/51892 (71%)]\tLoss: BinaryCrossEntropyLoss 0.01428, SoftDiceLoss 0.25445, SumLoss 0.26874\n",
      "Train Epoch: 5 [37200/51892 (72%)]\tLoss: BinaryCrossEntropyLoss 0.01394, SoftDiceLoss 0.46154, SumLoss 0.47549\n",
      "Train Epoch: 5 [37440/51892 (72%)]\tLoss: BinaryCrossEntropyLoss 0.01308, SoftDiceLoss 0.53404, SumLoss 0.54712\n",
      "Train Epoch: 5 [37680/51892 (73%)]\tLoss: BinaryCrossEntropyLoss 0.01255, SoftDiceLoss 0.46972, SumLoss 0.48227\n",
      "Train Epoch: 5 [37920/51892 (73%)]\tLoss: BinaryCrossEntropyLoss 0.00975, SoftDiceLoss 0.40181, SumLoss 0.41156\n",
      "Train Epoch: 5 [38160/51892 (74%)]\tLoss: BinaryCrossEntropyLoss 0.01430, SoftDiceLoss 0.42261, SumLoss 0.43691\n",
      "Train Epoch: 5 [38400/51892 (74%)]\tLoss: BinaryCrossEntropyLoss 0.01439, SoftDiceLoss 0.25448, SumLoss 0.26887\n",
      "Train Epoch: 5 [38640/51892 (74%)]\tLoss: BinaryCrossEntropyLoss 0.01090, SoftDiceLoss 0.21695, SumLoss 0.22785\n",
      "Train Epoch: 5 [38880/51892 (75%)]\tLoss: BinaryCrossEntropyLoss 0.01203, SoftDiceLoss 0.23684, SumLoss 0.24886\n",
      "Train Epoch: 5 [39120/51892 (75%)]\tLoss: BinaryCrossEntropyLoss 0.00683, SoftDiceLoss 0.18769, SumLoss 0.19452\n",
      "Train Epoch: 5 [39360/51892 (76%)]\tLoss: BinaryCrossEntropyLoss 0.00764, SoftDiceLoss 0.17489, SumLoss 0.18253\n",
      "Train Epoch: 5 [39600/51892 (76%)]\tLoss: BinaryCrossEntropyLoss 0.00652, SoftDiceLoss 0.17294, SumLoss 0.17947\n",
      "Train Epoch: 5 [39840/51892 (77%)]\tLoss: BinaryCrossEntropyLoss 0.00769, SoftDiceLoss 0.19277, SumLoss 0.20046\n",
      "Train Epoch: 5 [40080/51892 (77%)]\tLoss: BinaryCrossEntropyLoss 0.00538, SoftDiceLoss 0.17643, SumLoss 0.18181\n",
      "Train Epoch: 5 [40320/51892 (78%)]\tLoss: BinaryCrossEntropyLoss 0.00875, SoftDiceLoss -0.01642, SumLoss -0.00767\n",
      "Train Epoch: 5 [40560/51892 (78%)]\tLoss: BinaryCrossEntropyLoss 0.00890, SoftDiceLoss 0.16328, SumLoss 0.17217\n",
      "Train Epoch: 5 [40800/51892 (79%)]\tLoss: BinaryCrossEntropyLoss 0.00921, SoftDiceLoss 0.21020, SumLoss 0.21941\n",
      "Train Epoch: 5 [41040/51892 (79%)]\tLoss: BinaryCrossEntropyLoss 0.00932, SoftDiceLoss 0.17393, SumLoss 0.18326\n",
      "Train Epoch: 5 [41280/51892 (80%)]\tLoss: BinaryCrossEntropyLoss 0.01028, SoftDiceLoss 0.17125, SumLoss 0.18154\n",
      "Train Epoch: 5 [41520/51892 (80%)]\tLoss: BinaryCrossEntropyLoss 0.00993, SoftDiceLoss 0.20300, SumLoss 0.21293\n",
      "Train Epoch: 5 [41760/51892 (80%)]\tLoss: BinaryCrossEntropyLoss 0.01053, SoftDiceLoss 0.11405, SumLoss 0.12458\n",
      "Train Epoch: 5 [42000/51892 (81%)]\tLoss: BinaryCrossEntropyLoss 0.01137, SoftDiceLoss 0.19233, SumLoss 0.20370\n",
      "Train Epoch: 5 [42240/51892 (81%)]\tLoss: BinaryCrossEntropyLoss 0.01359, SoftDiceLoss 0.17685, SumLoss 0.19044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [42480/51892 (82%)]\tLoss: BinaryCrossEntropyLoss 0.00772, SoftDiceLoss 0.16730, SumLoss 0.17502\n",
      "Train Epoch: 5 [42720/51892 (82%)]\tLoss: BinaryCrossEntropyLoss 0.00796, SoftDiceLoss 0.16571, SumLoss 0.17366\n",
      "Train Epoch: 5 [42960/51892 (83%)]\tLoss: BinaryCrossEntropyLoss 0.00822, SoftDiceLoss 0.17494, SumLoss 0.18316\n",
      "Train Epoch: 5 [43200/51892 (83%)]\tLoss: BinaryCrossEntropyLoss 0.00584, SoftDiceLoss 0.14854, SumLoss 0.15438\n",
      "Train Epoch: 5 [43440/51892 (84%)]\tLoss: BinaryCrossEntropyLoss 0.01701, SoftDiceLoss 0.47795, SumLoss 0.49496\n",
      "Train Epoch: 5 [43680/51892 (84%)]\tLoss: BinaryCrossEntropyLoss 0.00854, SoftDiceLoss 0.15197, SumLoss 0.16050\n",
      "Train Epoch: 5 [43920/51892 (85%)]\tLoss: BinaryCrossEntropyLoss 0.01189, SoftDiceLoss 0.12724, SumLoss 0.13913\n",
      "Train Epoch: 5 [44160/51892 (85%)]\tLoss: BinaryCrossEntropyLoss 0.01007, SoftDiceLoss 0.17720, SumLoss 0.18727\n",
      "Train Epoch: 5 [44400/51892 (86%)]\tLoss: BinaryCrossEntropyLoss 0.00788, SoftDiceLoss 0.08537, SumLoss 0.09325\n",
      "Train Epoch: 5 [44640/51892 (86%)]\tLoss: BinaryCrossEntropyLoss 0.00858, SoftDiceLoss 0.10085, SumLoss 0.10943\n",
      "Train Epoch: 5 [44880/51892 (86%)]\tLoss: BinaryCrossEntropyLoss 0.00928, SoftDiceLoss 0.11854, SumLoss 0.12782\n",
      "Train Epoch: 5 [45120/51892 (87%)]\tLoss: BinaryCrossEntropyLoss 0.00754, SoftDiceLoss 0.09210, SumLoss 0.09964\n",
      "Train Epoch: 5 [45360/51892 (87%)]\tLoss: BinaryCrossEntropyLoss 0.00931, SoftDiceLoss 0.19824, SumLoss 0.20754\n",
      "Train Epoch: 5 [45600/51892 (88%)]\tLoss: BinaryCrossEntropyLoss 0.00930, SoftDiceLoss 0.16018, SumLoss 0.16949\n",
      "Train Epoch: 5 [45840/51892 (88%)]\tLoss: BinaryCrossEntropyLoss 0.00967, SoftDiceLoss 0.12622, SumLoss 0.13590\n",
      "Train Epoch: 5 [46080/51892 (89%)]\tLoss: BinaryCrossEntropyLoss 0.00748, SoftDiceLoss 0.11330, SumLoss 0.12078\n",
      "Train Epoch: 5 [46320/51892 (89%)]\tLoss: BinaryCrossEntropyLoss 0.00792, SoftDiceLoss 0.16330, SumLoss 0.17122\n",
      "Train Epoch: 5 [46560/51892 (90%)]\tLoss: BinaryCrossEntropyLoss 0.00896, SoftDiceLoss 0.16841, SumLoss 0.17738\n",
      "Train Epoch: 5 [46800/51892 (90%)]\tLoss: BinaryCrossEntropyLoss 0.01463, SoftDiceLoss 0.17366, SumLoss 0.18830\n",
      "Train Epoch: 5 [47040/51892 (91%)]\tLoss: BinaryCrossEntropyLoss 0.00996, SoftDiceLoss 0.15421, SumLoss 0.16418\n",
      "Train Epoch: 5 [47280/51892 (91%)]\tLoss: BinaryCrossEntropyLoss 0.01317, SoftDiceLoss 0.19756, SumLoss 0.21072\n",
      "Train Epoch: 5 [47520/51892 (92%)]\tLoss: BinaryCrossEntropyLoss 0.00938, SoftDiceLoss 0.17258, SumLoss 0.18197\n",
      "Train Epoch: 5 [47760/51892 (92%)]\tLoss: BinaryCrossEntropyLoss 0.01079, SoftDiceLoss 0.16027, SumLoss 0.17106\n",
      "Train Epoch: 5 [48000/51892 (92%)]\tLoss: BinaryCrossEntropyLoss 0.00970, SoftDiceLoss 0.18291, SumLoss 0.19260\n",
      "Train Epoch: 5 [48240/51892 (93%)]\tLoss: BinaryCrossEntropyLoss 0.00702, SoftDiceLoss 0.12398, SumLoss 0.13101\n",
      "Train Epoch: 5 [48480/51892 (93%)]\tLoss: BinaryCrossEntropyLoss 0.00810, SoftDiceLoss 0.18543, SumLoss 0.19353\n",
      "Train Epoch: 5 [48720/51892 (94%)]\tLoss: BinaryCrossEntropyLoss 0.01275, SoftDiceLoss 0.21409, SumLoss 0.22684\n",
      "Train Epoch: 5 [48960/51892 (94%)]\tLoss: BinaryCrossEntropyLoss 0.01042, SoftDiceLoss 0.17851, SumLoss 0.18893\n",
      "Train Epoch: 5 [49200/51892 (95%)]\tLoss: BinaryCrossEntropyLoss 0.01013, SoftDiceLoss 0.18794, SumLoss 0.19807\n",
      "Train Epoch: 5 [49440/51892 (95%)]\tLoss: BinaryCrossEntropyLoss 0.00706, SoftDiceLoss 0.14732, SumLoss 0.15438\n",
      "Train Epoch: 5 [49680/51892 (96%)]\tLoss: BinaryCrossEntropyLoss 0.01049, SoftDiceLoss 0.18737, SumLoss 0.19786\n",
      "Train Epoch: 5 [49920/51892 (96%)]\tLoss: BinaryCrossEntropyLoss 0.01117, SoftDiceLoss 0.18727, SumLoss 0.19845\n",
      "Train Epoch: 5 [50160/51892 (97%)]\tLoss: BinaryCrossEntropyLoss 0.01063, SoftDiceLoss 0.21233, SumLoss 0.22296\n",
      "Train Epoch: 5 [50400/51892 (97%)]\tLoss: BinaryCrossEntropyLoss 0.01592, SoftDiceLoss 0.28756, SumLoss 0.30348\n",
      "Train Epoch: 5 [50640/51892 (98%)]\tLoss: BinaryCrossEntropyLoss 0.01792, SoftDiceLoss 0.18239, SumLoss 0.20030\n",
      "Train Epoch: 5 [50880/51892 (98%)]\tLoss: BinaryCrossEntropyLoss 0.01319, SoftDiceLoss 0.23763, SumLoss 0.25082\n",
      "Train Epoch: 5 [51120/51892 (98%)]\tLoss: BinaryCrossEntropyLoss 0.01798, SoftDiceLoss 0.26847, SumLoss 0.28645\n",
      "Train Epoch: 5 [51360/51892 (99%)]\tLoss: BinaryCrossEntropyLoss 0.01316, SoftDiceLoss 0.20640, SumLoss 0.21955\n",
      "Train Epoch: 5 [51600/51892 (99%)]\tLoss: BinaryCrossEntropyLoss 0.02372, SoftDiceLoss 0.33961, SumLoss 0.36333\n",
      "Train Epoch: 5 [51840/51892 (100%)]\tLoss: BinaryCrossEntropyLoss 0.01327, SoftDiceLoss 0.24425, SumLoss 0.25752\n",
      "Loss on validation: 0.254986321855\n",
      "Train Epoch: 6 [0/51892 (0%)]\tLoss: BinaryCrossEntropyLoss 0.01090, SoftDiceLoss 0.16611, SumLoss 0.17701\n",
      "Train Epoch: 6 [240/51892 (0%)]\tLoss: BinaryCrossEntropyLoss 0.00747, SoftDiceLoss 0.10006, SumLoss 0.10753\n",
      "Train Epoch: 6 [480/51892 (1%)]\tLoss: BinaryCrossEntropyLoss 0.01177, SoftDiceLoss 0.17873, SumLoss 0.19051\n",
      "Train Epoch: 6 [720/51892 (1%)]\tLoss: BinaryCrossEntropyLoss 0.01296, SoftDiceLoss 0.27491, SumLoss 0.28787\n",
      "Train Epoch: 6 [960/51892 (2%)]\tLoss: BinaryCrossEntropyLoss 0.00569, SoftDiceLoss 0.13172, SumLoss 0.13741\n",
      "Train Epoch: 6 [1200/51892 (2%)]\tLoss: BinaryCrossEntropyLoss 0.00608, SoftDiceLoss 0.14946, SumLoss 0.15555\n",
      "Train Epoch: 6 [1440/51892 (3%)]\tLoss: BinaryCrossEntropyLoss 0.00647, SoftDiceLoss 0.14825, SumLoss 0.15471\n",
      "Train Epoch: 6 [1680/51892 (3%)]\tLoss: BinaryCrossEntropyLoss 0.00622, SoftDiceLoss 0.14472, SumLoss 0.15093\n",
      "Train Epoch: 6 [1920/51892 (4%)]\tLoss: BinaryCrossEntropyLoss 0.00712, SoftDiceLoss 0.14579, SumLoss 0.15291\n",
      "Train Epoch: 6 [2160/51892 (4%)]\tLoss: BinaryCrossEntropyLoss 0.00613, SoftDiceLoss 0.15863, SumLoss 0.16476\n",
      "Train Epoch: 6 [2400/51892 (5%)]\tLoss: BinaryCrossEntropyLoss 0.00664, SoftDiceLoss 0.15373, SumLoss 0.16037\n",
      "Train Epoch: 6 [2640/51892 (5%)]\tLoss: BinaryCrossEntropyLoss 0.01189, SoftDiceLoss 0.21684, SumLoss 0.22873\n",
      "Train Epoch: 6 [2880/51892 (6%)]\tLoss: BinaryCrossEntropyLoss 0.01283, SoftDiceLoss 0.27101, SumLoss 0.28384\n",
      "Train Epoch: 6 [3120/51892 (6%)]\tLoss: BinaryCrossEntropyLoss 0.02186, SoftDiceLoss 0.44768, SumLoss 0.46954\n",
      "Train Epoch: 6 [3360/51892 (6%)]\tLoss: BinaryCrossEntropyLoss 0.00716, SoftDiceLoss 0.16199, SumLoss 0.16915\n",
      "Train Epoch: 6 [3600/51892 (7%)]\tLoss: BinaryCrossEntropyLoss 0.00702, SoftDiceLoss 0.19253, SumLoss 0.19955\n",
      "Train Epoch: 6 [3840/51892 (7%)]\tLoss: BinaryCrossEntropyLoss 0.01437, SoftDiceLoss 0.25433, SumLoss 0.26870\n",
      "Train Epoch: 6 [4080/51892 (8%)]\tLoss: BinaryCrossEntropyLoss 0.01437, SoftDiceLoss 0.23832, SumLoss 0.25269\n",
      "Train Epoch: 6 [4320/51892 (8%)]\tLoss: BinaryCrossEntropyLoss 0.01174, SoftDiceLoss 0.09377, SumLoss 0.10550\n",
      "Train Epoch: 6 [4560/51892 (9%)]\tLoss: BinaryCrossEntropyLoss 0.01687, SoftDiceLoss 0.20290, SumLoss 0.21977\n",
      "Train Epoch: 6 [4800/51892 (9%)]\tLoss: BinaryCrossEntropyLoss 0.01147, SoftDiceLoss 0.07902, SumLoss 0.09049\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-91d54341b587>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mloss_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mloss_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pytorch_segm_model_sum_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-a71a05c62d69>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, gpu)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Simple summ of losses:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoft_dice_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-b27b84473a83>\u001b[0m in \u001b[0;36msoft_dice_loss\u001b[0;34m(inputs, targets)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mintersection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mm1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mm2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mm1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mm2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mjaccard_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_tr = []\n",
    "loss_val = []\n",
    "gpu = torch.cuda.is_available()\n",
    "for epoch in range(15):\n",
    "    loss_tr.append(train(epoch, gpu))\n",
    "    loss_val.append(validate(gpu))\n",
    "    torch.save(model.state_dict(), 'pytorch_segm_model_sum_loss')\n",
    "torch.save(model.state_dict(), 'pytorch_segm_model_sum_loss_final')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet11('trained_recently', 'pytorch_segm_model_sum_loss')\n",
    "#model = unet11('carvana')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs, classes = next(iter(val_loader))\n",
    "input_img = torch.unsqueeze(Variable(inputs.cuda(async=True)), dim=0)[0]#Variable(inputs.cuda(async=True))\n",
    "model.cuda()\n",
    "model.eval()\n",
    "output = model(input_img).data.cpu().numpy()\n",
    "#show_pics(model(input_img).data.cpu().numpy().reshape((24, 256, 320)), 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAskAAAGtCAYAAADkuOk8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAIABJREFUeJzt3X+U3HV97/HnO5vNht8k/GpMUkGNp+KtBdwbQrW9tGjF9LborbbhnFtyLffEWjjFe+25BT2t9vb0HNtT8UiraCxcsLUgFSypYmmMWG0rCQERgmlkVYQ1MamCAUFCsvu+f+w3MnzYZGd3fs88H+fMmZnPfGbn89nMi30x892dyEwkSZIkPWtepxcgSZIkdRtLsiRJklSwJEuSJEkFS7IkSZJUsCRLkiRJBUuyJEmSVGhZSY6I8yNiR0SMRcTlrXocSY0zr1LvMK9Se0Qr/k5yRAwBXwdeC4wDdwEXZubXmv5gkhpiXqXeYV6l9mnVK8krgbHM/GZmPgPcCFzQoseS1BjzKvUO8yq1yfwWfd2lwCM118eBs2snRMQ6YB3AEEOvPJJjW7QUqfc8wWPfy8yT2vRwM+YViszG8CuPzKPbszqpyz3NkzyT+6JNDzf7vPozVnqOen/GtqokT/cfi+cc15GZ64H1AMfG4jw7zmvRUqTe87n85Lfb+HAz5hXMrHQom3NTOx/OvEoNqvdnbKsOtxgHltdcXwbsbNFjSWqMeZV6h3mV2qRVJfkuYEVEnBYRC4A1wIYWPZakxphXqXeYV6lNWnK4RWYeiIhLgduBIeDazHygFY8lqTHmVeod5lVqn1Ydk0xm3gbc1qqvL6l5zKvUO8yr1B5+4p4kSZJUsCRLkiRJBUuyJEmSVLAkS5IkSQVLsiRJklSwJEuSJEkFS7IkSZJUsCRLkiRJBUuyJEmSVLAkS5IkSQVLsiRJklSwJEuSJEkFS7IkSZJUsCRLkiRJBUuyJElqTARDJyxm6KUvZt7ChRDR6RVJDbMkS5KkuYvgh29aycfu/Qc+dceNfGDHJr572TkWZfU8S7IkSZqTp39lJdd8+0v86wc+wolDRzESw7x0+Ci++n8+xIbxLcTISKeXKM2ZJVmSJM3avIUL+eePrGfZ/KOnvX0khvmbsc8zf8lPtHllUnNYkiVJ0qw9fe5Pzzhn0bwj2P7eF7RhNVLzWZIlSdKsfeavPjjjnKGYx7bzPtyG1UjNZ0mWJEmzNhLzmzpP6jaWZEmS1DJDYdVQb2romRsRD0XE/RFxb0RsrcYWR8TGiHiwOl/UnKVKapSZlXpHt+f1xh+eVNe8/TnR4pVIrdGM/737hcw8IzNHq+uXA5sycwWwqbouqXuYWal3dG1eP/L7v1bXvIcP/KjFK5FaoxXvgVwAXF9dvh54QwseQ1LzmFmpd3RNXo/4+y38cPJpJnLykHNe9q+/ySUvPrd9i5KaqNGj6RP4p4hI4COZuR44JTN3AWTmrog4ebo7RsQ6YB3AQo5scBmS6mRmpd7R9Xn9tWWrpj6S+rhjOfYzQ/zfZf/AKUPz2PbMCH/yy7/BT26/n2zZo0ut1WhJflVm7qxCujEi/r3eO1ZhXw9wbCw2Q1J7mFmpd/RGXjOZ+MFeHnt1cNkRr2XeCYuZ+O4ecv+DLX1YqdUaOtwiM3dW53uATwErgd0RsQSgOt/T6CIlNYeZlXpHz+U1k8mnnuLAI+Pk/mc6vRqpYXMuyRFxVEQcc/Ay8EvANmADsLaatha4tdFFSmqcmZV6h3mVOq+Rwy1OAT4VEQe/zt9m5j9GxF3ATRFxMfAw8ObGlympCcys1DvMq9Rhcy7JmflN4GemGf8+cF4ji5LUfGZW6h3mVeo8PwZHkiRJKliSJUmSpIIlWZIkSSpYkiVJkqSCJVmSJEkqWJK7SMyfD6teweSm5Tz4wbNh3lCnlyRJkjSQGv1YajVJDC/gH7+9Bdg6NfAy2P+GCf7rslHIHv4E4AhiaIgYGQFg3jFHk8ceDXufYGJ393xQlCRJUi1Lcpf4zEN3Ur6wPxxDXDH2Vd77U6M9+xGf+84f5T1/eQ0/t/AAQ/Hc/b3yPW/jxPVf7tDKJEmSDs3DLbpBxPMK5EE/t/AAz/yXn27zgprn229OVo48Pe3+Pv+HV8LUp0lJkiR1FUtyNzjM4RTXPf4Chj9/bxsX01wve8c3+IvHXj7tbcfNO4Kh445t84okSZJmZknuEhM5Oe34LavPhsmJNq+meSYee4w7zjyO3935n9k7+aPn3LbpR0NM7H28QyuTJEk6NI9J7hK/+Nbf5kN/+QFeOD/Yn5PsnAh+71ffwuS3/r3TS2tYHjjAjlH4dc4hhhcQC4bJffvIiYne/qVESZLUtyzJXWLhp7fwvz99TjHa+wW5lPuf6dlfQpQkSYPDwy0kSZKkgiVZkiRJKliSJUmSpIIlWZIkSSpYkiVJkqSCJVmSJEkqWJIlSZKkgiVZkiRJKliSJUmSpMKMJTkiro2IPRGxrWZscURsjIgHq/NF1XhExFURMRYR90XEWa1cvKTnM7NS7zCvUveq55Xk64Dzi7HLgU2ZuQLYVF0HeD2wojqtA65uzjIlzcJ1mFmpV1yHeZW60owlOTO/CDxaDF8AXF9dvh54Q834x3LKncDxEbGkWYuVNDMzK/UO8yp1r7kek3xKZu4CqM5PrsaXAo/UzBuvxp4nItZFxNaI2LqffXNchqQ6mVmpd5hXqQs0+xf3YpqxnG5iZq7PzNHMHB1mpMnLkFQnMyv1DvMqtdFcS/Lug2/xVOd7qvFxYHnNvGXAzrkvT1KTmFmpd5hXqQvMtSRvANZWl9cCt9aMX1T9Bu4qYO/Bt4wkdZSZlXqHeZW6wPyZJkTEDcC5wIkRMQ68G3gvcFNEXAw8DLy5mn4bsBoYA54C3tKCNUs6DDMr9Q7zKnWvGUtyZl54iJvOm2ZuApc0uihJc2dmpd5hXqXu5SfuSZIkSQVLsiRJklSwJEuSJEkFS7IkSZJUsCRLkiRJBUuyJEmSVLAkS5IkSQVLsiRJklSwJEuSJEkFS7IkSZJUsCRLkiRJBUuyJEmSVLAkS5IkSQVLsiRJklSwJEuSJEkFS7IkSZJUsCRLkiRJBUuyJEmSVLAkS5IkSQVLsiRJklSwJEuSJEkFS7IkSZJUmLEkR8S1EbEnIrbVjL0nIr4TEfdWp9U1t10REWMRsSMiXteqhUuanpmVeod5lbpXPa8kXwecP834+zPzjOp0G0BEnA6sAV5e3edDETHUrMVKqst1mFmpV1yHeZW60owlOTO/CDxa59e7ALgxM/dl5reAMWBlA+uTNEtmVuod5lXqXo0ck3xpRNxXvVW0qBpbCjxSM2e8GnueiFgXEVsjYut+9jWwDEl1MrNS7zCvUofNtSRfDbwYOAPYBbyvGo9p5uZ0XyAz12fmaGaODjMyx2VIqpOZlXqHeZW6wJxKcmbuzsyJzJwEPsqzb/eMA8trpi4Ddja2REmNMrNS7zCvUneYU0mOiCU1V98IHPyt3A3AmogYiYjTgBXAlsaWKKlRZlbqHeZV6g7zZ5oQETcA5wInRsQ48G7g3Ig4g6m3eR4C3gqQmQ9ExE3A14ADwCWZOdGapUuajpmVeod5lbpXZE57OFNbHRuL8+w4r9PLkLrG5/KTd2fmaKfXcShmVnrW5tzE4/nodMcLdwXzKj1XvT9j/cQ9SZIkqWBJliRJkgqWZEmSJKlgSZYkSZIKlmRJkiSpYEmWJEmSCpZkSZIkqWBJliRJkgqWZEmSJKlgSZYkSZIKlmRJkiSpYEmWJEmSCpZkSZIkqWBJliRJkgqWZEmSJKlgSZYkSZIKlmRJkiSpYEmWJEmSCpZkSZIkqWBJliRJkgqWZEmSJKkwY0mOiOURcUdEbI+IByLismp8cURsjIgHq/NF1XhExFURMRYR90XEWa3ehKQp5lXqLWZW6l71vJJ8AHhHZr4MWAVcEhGnA5cDmzJzBbCpug7wemBFdVoHXN30VUs6FPMq9RYzK3WpGUtyZu7KzHuqy08A24GlwAXA9dW064E3VJcvAD6WU+4Ejo+IJU1fuaTnMa9SbzGzUvea1THJEXEqcCawGTglM3fBVMiBk6tpS4FHau42Xo2VX2tdRGyNiK372Tf7lUs6rGbmtfp6ZlZqIX/GSt2l7pIcEUcDNwNvz8zHDzd1mrF83kDm+swczczRYUbqXYakOjQ7r2BmpVbyZ6zUfeoqyRExzFR4P56Zt1TDuw++xVOd76nGx4HlNXdfBuxsznIlzcS8Sr3FzErdqZ6/bhHANcD2zLyy5qYNwNrq8lrg1prxi6rfwF0F7D34lpGk1jKvUm8xs1L3ml/HnFcBvwncHxH3VmPvBN4L3BQRFwMPA2+ubrsNWA2MAU8Bb2nqiiUdjnmVeouZlbrUjCU5M/+F6Y+BAjhvmvkJXNLguiTNgXmVeouZlbqXn7gnSZIkFSzJkiRJUsGSLEmSJBUsyZIkSVLBkixJkiQVLMmSJElSwZIsSZIkFSzJkiRJUsGSLEmSJBUsyZIkSVLBkixp9g71IbqSJPUJS7KkWYvwPx2SpP7mTzpJs5fZ6RVIktRSlmRJs5aWZElSn7MkS5o9j0mWJPU5S7KkObAlS5L6myVZ0qxFWJIlSf3NkixJkiQVLMmSZi0nJzu9BEmSWsqSLEmSJBUsyZIkSVLBkixJkiQVZizJEbE8Iu6IiO0R8UBEXFaNvycivhMR91an1TX3uSIixiJiR0S8rpUbkPQs8yr1FjMrda/5dcw5ALwjM++JiGOAuyNiY3Xb+zPzz2snR8TpwBrg5cALgM9FxEszc6KZC5c0LfMq9RYzK3WpGV9JzsxdmXlPdfkJYDuw9DB3uQC4MTP3Zea3gDFgZTMWK+nwzKvUW8ys1L1mdUxyRJwKnAlsroYujYj7IuLaiFhUjS0FHqm52zjTBD4i1kXE1ojYup99s164pMNrZl6rr2dmpRbyZ6zUXeouyRFxNHAz8PbMfBy4GngxcAawC3jfwanT3D2fN5C5PjNHM3N0mJFZL1zSoTU7r2BmpVbyZ6zUfeoqyRExzFR4P56ZtwBk5u7MnMjMSeCjPPt2zziwvObuy4CdzVuypMMxr1JvMbNSd6rnr1sEcA2wPTOvrBlfUjPtjcC26vIGYE1EjETEacAKYEvzliz1uZjuhaJ679qevL70FU/NeY1S35l7ZP0ZK3WxyJz2ndVnJ0S8GvgScD9w8LNo3wlcyNTbQAk8BLw1M3dV93kX8FtM/dbu2zPzszM8xn8ATwLfm+tGetSJDN6eYTD3Pds9vzAzT5rtg7Qjr9V9ngB2zHZ9PW4Qn7cwmPtuS16hbT9jBzGv4HN3kLQkszOW5HaJiK2ZOdrpdbTTIO4ZBnPf/bbnfttPPQZxzzCY++63Pffbfuo1iPsexD1D6/btJ+5JkiRJBUuyJEmSVOimkry+0wvogEHcMwzmvvttz/22n3oM4p5hMPfdb3vut/3UaxD3PYh7hhbtu2uOSZYkSZK6RTe9kixJkiR1hY6X5Ig4PyJ2RMRYRFze6fU0U/VRonsiYlvN2OKI2BgRD1bni6rxiIirqu/DfRFxVudWPncRsTwi7oiI7RHxQERcVo337b4jYmFEbImIr1Z7/qNq/LSI2Fzt+RMRsaAaH6muj1W3n9rJ9c+Gee2f5y0MZl7BzPaDQcwrDGZmO5rXzOzYCRgCvgG8CFgAfBU4vZNravL+fh44C9hWM/ZnwOXV5cuBP60urwY+y9SfpV8FbO70+ue45yXAWdXlY4CvA6f3876rtR9dXR4GNld7uQlYU41/GHhbdfl3gA9Xl9cAn+j0Hurcp3nto+dttY+By2u1DzPb46dBzGu1l4HLbCfz2umNnwPcXnP9CuCKTv+DNHmPpxYh3gEsqS4vAXZUlz8CXDjdvF4+AbcCrx2UfQNHAvcAZzP1h83nV+M/fq4DtwPnVJfnV/Oi02uvY2/mtU+ftzX7GKi8Vnswsz16GvS8VnsZqMy2O6+dPtxiKfBIzfXxaqyfnZLVpyZV5ydX4333vaje4jiTqf/r6+t9R8RQRNwL7AE2MvXqzQ8y80A1pXZfP95zdfte4IT2rnhO+uLfapb6+nlba5DyCma2T/X987bWIGW2U3ntdEme7hPvB/XPbfTV9yIijgZuZuojUx8/3NRpxnpu35k5kZlnAMuAlcDLpptWnffqnnt13a3QV9+LQcsrmNkB03ffh0HLbKfy2umSPA4sr7m+DNjZobW0y+6IWAJQne+pxvvmexERw0yF9+OZeUs13Pf7BsjMHwBfYOp4qeMjYn51U+2+frzn6vbjgEfbu9I56at/qzr1/fN2kPMKZrbPDMTzdpAz2+68drok3wWsqH5DcQFTB1hv6PCaWm0DsLa6vJap44kOjl9U/SbqKmDvwbdOeklEBHANsD0zr6y5qW/3HREnRcTx1eUjgNcA24E7gDdV08o9H/xevAn4fFYHT3U589pHz1sYzLyCme3wmlqpr5+3MJiZ7Wheu+Ag7NVM/XbmN4B3dXo9Td7bDcAuYD9T/2dzMVPHxWwCHqzOF1dzA/hg9X24Hxjt9PrnuOdXM/W2xn3AvdVpdT/vG3gF8JVqz9uAP6zGXwRsAcaAvwNGqvGF1fWx6vYXdXoPs9irec3+eN5W+xi4vFb7MLM9fhrEvFZ7GbjMdjKvfuKeJEmSVOj04RaSJElS17EkS5IkSQVLsiRJklSwJEuSJEkFS7IkSZJUsCRLkiRJBUuyJEmSVLAkS5IkSQVLsiRJklSwJEuSJEkFS7IkSZJUsCRLkiRJBUuyJEmSVLAkS5IkSQVLsiRJklSwJEuSJEkFS7IkSZJUsCRLkiRJBUuyJEmSVLAkS5IkSQVLsiRJklSwJEuSJEkFS7IkSZJUsCRLkiRJBUuyJEmSVLAkS5IkSQVLsiRJklSwJEuSJEkFS7IkSZJUsCRLkiRJBUuyJEmSVLAkS5IkSQVLsiRJklSwJEuSJEkFS7IkSZJUsCRLkiRJBUuyJEmSVLAkS5IkSQVLsiRJklSwJEuSJEkFS7IkSZJUsCRLkiRJBUuyJEmSVLAkS5IkSQVLsiRJklSwJEuSJEkFS7IkSZJUsCRLkiRJBUuyJEmSVLAkS5IkSQVLsiRJklSwJEuSJEkFS7IkSZJUsCRLkiRJBUuyJEmSVLAkS5IkSQVLsiRJklSwJEuSJEkFS7IkSZJUsCRLkiRJhZaV5Ig4PyJ2RMRYRFzeqseR1DjzKvUO8yq1R2Rm879oxBDwdeC1wDhwF3BhZn6t6Q8mqSHmVeod5lVqn1a9krwSGMvMb2bmM8CNwAUteixJjTGvUu8wr1KbzG/R110KPFJzfRw4+1CTF8RILuSoFi1F6j1P8Nj3MvOkNj3crPIKZlaq9TRP8kzuizY9nHmVGlTvz9hWleTp/mPxnOM6ImIdsA5gIUdydpzXoqVIvedz+clvt/HhZswrmFnpUDbnpnY+nHmVGlTvz9hWHW4xDiyvub4M2Fk7ITPXZ+ZoZo4OM9KiZUiqw4x5BTMrdQnzKrVJq0ryXcCKiDgtIhYAa4ANLXosSY0xr1LvMK9Sm7TkcIvMPBARlwK3A0PAtZn5QCseS1JjzKvUO8yr1D6tOiaZzLwNuK1VX19S85hXqXeYV6k9/MQ9SZIkqWBJliRJkgqWZEmSJKlgSZYkSZIKlmRJkiSpYEmWJEmSCpZkSZIkqWBJliRJkgqWZEmSJKlgSZYkSZIKlmRJkiSpYEmWJEmSCpZkSZIkqWBJliRJqsO8o47ij791FzeP38nN43fy/YvPIYYXdHpZapH5nV6AJElSV4vg9u98pboyXJ1g6x9fDX8Mv/yzv8qBhx7u2PLUGr6SLEmSdBgP/uXKw95+/ZduaNNK1E6WZEmSpMP45hs/ctjbTxw6ihgZadNq1C6WZEmSpEOY/8Lldc0bWrqkxStRu1mSJUmSDmHH7y6ta97k0QtbvBK1myVZkiTpEI568d665u0/4cgWr0TtZkmWJEk6hGP++ti65i3Y+XiLV6J280/ASVI/iGDn753DTZf8OcfMm2QS2PjkS7j57Jcy8bg/vKW5OuqWLfAXM8/L8V2tX4zaqqGSHBEPAU8AE8CBzByNiMXAJ4BTgYeAX8/MxxpbpqRmMLP96RvvW8V9v3EVR877CvDsW74XH/dd/sf2nZx76ds46h/uJg8c6NwiNWvmtUtk1jVt8sknW7wQtVszDrf4hcw8IzNHq+uXA5sycwWwqbouqXuY2X4yb4gH1vwFR86b/lO/hmIeN191JROr/lObF6YmMa9d4Fv7f3jY27++34Lcj1pxuMUFwLnV5euBLwC/34LHkdQcZrZHxfz57PrkCkbi7sPOO3noKDbedB3n/+Soryb3PvPaAb/9wldDBPNGRthz0Zn84Oef5qKf3szHP/NfeMn7djDx/Uc7vUS1QKOvJCfwTxFxd0Ssq8ZOycxdANX5ydPdMSLWRcTWiNi6n30NLkNSncxsH4n58/n7Mz9a9/xdlxz+U8PUdcxrN8lk8umnOXH9l3nJf/8K//YzCzjtnV+2IPexRl9JflVm7oyIk4GNEfHv9d4xM9cD6wGOjcX1HfAjqVFmto/EEUewbP4Rdc//n+s+w6c/sKiFK1KTmVepgxp6JTkzd1bne4BPASuB3RGxBKA639PoIiU1h5ntLz+66ViGY6ju+b94VN0dS13AvEqdNeeSHBFHRcQxBy8DvwRsAzYAa6tpa4FbG12kpMaZ2f7zoRU3zGr+l556SYtWomYzr1LnNXK4xSnApyLi4Nf528z8x4i4C7gpIi4GHgbe3PgyJTWBme0zLxoentX8a//0V1nEl1u0GjWZeZU6bM4lOTO/CfzMNOPfB85rZFFqggie+aVXsv+YIY77l4c48N3dnV6ROszM9p8d+yd4xYL6i/Kiv97SwtWomcyr1Hl+4l4/mjfE3k+fxp1n/NWPh878k9/h5A/+WwcXJanZ/mzn+fzNqV+o/w6TEy1biyT1m2Z8mIi6zKcf2cKdZ3zyOWNfedeH2HPpz3ZoRZJa4dFfmce+3F/X3Dd94zUtXo0k9RdLcp+Zd+SRh/xt96+880OM/c2ZbV6RpFaZ+P6j/LdX/dph59y97xlet/RMnvi577VpVZLUHyzJ/SSCx974isNO+eCqv23TYiS1w4GHv8M7d7+Cpyafec74RE7y2MRT/MGZr4X0z+RK0mx5THIfmXfkkfz2H9xy2DkP7vuJNq1GUltMTnD3mfN4Y5xNzB8mFgzDxASTTz9dTdjb0eVJUq/yleQ+88TEoT99ayInef/nz2/jaiS1TSa5/xkmn3yypiBLkubKktxHct8+Pva+1Ye8/YH9z/BT73mwjSuSJEnqTZbkPpIHDrD4/915yN92/18XX8LE9x9t86okSZJ6jyW532TyyR/+BBM5+Zzh/TnBgi9v79CiJEmSeosluQ997Kd+ktdc/Fb25wT7cj8PH/ghb1j5K0w+9VSnlyZJktQT/OsW/SiTBbdvZf3eU/nmj07inz98Nid858udXpUkSVLPsCT3q0w2nH4CMMkJWJAlSZJmw8MtJEmSpIIlWZIkSSpYkiVJkqSCJVmSJEkqWJIlSZKkgiVZkiRJKliSJUmSpIIlWZIkSSpYkiVJkqSCJVmSJEkqzFiSI+LaiNgTEdtqxhZHxMaIeLA6X1SNR0RcFRFjEXFfRJzVysVLej4zK/UO8yp1r3peSb4OOL8YuxzYlJkrgE3VdYDXAyuq0zrg6uYsU9IsXIeZlXrFdZhXqSvNWJIz84vAo8XwBcD11eXrgTfUjH8sp9wJHB8RS5q1WLVRRKdXoDkys1LvMK9S95rrMcmnZOYugOr85Gp8KfBIzbzxaux5ImJdRGyNiK372TfHZUiqk5mVeod5lbpAs39xb7qXH3O6iZm5PjNHM3N0mJEmL0MNy2n/2dR/zKzUO8yr1EZzLcm7D77FU53vqcbHgeU185YBO+e+PElNYmal3mFepS4w15K8AVhbXV4L3FozflH1G7irgL0H3zKS1FFmVuod5lXqAvNnmhARNwDnAidGxDjwbuC9wE0RcTHwMPDmavptwGpgDHgKeEsL1izpMMys1DvMq9S9ZizJmXnhIW46b5q5CVzS6KIkzZ2ZlXqHeZW6l5+4J0mSJBUsyZIkSVLBkixJkiQVLMmSJElSwZIsSZIkFSzJkiRJUsGSLEmSJBUsyZIkSVLBkixJkiQVLMmSJElSwZIsSZIkFSzJkiRJUsGSLEmSJBUsyZIkSVLBkixJkiQVLMmSJElSwZIsSZIkFSzJkiRJUsGSLEmSJBUsyZIkSVLBkixJkiQVLMmSJElSYcaSHBHXRsSeiNhWM/aeiPhORNxbnVbX3HZFRIxFxI6IeF2rFi5pemZW6h3mVepe9bySfB1w/jTj78/MM6rTbQARcTqwBnh5dZ8PRcRQsxYrqS7XYWalXnEd5lXqSjOW5Mz8IvBonV/vAuDGzNyXmd8CxoCVDaxP0iyZWal3mFepezVyTPKlEXFf9VbRompsKfBIzZzxaux5ImJdRGyNiK372dfAMiTVycxKvcO8Sh0215J8NfBi4AxgF/C+ajymmZvTfYHMXJ+Zo5k5OszIHJchqU5mVuod5lXqAnMqyZm5OzMnMnMS+CjPvt0zDiyvmboM2NnYEiU1ysxKvcO8St1hTiU5IpbUXH0jcPC3cjcAayJiJCJOA1YAWxpboqRGmVmpd5hXqTvMn2lCRNwAnAucGBHjwLuBcyPiDKbe5nkIeCtAZj4QETcBXwMOAJdk5kRrli5pOmZW6h3mVepekTnt4UxtdWwszrPjvE4vQ+oan8tP3p2Zo51ex6GYWelZm3MTj+ej0x0v3BXMq/Rc9f6M9RP3JEmSpIIlWZIkSSpYkiVJkqSCJVmSJEkqWJIlSZKkgiVZkiRJKliSJUmSpIIlWZIkSSpYkiVJkqSCJVmSJEkqWJIlSZKkgiVZkiRJKliSJUmSpIIlWZIkSSpYkiVJkqSCJVmSJEkqWJIlSZKkgiVZkiRJKliSJUmSpIIlWZIkSSpYkiVJkqTCjCU5IpZHxB0RsT0iHoiIy6rxxRGxMSIerM4XVeMREVdFxFhE3BcRZ7V6E5J+Y+MtAAAIEUlEQVSmmFept5hZqXvV80ryAeAdmfkyYBVwSUScDlwObMrMFcCm6jrA64EV1WkdcHXTVy3pUMyr1FvMrNSlZizJmbkrM++pLj8BbAeWAhcA11fTrgfeUF2+APhYTrkTOD4iljR95ZKex7xKvcXMSt1rVsckR8SpwJnAZuCUzNwFUyEHTq6mLQUeqbnbeDUmqY3Mq9RbzKzUXeouyRFxNHAz8PbMfPxwU6cZy2m+3rqI2BoRW/ezr95lSKpDs/NafU0zK7WIP2Ol7lNXSY6IYabC+/HMvKUa3n3wLZ7qfE81Pg4sr7n7MmBn+TUzc31mjmbm6DAjc12/pEIr8gpmVmoVf8ZK3amev24RwDXA9sy8suamDcDa6vJa4Naa8Yuq38BdBew9+JaRpNYyr1JvMbNS95pfx5xXAb8J3B8R91Zj7wTeC9wUERcDDwNvrm67DVgNjAFPAW9p6oolHY55lXqLmZW61IwlOTP/hemPgQI4b5r5CVzS4LokzYF5lXqLmZW6l5+4J0mSJBUsyZIkSVLBkixJkiQVLMmSJElSwZIsSZIkFSzJkiRJUsGSLEmSJBUsyZIkSVLBkixJkiQVLMmSJElSwZIsSZIkFSzJkiRJUsGSLEmSJBUsyZIkSVLBkixJkiQVLMmSJElSwZIsSZIkFSzJkiRJUsGSLEmSJBUsyZIkSVLBkixJkiQVLMmSJElSYcaSHBHLI+KOiNgeEQ9ExGXV+Hsi4jsRcW91Wl1znysiYiwidkTE61q5AUnPMq9SbzGzUveaX8ecA8A7MvOeiDgGuDsiNla3vT8z/7x2ckScDqwBXg68APhcRLw0MyeauXBJ0zKvUm8xs1KXmvGV5MzclZn3VJefALYDSw9zlwuAGzNzX2Z+CxgDVjZjsZIOz7xKvcXMSt1rVsckR8SpwJnA5mro0oi4LyKujYhF1dhS4JGau40zTeAjYl1EbI2IrfvZN+uFSzq8Zua1+npmVmohf8ZK3aXukhwRRwM3A2/PzMeBq4EXA2cAu4D3HZw6zd3zeQOZ6zNzNDNHhxmZ9cIlHVqz8wpmVmolf8ZK3aeukhwRw0yF9+OZeQtAZu7OzInMnAQ+yrNv94wDy2vuvgzY2bwlSzoc8yr1FjMrdad6/rpFANcA2zPzyprxJTXT3ghsqy5vANZExEhEnAasALY0b8mSDsW8Sr3FzErdKzKnfWf12QkRrwa+BNwPTFbD7wQuZOptoAQeAt6ambuq+7wL+C2mfmv37Zn52Rke4z+AJ4HvzXUjPepEBm/PMJj7nu2eX5iZJ832QdqR1+o+TwA7Zru+HjeIz1sYzH23Ja/Qtp+xg5hX8Lk7SFqS2RlLcrtExNbMHO30OtppEPcMg7nvfttzv+2nHoO4ZxjMfffbnvttP/UaxH0P4p6hdfv2E/ckSZKkgiVZkiRJKnRTSV7f6QV0wCDuGQZz3/22537bTz0Gcc8wmPvutz33237qNYj7HsQ9Q4v23TXHJEuSJEndopteSZYkSZK6giVZkiRJKnS8JEfE+RGxIyLGIuLyTq+nmSLi2ojYExHbasYWR8TGiHiwOl9UjUdEXFV9H+6LiLM6t/K5i4jlEXFHRGyPiAci4rJqvG/3HRELI2JLRHy12vMfVeOnRcTmas+fiIgF1fhIdX2suv3UTq5/Nsxr/zxvYTDzCma2HwxiXmEwM9vRvGZmx07AEPAN4EXAAuCrwOmdXFOT9/fzwFnAtpqxPwMury5fDvxpdXk18FkggFXA5k6vf457XgKcVV0+Bvg6cHo/77ta+9HV5WFgc7WXm4A11fiHgbdVl38H+HB1eQ3wiU7voc59mtc+et5W+xi4vFb7MLM9fhrEvFZ7GbjMdjKvnd74OcDtNdevAK7o9D9Ik/d4ahHiHcCS6vISYEd1+SPAhdPN6+UTcCvw2kHZN3AkcA9wNlOf/jO/Gv/xcx24HTinujy/mhedXnsdezOvffq8rdnHQOW12oOZ7dHToOe12stAZbbdee304RZLgUdqro9XY/3slKw+WrQ6P7ka77vvRfUWx5lM/V9fX+87IoYi4l5gD7CRqVdvfpCZB6optfv68Z6r2/cCJ7R3xXPSF/9Ws9TXz9tag5RXMLN9qu+ft7UGKbOdymunS3JMMzaof5Our74XEXE0cDPw9sx8/HBTpxnruX1n5kRmngEsA1YCL5tuWnXeq3vu1XW3Ql99LwYtr2BmB0zffR8GLbOdymunS/I4sLzm+jJgZ4fW0i67I2IJQHW+pxrvm+9FRAwzFd6PZ+Yt1XDf7xsgM38AfIGp46WOj4j51U21+/rxnqvbjwMebe9K56Sv/q3q1PfP20HOK5jZPjMQz9tBzmy789rpknwXsKL6DcUFTB1gvaHDa2q1DcDa6vJapo4nOjh+UfWbqKuAvQffOuklERHANcD2zLyy5qa+3XdEnBQRx1eXjwBeA2wH7gDeVE0r93zwe/Em4PNZHTzV5cxrHz1vYTDzCma2w2tqpb5+3sJgZrajee2Cg7BXM/Xbmd8A3tXp9TR5bzcAu4D9TP2fzcVMHRezCXiwOl9czQ3gg9X34X5gtNPrn+OeX83U2xr3AfdWp9X9vG/gFcBXqj1vA/6wGn8RsAUYA/4OGKnGF1bXx6rbX9TpPcxir+Y1++N5W+1j4PJa7cPM9vhpEPNa7WXgMtvJvPqx1JIkSVKh04dbSJIkSV3HkixJkiQVLMmSJElSwZIsSZIkFSzJkiRJUsGSLEmSJBUsyZIkSVLh/wMFGTLmyao3tAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f60991f6630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_pics(output.reshape((24, 256, 320)), 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_2 = classes.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f60804a2518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_pics(tmp_2.reshape(-1, 256, 320), 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NClasses Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/milesial/Pytorch-UNet/blob/master/unet\n",
    "class double_conv(nn.Module):\n",
    "    '''(conv => BN => ReLU) * 2'''\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(double_conv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class inconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(inconv, self).__init__()\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(down, self).__init__()\n",
    "        self.mpconv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            double_conv(in_ch, out_ch)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mpconv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
    "        super(up, self).__init__()\n",
    "\n",
    "        #  would be a nice idea if the upsampling could be learned too,\n",
    "        #  but my machine do not have enough memory to handle all those weights\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_ch, out_ch, 2, stride=2)\n",
    "\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diffX = x1.size()[2] - x2.size()[2]\n",
    "        diffY = x1.size()[3] - x2.size()[3]\n",
    "        x2 = F.pad(x2, (diffX // 2, int(diffX / 2),\n",
    "                        diffY // 2, int(diffY / 2)))\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class outconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(outconv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.inc = inconv(n_channels, 64)\n",
    "        self.down1 = down(64, 128)\n",
    "        self.down2 = down(128, 256)\n",
    "        self.down3 = down(256, 512)\n",
    "        self.down4 = down(512, 512)\n",
    "        self.up1 = up(1024, 256)\n",
    "        self.up2 = up(512, 128)\n",
    "        self.up3 = up(256, 64)\n",
    "        self.up4 = up(128, 64)\n",
    "        self.outc = outconv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        x = self.outc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(3, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    patience=1,\n",
    "    verbose=True,\n",
    "    threshold=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0faeaa2f550e48fca908811e105ab1f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1967), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "762e27c9dc494bc3aae2042c3ebcf096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=704), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e94b958ef52749398bddc6517195f47e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=174), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dset_train_1 = SURREALDataset('/home/novikov/data/skoltech/segmentation/SURREAL/data/sur/SURREAL/data/cmu/train/run0', 10000, True)\n",
    "dset_test_1 = SURREALDataset('/home/novikov/data/skoltech/segmentation/SURREAL/data/sur/SURREAL/data/cmu/test/run0',  10000, True)\n",
    "dset_val_1 = SURREALDataset('/home/novikov/data/skoltech/segmentation/SURREAL/data/sur/SURREAL/data/cmu/val/run0', 10000, True)\n",
    "\n",
    "batch_sz = 16\n",
    "train_loader = DataLoader(dset_train_1, batch_size=batch_sz, shuffle=False)\n",
    "test_loader = DataLoader(dset_test_1, batch_size=batch_sz, shuffle=True)\n",
    "val_loader = DataLoader(dset_val_1, batch_size=batch_sz, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "$ Torch: not enough memory: you tried to allocate 7GB. Buy new RAM! at /opt/conda/conda-bld/pytorch_1512386481460/work/torch/lib/TH/THGeneral.c:246",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-800c4f7e09e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mloss_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mloss_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pytorch_segm_model_2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-344c567fe2c9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, gpu)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-79c3c962d5f8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-79c3c962d5f8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m     61\u001b[0m                         diffY // 2, int(diffY / 2)))\n\u001b[1;32m     62\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-79c3c962d5f8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 277\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0m_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 torch.backends.cudnn.deterministic, torch.backends.cudnn.enabled)\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: $ Torch: not enough memory: you tried to allocate 7GB. Buy new RAM! at /opt/conda/conda-bld/pytorch_1512386481460/work/torch/lib/TH/THGeneral.c:246"
     ]
    }
   ],
   "source": [
    "loss_tr = []\n",
    "loss_val = []\n",
    "gpu = torch.cuda.is_available()\n",
    "for epoch in range(5):\n",
    "    loss_tr.append(train(epoch))\n",
    "    loss_val.append(validate())\n",
    "torch.save(model.state_dict(), 'pytorch_segm_model_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
